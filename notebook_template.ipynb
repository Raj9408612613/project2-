{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mongodb-developer/GenAI-Showcase/blob/main/notebooks/agents/agent_fireworks_ai_langchain_mongodb.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3kMALXaMv-MS"
   },
   "source": [
    "## Install Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cxTXczeTghzU",
    "outputId": "ae3a81b2-cba6-42fc-f593-8646bff77b14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchainhub\n",
      "  Obtaining dependency information for langchainhub from https://files.pythonhosted.org/packages/8f/24/10dd40be612a2e7225517a0c95edde5809e319c6bc4294d8bfbe18108beb/langchainhub-0.1.17-py3-none-any.whl.metadata\n",
      "  Using cached langchainhub-0.1.17-py3-none-any.whl.metadata (621 bytes)\n",
      "Requirement already satisfied: langchain-fireworks in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (0.1.3)\n",
      "Requirement already satisfied: langchain-huggingface in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (0.0.1)\n",
      "Requirement already satisfied: langchain-mongodb in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (0.1.3)\n",
      "Requirement already satisfied: arxiv in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (2.1.0)\n",
      "Requirement already satisfied: pymupdf in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (1.24.4)\n",
      "Requirement already satisfied: datasets in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (2.18.0)\n",
      "Requirement already satisfied: pymongo in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (4.6.3)\n",
      "Requirement already satisfied: tqdm in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (4.66.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from langchainhub) (2.31.0)\n",
      "Collecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub)\n",
      "  Obtaining dependency information for types-requests<3.0.0.0,>=2.31.0.2 from https://files.pythonhosted.org/packages/cf/12/41212fac764c538e9472c7abccf40db3d9d9a40658184388f9dbb78dc6e1/types_requests-2.32.0.20240523-py3-none-any.whl.metadata\n",
      "  Using cached types_requests-2.32.0.20240523-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.9.1 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from langchain-fireworks) (3.9.3)\n",
      "Requirement already satisfied: fireworks-ai>=0.13.0 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from langchain-fireworks) (0.14.0)\n",
      "Requirement already satisfied: langchain-core<0.3,>=0.1.52 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from langchain-fireworks) (0.1.52)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.10.0 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from langchain-fireworks) (1.28.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from langchain-huggingface) (0.23.2)\n",
      "Requirement already satisfied: sentence-transformers>=2.6.0 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from langchain-huggingface) (3.0.0)\n",
      "Requirement already satisfied: text-generation<0.8.0,>=0.7.0 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from langchain-huggingface) (0.7.0)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from langchain-huggingface) (0.19.1)\n",
      "Requirement already satisfied: transformers>=4.39.0 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from langchain-huggingface) (4.41.1)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from langchain-mongodb) (1.26.4)\n",
      "Requirement already satisfied: feedparser==6.0.10 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from arxiv) (6.0.10)\n",
      "Requirement already satisfied: sgmllib3k in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from feedparser==6.0.10->arxiv) (1.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from requests<3,>=2->langchainhub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from requests<3,>=2->langchainhub) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from requests<3,>=2->langchainhub) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from requests<3,>=2->langchainhub) (2024.2.2)\n",
      "Requirement already satisfied: PyMuPDFb==1.24.3 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from pymupdf) (1.24.3)\n",
      "Requirement already satisfied: filelock in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from datasets) (15.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from datasets) (2.2.1)\n",
      "Requirement already satisfied: xxhash in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from datasets) (2023.10.0)\n",
      "Requirement already satisfied: packaging in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from pymongo) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.9.1->langchain-fireworks) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.9.1->langchain-fireworks) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.9.1->langchain-fireworks) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.9.1->langchain-fireworks) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.9.1->langchain-fireworks) (1.9.4)\n",
      "Requirement already satisfied: httpx in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from fireworks-ai>=0.13.0->langchain-fireworks) (0.27.0)\n",
      "Requirement already satisfied: httpx-sse in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from fireworks-ai>=0.13.0->langchain-fireworks) (0.4.0)\n",
      "Requirement already satisfied: pydantic in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from fireworks-ai>=0.13.0->langchain-fireworks) (2.6.3)\n",
      "Requirement already satisfied: Pillow in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from fireworks-ai>=0.13.0->langchain-fireworks) (10.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (4.10.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from langchain-core<0.3,>=0.1.52->langchain-fireworks) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from langchain-core<0.3,>=0.1.52->langchain-fireworks) (0.1.23)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from langchain-core<0.3,>=0.1.52->langchain-fireworks) (8.2.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from openai<2.0.0,>=1.10.0->langchain-fireworks) (4.3.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from openai<2.0.0,>=1.10.0->langchain-fireworks) (1.9.0)\n",
      "Requirement already satisfied: sniffio in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from openai<2.0.0,>=1.10.0->langchain-fireworks) (1.3.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (2.2.1)\n",
      "Requirement already satisfied: scikit-learn in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.4.1.post1)\n",
      "Requirement already satisfied: scipy in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.12.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from transformers>=4.39.0->langchain-huggingface) (2023.12.25)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from transformers>=4.39.0->langchain-huggingface) (0.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from httpx->fireworks-ai>=0.13.0->langchain-fireworks) (1.0.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from httpcore==1.*->httpx->fireworks-ai>=0.13.0->langchain-fireworks) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.52->langchain-fireworks) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.3,>=0.1.52->langchain-fireworks) (3.9.15)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from pydantic->fireworks-ai>=0.13.0->langchain-fireworks) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from pydantic->fireworks-ai>=0.13.0->langchain-fireworks) (2.16.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: sympy in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.1.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (3.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from sympy->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.3.0)\n",
      "Using cached langchainhub-0.1.17-py3-none-any.whl (4.8 kB)\n",
      "Using cached types_requests-2.32.0.20240523-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: types-requests, langchainhub\n",
      "Successfully installed langchainhub-0.1.17 types-requests-2.32.0.20240523\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -qU langchainhub langchain-fireworks langchain-huggingface langchain-mongodb arxiv pymupdf datasets pymongo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RM8rg08YhqZe"
   },
   "source": [
    "## Set Evironment Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "oXLWCWEghuOX"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your MongoDB connection string: ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "MONGODB_URI = getpass.getpass(\"Enter your MongoDB connection string:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Fireworks API key: ········\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass(\"Enter Fireworks API key:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UUf3jtFzO4-V"
   },
   "source": [
    "## Ingest Data into MongoDB Vector Database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "referenced_widgets": [
      "cebfba144ba6418092df949783f93455",
      "09dcf4ce88064f11980bbefaad1ebc75",
      "f2bd7bda4d0c4d93b88e53aeb4e1b62d",
      "278513c5a8b04a24b1823d38107f1e50",
      "d3941c633788427abb858b21e285088f",
      "39563df9477648398456675ec51075aa",
      "f4353368efbd4c3891f805ddc3d05e1b",
      "30fe0bcd02cb47f3ba23bb480e2eaaea",
      "d17d8c8f45ee44cd87dcd787c05dbdc3",
      "62e196b6d30746578e137c50b661f946",
      "ced7f9d61e06442a960dcda95852048e",
      "7dbfebff68ff45628da832fac5233c93",
      "164d16df28d24ab796b7c9cf85174800",
      "e70e0d317f1e4e73bd95349ed1510cce",
      "41056c822b9d44559147d2b21416b956",
      "b1929fb112174c0abcd8004f6be0f880",
      "95e4af5b420242b7a6b74a18cad98961",
      "dff65b579f0746ffae8739ecb0aa5a41",
      "f73ae771c24645c79fd41409a8fc7b34",
      "20d693a09c534414a5c4c0dd58cf94ed",
      "a43c349d171e469c8cc94d48060f775b",
      "373ed3b6307741859ab297c270cf42c8"
     ]
    },
    "id": "pq4SA6r7O30i",
    "outputId": "904f4112-79fb-45cc-954b-d2b818cb2748"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"mongodb-eai/arxiv-embeddings\")\n",
    "dataset_df = pd.DataFrame(data[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>submitter</th>\n",
       "      <th>authors</th>\n",
       "      <th>title</th>\n",
       "      <th>comments</th>\n",
       "      <th>journal-ref</th>\n",
       "      <th>doi</th>\n",
       "      <th>report-no</th>\n",
       "      <th>categories</th>\n",
       "      <th>license</th>\n",
       "      <th>abstract</th>\n",
       "      <th>versions</th>\n",
       "      <th>update_date</th>\n",
       "      <th>authors_parsed</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>704.0001</td>\n",
       "      <td>Pavel Nadolsky</td>\n",
       "      <td>C. Bal\\'azs, E. L. Berger, P. M. Nadolsky, C.-...</td>\n",
       "      <td>Calculation of prompt diphoton production cros...</td>\n",
       "      <td>37 pages, 15 figures; published version</td>\n",
       "      <td>Phys.Rev.D76:013009,2007</td>\n",
       "      <td>10.1103/PhysRevD.76.013009</td>\n",
       "      <td>ANL-HEP-PR-07-12</td>\n",
       "      <td>hep-ph</td>\n",
       "      <td>None</td>\n",
       "      <td>A fully differential calculation in perturba...</td>\n",
       "      <td>[{'version': 'v1', 'created': 'Mon, 2 Apr 2007...</td>\n",
       "      <td>1227657600000</td>\n",
       "      <td>[[Balázs, C., ], [Berger, E. L., ], [Nadolsky,...</td>\n",
       "      <td>[0.2324569076, -0.894839108, -0.242858842, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>704.0002</td>\n",
       "      <td>Louis Theran</td>\n",
       "      <td>Ileana Streinu and Louis Theran</td>\n",
       "      <td>Sparsity-certifying Graph Decompositions</td>\n",
       "      <td>To appear in Graphs and Combinatorics</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>math.CO cs.CG</td>\n",
       "      <td>http://arxiv.org/licenses/nonexclusive-distrib...</td>\n",
       "      <td>We describe a new algorithm, the $(k,\\ell)$-...</td>\n",
       "      <td>[{'version': 'v1', 'created': 'Sat, 31 Mar 200...</td>\n",
       "      <td>1229126400000</td>\n",
       "      <td>[[Streinu, Ileana, ], [Theran, Louis, ]]</td>\n",
       "      <td>[0.6949232221, 0.3588359952, 0.1817755997, 0.7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>704.0003</td>\n",
       "      <td>Hongjun Pan</td>\n",
       "      <td>Hongjun Pan</td>\n",
       "      <td>The evolution of the Earth-Moon system based o...</td>\n",
       "      <td>23 pages, 3 figures</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>physics.gen-ph</td>\n",
       "      <td>None</td>\n",
       "      <td>The evolution of Earth-Moon system is descri...</td>\n",
       "      <td>[{'version': 'v1', 'created': 'Sun, 1 Apr 2007...</td>\n",
       "      <td>1200182400000</td>\n",
       "      <td>[[Pan, Hongjun, ]]</td>\n",
       "      <td>[0.1294624656, 1.1964389086, 0.8928941488, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>704.0004</td>\n",
       "      <td>David Callan</td>\n",
       "      <td>David Callan</td>\n",
       "      <td>A determinant of Stirling cycle numbers counts...</td>\n",
       "      <td>11 pages</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>math.CO</td>\n",
       "      <td>None</td>\n",
       "      <td>We show that a determinant of Stirling cycle...</td>\n",
       "      <td>[{'version': 'v1', 'created': 'Sat, 31 Mar 200...</td>\n",
       "      <td>1179878400000</td>\n",
       "      <td>[[Callan, David, ]]</td>\n",
       "      <td>[-0.0994227678, -0.364127785, 0.5390082002, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>704.0005</td>\n",
       "      <td>Alberto Torchinsky</td>\n",
       "      <td>Wael Abu-Shammala and Alberto Torchinsky</td>\n",
       "      <td>From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...</td>\n",
       "      <td>None</td>\n",
       "      <td>Illinois J. Math. 52 (2008) no.2, 681-689</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>math.CA math.FA</td>\n",
       "      <td>None</td>\n",
       "      <td>In this paper we show how to compute the $\\L...</td>\n",
       "      <td>[{'version': 'v1', 'created': 'Mon, 2 Apr 2007...</td>\n",
       "      <td>1381795200000</td>\n",
       "      <td>[[Abu-Shammala, Wael, ], [Torchinsky, Alberto, ]]</td>\n",
       "      <td>[0.0711007342, 0.5356642008, 0.5095595121, 0.4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id           submitter  \\\n",
       "0  704.0001      Pavel Nadolsky   \n",
       "1  704.0002        Louis Theran   \n",
       "2  704.0003         Hongjun Pan   \n",
       "3  704.0004        David Callan   \n",
       "4  704.0005  Alberto Torchinsky   \n",
       "\n",
       "                                             authors  \\\n",
       "0  C. Bal\\'azs, E. L. Berger, P. M. Nadolsky, C.-...   \n",
       "1                    Ileana Streinu and Louis Theran   \n",
       "2                                        Hongjun Pan   \n",
       "3                                       David Callan   \n",
       "4           Wael Abu-Shammala and Alberto Torchinsky   \n",
       "\n",
       "                                               title  \\\n",
       "0  Calculation of prompt diphoton production cros...   \n",
       "1           Sparsity-certifying Graph Decompositions   \n",
       "2  The evolution of the Earth-Moon system based o...   \n",
       "3  A determinant of Stirling cycle numbers counts...   \n",
       "4  From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...   \n",
       "\n",
       "                                  comments  \\\n",
       "0  37 pages, 15 figures; published version   \n",
       "1    To appear in Graphs and Combinatorics   \n",
       "2                      23 pages, 3 figures   \n",
       "3                                 11 pages   \n",
       "4                                     None   \n",
       "\n",
       "                                 journal-ref                         doi  \\\n",
       "0                   Phys.Rev.D76:013009,2007  10.1103/PhysRevD.76.013009   \n",
       "1                                       None                        None   \n",
       "2                                       None                        None   \n",
       "3                                       None                        None   \n",
       "4  Illinois J. Math. 52 (2008) no.2, 681-689                        None   \n",
       "\n",
       "          report-no       categories  \\\n",
       "0  ANL-HEP-PR-07-12           hep-ph   \n",
       "1              None    math.CO cs.CG   \n",
       "2              None   physics.gen-ph   \n",
       "3              None          math.CO   \n",
       "4              None  math.CA math.FA   \n",
       "\n",
       "                                             license  \\\n",
       "0                                               None   \n",
       "1  http://arxiv.org/licenses/nonexclusive-distrib...   \n",
       "2                                               None   \n",
       "3                                               None   \n",
       "4                                               None   \n",
       "\n",
       "                                            abstract  \\\n",
       "0    A fully differential calculation in perturba...   \n",
       "1    We describe a new algorithm, the $(k,\\ell)$-...   \n",
       "2    The evolution of Earth-Moon system is descri...   \n",
       "3    We show that a determinant of Stirling cycle...   \n",
       "4    In this paper we show how to compute the $\\L...   \n",
       "\n",
       "                                            versions    update_date  \\\n",
       "0  [{'version': 'v1', 'created': 'Mon, 2 Apr 2007...  1227657600000   \n",
       "1  [{'version': 'v1', 'created': 'Sat, 31 Mar 200...  1229126400000   \n",
       "2  [{'version': 'v1', 'created': 'Sun, 1 Apr 2007...  1200182400000   \n",
       "3  [{'version': 'v1', 'created': 'Sat, 31 Mar 200...  1179878400000   \n",
       "4  [{'version': 'v1', 'created': 'Mon, 2 Apr 2007...  1381795200000   \n",
       "\n",
       "                                      authors_parsed  \\\n",
       "0  [[Balázs, C., ], [Berger, E. L., ], [Nadolsky,...   \n",
       "1           [[Streinu, Ileana, ], [Theran, Louis, ]]   \n",
       "2                                 [[Pan, Hongjun, ]]   \n",
       "3                                [[Callan, David, ]]   \n",
       "4  [[Abu-Shammala, Wael, ], [Torchinsky, Alberto, ]]   \n",
       "\n",
       "                                           embedding  \n",
       "0  [0.2324569076, -0.894839108, -0.242858842, 0.1...  \n",
       "1  [0.6949232221, 0.3588359952, 0.1817755997, 0.7...  \n",
       "2  [0.1294624656, 1.1964389086, 0.8928941488, -0....  \n",
       "3  [-0.0994227678, -0.364127785, 0.5390082002, -0...  \n",
       "4  [0.0711007342, 0.5356642008, 0.5095595121, 0.4...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "o2gHwRjMfJlO"
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "# Initialize MongoDB python client\n",
    "client = MongoClient(MONGODB_URI)\n",
    "\n",
    "DB_NAME = \"agent_demo\"\n",
    "COLLECTION_NAME = \"knowledge\"\n",
    "ATLAS_VECTOR_SEARCH_INDEX_NAME = \"vector_index\"\n",
    "collection = client[DB_NAME][COLLECTION_NAME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zJkyy9UbffZT",
    "outputId": "c6f78ea3-fc93-4d57-95eb-98cea5bf15d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data ingestion into MongoDB completed\n"
     ]
    }
   ],
   "source": [
    "# Delete any existing records in the collection\n",
    "collection.delete_many({})\n",
    "\n",
    "# Data Ingestion\n",
    "records = dataset_df.to_dict(\"records\")\n",
    "collection.insert_many(records)\n",
    "\n",
    "print(\"Data ingestion into MongoDB completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6S1Cz9dtGPwL"
   },
   "source": [
    "## Create Vector Search Index Defintion\n",
    "\n",
    "```\n",
    "{\n",
    "  \"fields\": [\n",
    "    {\n",
    "      \"type\": \"vector\",\n",
    "      \"path\": \"embedding\",\n",
    "      \"numDimensions\": 1024,\n",
    "      \"similarity\": \"cosine\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create MongoDB Vector Store Retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_mongodb import MongoDBAtlasVectorSearch\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"mixedbread-ai/mxbai-embed-large-v1\")\n",
    "\n",
    "# Vector Store Creation\n",
    "vector_store = MongoDBAtlasVectorSearch.from_connection_string(\n",
    "    connection_string=MONGODB_URI,\n",
    "    namespace=DB_NAME + \".\" + COLLECTION_NAME,\n",
    "    embedding=embedding_model,\n",
    "    index_name=ATLAS_VECTOR_SEARCH_INDEX_NAME,\n",
    "    text_key=\"abstract\",\n",
    ")\n",
    "\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate Chat Completion LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_fireworks import ChatFireworks\n",
    "\n",
    "llm = ChatFireworks(\n",
    "    model=\"accounts/fireworks/models/firefunction-v1\", temperature=0.0, max_tokens=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZfheX5FiIhU"
   },
   "source": [
    "## Create Agent Tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "3eufR9H8gopU"
   },
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "from typing import Type\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_paper_metadata_from_arxiv(topic: str) -> list:\n",
    "    \"\"\"\n",
    "    Fetch and return paper metadata for 10 arxiv papers matching the given topic, for example: Retrieval Augmented Generation.\n",
    "\n",
    "    Args:\n",
    "    topic (str): The topic to find papers for on arXiv.\n",
    "\n",
    "    Returns:\n",
    "    list: Metadata about the papers matching the topic.\n",
    "    \"\"\"\n",
    "    docs = ArxivLoader(query=topic, load_max_docs=5).load()\n",
    "    # Extract just the metadata from each document\n",
    "    metadata = [doc.metadata for doc in docs]\n",
    "    return metadata\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_paper_summary_from_arxiv(id: str) -> list:\n",
    "    \"\"\"\n",
    "    Fetch and return the summary for a single research paper from arXiv given the paper ID, for example: 1605.08386.\n",
    "\n",
    "    Args:\n",
    "    id (str): The paper ID.\n",
    "\n",
    "    Returns:\n",
    "    str: Summary of the paper.\n",
    "    \"\"\"\n",
    "    doc = ArxivLoader(query=id, load_max_docs=1).get_summaries_as_docs()\n",
    "    if len(doc) == 0:\n",
    "        return \"No summary found for this paper.\"\n",
    "    return doc[0].page_content\n",
    "\n",
    "\n",
    "@tool\n",
    "def answer_questions_about_topics(query: str) -> list:\n",
    "    \"\"\"\n",
    "    Answer questions about a given topic based on information in the knowledge base.\n",
    "\n",
    "    Args:\n",
    "    query (str): User query about a topic.\n",
    "\n",
    "    Returns:\n",
    "    str: Information about the topic.\n",
    "    \"\"\"\n",
    "    retrieve = {\n",
    "        \"context\": retriever\n",
    "        | (lambda docs: \"\\n\\n\".join([d.page_content for d in docs])),\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    template = \"\"\"Answer the question based only on the following context. If no context is provided, say I do not know: \\\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "    # Defining the chat prompt\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    # Parse output as a string\n",
    "    parse_output = StrOutputParser()\n",
    "    # Retrieval chain\n",
    "    retrieval_chain = retrieve | prompt | llm | parse_output\n",
    "\n",
    "    answer = retrieval_chain.invoke(query)\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test out the tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Published': '2022-02-13',\n",
       "  'Title': 'A Survey on Retrieval-Augmented Text Generation',\n",
       "  'Authors': 'Huayang Li, Yixuan Su, Deng Cai, Yan Wang, Lemao Liu',\n",
       "  'Summary': 'Recently, retrieval-augmented text generation attracted increasing attention\\nof the computational linguistics community. Compared with conventional\\ngeneration models, retrieval-augmented text generation has remarkable\\nadvantages and particularly has achieved state-of-the-art performance in many\\nNLP tasks. This paper aims to conduct a survey about retrieval-augmented text\\ngeneration. It firstly highlights the generic paradigm of retrieval-augmented\\ngeneration, and then it reviews notable approaches according to different tasks\\nincluding dialogue response generation, machine translation, and other\\ngeneration tasks. Finally, it points out some important directions on top of\\nrecent methods to facilitate future research.'},\n",
       " {'Published': '2024-05-12',\n",
       "  'Title': 'DuetRAG: Collaborative Retrieval-Augmented Generation',\n",
       "  'Authors': 'Dian Jiao, Li Cai, Jingsheng Huang, Wenqiao Zhang, Siliang Tang, Yueting Zhuang',\n",
       "  'Summary': \"Retrieval-Augmented Generation (RAG) methods augment the input of Large\\nLanguage Models (LLMs) with relevant retrieved passages, reducing factual\\nerrors in knowledge-intensive tasks. However, contemporary RAG approaches\\nsuffer from irrelevant knowledge retrieval issues in complex domain questions\\n(e.g., HotPot QA) due to the lack of corresponding domain knowledge, leading to\\nlow-quality generations. To address this issue, we propose a novel\\nCollaborative Retrieval-Augmented Generation framework, DuetRAG. Our\\nbootstrapping philosophy is to simultaneously integrate the domain fintuning\\nand RAG models to improve the knowledge retrieval quality, thereby enhancing\\ngeneration quality. Finally, we demonstrate DuetRAG' s matches with expert\\nhuman researchers on HotPot QA.\"},\n",
       " {'Published': '2023-12-09',\n",
       "  'Title': 'Context Tuning for Retrieval Augmented Generation',\n",
       "  'Authors': 'Raviteja Anantha, Tharun Bethi, Danil Vodianik, Srinivas Chappidi',\n",
       "  'Summary': \"Large language models (LLMs) have the remarkable ability to solve new tasks\\nwith just a few examples, but they need access to the right tools. Retrieval\\nAugmented Generation (RAG) addresses this problem by retrieving a list of\\nrelevant tools for a given task. However, RAG's tool retrieval step requires\\nall the required information to be explicitly present in the query. This is a\\nlimitation, as semantic search, the widely adopted tool retrieval method, can\\nfail when the query is incomplete or lacks context. To address this limitation,\\nwe propose Context Tuning for RAG, which employs a smart context retrieval\\nsystem to fetch relevant information that improves both tool retrieval and plan\\ngeneration. Our lightweight context retrieval model uses numerical,\\ncategorical, and habitual usage signals to retrieve and rank context items. Our\\nempirical results demonstrate that context tuning significantly enhances\\nsemantic search, achieving a 3.5-fold and 1.5-fold improvement in Recall@K for\\ncontext retrieval and tool retrieval tasks respectively, and resulting in an\\n11.6% increase in LLM-based planner accuracy. Additionally, we show that our\\nproposed lightweight model using Reciprocal Rank Fusion (RRF) with LambdaMART\\noutperforms GPT-4 based retrieval. Moreover, we observe context augmentation at\\nplan generation, even after tool retrieval, reduces hallucination.\"},\n",
       " {'Published': '2024-02-16',\n",
       "  'Title': 'Corrective Retrieval Augmented Generation',\n",
       "  'Authors': 'Shi-Qi Yan, Jia-Chen Gu, Yun Zhu, Zhen-Hua Ling',\n",
       "  'Summary': 'Large language models (LLMs) inevitably exhibit hallucinations since the\\naccuracy of generated texts cannot be secured solely by the parametric\\nknowledge they encapsulate. Although retrieval-augmented generation (RAG) is a\\npracticable complement to LLMs, it relies heavily on the relevance of retrieved\\ndocuments, raising concerns about how the model behaves if retrieval goes\\nwrong. To this end, we propose the Corrective Retrieval Augmented Generation\\n(CRAG) to improve the robustness of generation. Specifically, a lightweight\\nretrieval evaluator is designed to assess the overall quality of retrieved\\ndocuments for a query, returning a confidence degree based on which different\\nknowledge retrieval actions can be triggered. Since retrieval from static and\\nlimited corpora can only return sub-optimal documents, large-scale web searches\\nare utilized as an extension for augmenting the retrieval results. Besides, a\\ndecompose-then-recompose algorithm is designed for retrieved documents to\\nselectively focus on key information and filter out irrelevant information in\\nthem. CRAG is plug-and-play and can be seamlessly coupled with various\\nRAG-based approaches. Experiments on four datasets covering short- and\\nlong-form generation tasks show that CRAG can significantly improve the\\nperformance of RAG-based approaches.'},\n",
       " {'Published': '2023-05-27',\n",
       "  'Title': 'Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In',\n",
       "  'Authors': 'Zichun Yu, Chenyan Xiong, Shi Yu, Zhiyuan Liu',\n",
       "  'Summary': \"Retrieval augmentation can aid language models (LMs) in knowledge-intensive\\ntasks by supplying them with external information. Prior works on retrieval\\naugmentation usually jointly fine-tune the retriever and the LM, making them\\nclosely coupled. In this paper, we explore the scheme of generic retrieval\\nplug-in: the retriever is to assist target LMs that may not be known beforehand\\nor are unable to be fine-tuned together. To retrieve useful documents for\\nunseen target LMs, we propose augmentation-adapted retriever (AAR), which\\nlearns LM's preferences obtained from a known source LM. Experiments on the\\nMMLU and PopQA datasets demonstrate that our AAR trained with a small source LM\\nis able to significantly improve the zero-shot generalization of larger target\\nLMs ranging from 250M Flan-T5 to 175B InstructGPT. Further analysis indicates\\nthat the preferences of different LMs overlap, enabling AAR trained with a\\nsingle source LM to serve as a generic plug-in for various target LMs. Our code\\nis open-sourced at https://github.com/OpenMatch/Augmentation-Adapted-Retriever.\"}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_paper_metadata_from_arxiv.invoke(\"Retrieval Augmented Generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We determine the non-perturbatively renormalized axial current for O($a$)\\nimproved lattice QCD with Wilson quarks. Our strategy is based on the chirally\\nrotated Schr\\\\\"odinger functional and can be generalized to other finite (ratios\\nof) renormalization constants which are traditionally obtained by imposing\\ncontinuum chiral Ward identities as normalization conditions. Compared to the\\nlatter we achieve an error reduction up to one order of magnitude. Our results\\nhave already enabled the setting of the scale for the $N_{\\\\rm f}=2+1$ CLS\\nensembles [1] and are thus an essential ingredient for the recent $\\\\alpha_s$\\ndetermination by the ALPHA collaboration [2]. In this paper we shortly review\\nthe strategy and present our results for both $N_{\\\\rm f}=2$ and $N_{\\\\rm f}=3$\\nlattice QCD, where we match the $\\\\beta$-values of the CLS gauge configurations.\\nIn addition to the axial current renormalization, we also present precise\\nresults for the renormalized local vector current.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_paper_summary_from_arxiv.invoke(\"1808.09236\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No summary found for this paper.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_paper_summary_from_arxiv.invoke(\"808.09236\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Partial cubes are isometric subgraphs of hypercubes. They are characterized by structures on a graph defined by means of semicubes, and Djokovi\\\\'{c}'s and Winkler's relations. These structures are employed in the paper to characterize bipartite graphs and partial cubes of arbitrary dimension.\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_questions_about_topics.invoke(\"What are partial cubes?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    get_paper_metadata_from_arxiv,\n",
    "    get_paper_summary_from_arxiv,\n",
    "    answer_questions_about_topics,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Basic Tool-calling Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "RY13DrVXFDrm"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain.tools.render import render_text_description\n",
    "\n",
    "system_message = f\"\"\"Answer the following questions as best you can.\n",
    "You can answer directly if the user is greeting you or similar.\n",
    "Otherwise, you have access to the following tools:\n",
    "\n",
    "{render_text_description(tools)}\n",
    "\"\"\"\n",
    "\n",
    "# # CoT prompt\n",
    "# system_message = f\"\"\"Given a question, write out in a step-by-step manner your reasoning\n",
    "# for how you will solve the problem to be sure that your conclusion is correct. \n",
    "# Avoid simply stating the correct answer at the outset.You can answer directly if the user\n",
    "# is greeting you or similar. Otherwise, you have access to the following tools:\n",
    "\n",
    "# {render_text_description(tools)}\n",
    "\n",
    "# Begin!\n",
    "# \"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_message),\n",
    "        (\"human\", \"{input}\"),\n",
    "        # Placeholders fill up a **list** of messages\n",
    "        MessagesPlaceholder(\"agent_scratchpad\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent, tools=tools, verbose=True, handle_parsing_errors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `get_paper_metadata_from_arxiv` with `{'topic': 'prompt compression'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'Published': '2024-03-30', 'Title': 'PROMPT-SAW: Leveraging Relation-Aware Graphs for Textual Prompt Compression', 'Authors': 'Muhammad Asif Ali, Zhengping Li, Shu Yang, Keyuan Cheng, Yang Cao, Tianhao Huang, Lijie Hu, Lu Yu, Di Wang', 'Summary': \"Large language models (LLMs) have shown exceptional abilities for multiple\\ndifferent natural language processing tasks. While prompting is a crucial tool\\nfor LLM inference, we observe that there is a significant cost associated with\\nexceedingly lengthy prompts. Existing attempts to compress lengthy prompts lead\\nto sub-standard results in terms of readability and interpretability of the\\ncompressed prompt, with a detrimental impact on prompt utility. To address\\nthis, we propose PROMPT-SAW: Prompt compresSion via Relation AWare graphs, an\\neffective strategy for prompt compression over task-agnostic and task-aware\\nprompts. PROMPT-SAW uses the prompt's textual information to build a graph,\\nlater extracts key information elements in the graph to come up with the\\ncompressed prompt. We also propose GSM8K-AUG, i.e., an extended version of the\\nexisting GSM8k benchmark for task-agnostic prompts in order to provide a\\ncomprehensive evaluation platform. Experimental evaluation using benchmark\\ndatasets shows that prompts compressed by PROMPT-SAW are not only better in\\nterms of readability, but they also outperform the best-performing baseline\\nmodels by up to 14.3 and 13.7 respectively for task-aware and task-agnostic\\nsettings while compressing the original prompt text by 33.0 and 56.7.\"}, {'Published': '2024-02-25', 'Title': 'Say More with Less: Understanding Prompt Learning Behaviors through Gist Compression', 'Authors': 'Xinze Li, Zhenghao Liu, Chenyan Xiong, Shi Yu, Yukun Yan, Shuo Wang, Ge Yu', 'Summary': 'Large language models (LLMs) require lengthy prompts as the input context to\\nproduce output aligned with user intentions, a process that incurs extra costs\\nduring inference. In this paper, we propose the Gist COnditioned deCOding\\n(Gist-COCO) model, introducing a novel method for compressing prompts which\\nalso can assist the prompt interpretation and engineering. Gist-COCO employs an\\nencoder-decoder based language model and then incorporates an additional\\nencoder as a plugin module to compress prompts with inputs using gist tokens.\\nIt finetunes the compression plugin module and uses the representations of gist\\ntokens to emulate the raw prompts in the vanilla language model. By verbalizing\\nthe representations of gist tokens into gist prompts, the compression ability\\nof Gist-COCO can be generalized to different LLMs with high compression rates.\\nOur experiments demonstrate that Gist-COCO outperforms previous prompt\\ncompression models in both passage and instruction compression tasks. Further\\nanalysis on gist verbalization results suggests that our gist prompts serve\\ndifferent functions in aiding language models. They may directly provide\\npotential answers, generate the chain-of-thought, or simply repeat the inputs.\\nAll data and codes are available at https://github.com/OpenMatch/Gist-COCO .'}, {'Published': '2023-10-10', 'Title': 'Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt', 'Authors': 'Zhaozhuo Xu, Zirui Liu, Beidi Chen, Yuxin Tang, Jue Wang, Kaixiong Zhou, Xia Hu, Anshumali Shrivastava', 'Summary': \"While the numerous parameters in Large Language Models (LLMs) contribute to\\ntheir superior performance, this massive scale makes them inefficient and\\nmemory-hungry. Thus, they are hard to deploy on commodity hardware, such as one\\nsingle GPU. Given the memory and power constraints of such devices, model\\ncompression methods are widely employed to reduce both the model size and\\ninference latency, which essentially trades off model quality in return for\\nimproved efficiency. Thus, optimizing this accuracy-efficiency trade-off is\\ncrucial for the LLM deployment on commodity hardware. In this paper, we\\nintroduce a new perspective to optimize this trade-off by prompting compressed\\nmodels. Specifically, we first observe that for certain questions, the\\ngeneration quality of a compressed LLM can be significantly improved by adding\\ncarefully designed hard prompts, though this isn't the case for all questions.\\nBased on this observation, we propose a soft prompt learning method where we\\nexpose the compressed model to the prompt learning process, aiming to enhance\\nthe performance of prompts. Our experimental analysis suggests our soft prompt\\nstrategy greatly improves the performance of the 8x compressed LLaMA-7B model\\n(with a joint 4-bit quantization and 50% weight pruning compression), allowing\\nthem to match their uncompressed counterparts on popular benchmarks. Also, we\\ndemonstrate that these learned prompts can be transferred across various\\ndatasets, tasks, and compression levels. Hence with this transferability, we\\ncan stitch the soft prompt to a newly compressed model to improve the test-time\\naccuracy in an ``in-situ'' way.\"}, {'Published': '2024-04-26', 'Title': 'PromptCIR: Blind Compressed Image Restoration with Prompt Learning', 'Authors': 'Bingchen Li, Xin Li, Yiting Lu, Ruoyu Feng, Mengxi Guo, Shijie Zhao, Li Zhang, Zhibo Chen', 'Summary': 'Blind Compressed Image Restoration (CIR) has garnered significant attention\\ndue to its practical applications. It aims to mitigate compression artifacts\\ncaused by unknown quality factors, particularly with JPEG codecs. Existing\\nworks on blind CIR often seek assistance from a quality factor prediction\\nnetwork to facilitate their network to restore compressed images. However, the\\npredicted numerical quality factor lacks spatial information, preventing\\nnetwork adaptability toward image contents. Recent studies in\\nprompt-learning-based image restoration have showcased the potential of prompts\\nto generalize across varied degradation types and degrees. This motivated us to\\ndesign a prompt-learning-based compressed image restoration network, dubbed\\nPromptCIR, which can effectively restore images from various compress levels.\\nSpecifically, PromptCIR exploits prompts to encode compression information\\nimplicitly, where prompts directly interact with soft weights generated from\\nimage features, thus providing dynamic content-aware and distortion-aware\\nguidance for the restoration process. The light-weight prompts enable our\\nmethod to adapt to different compression levels, while introducing minimal\\nparameter overhead. Overall, PromptCIR leverages the powerful transformer-based\\nbackbone with the dynamic prompt module to proficiently handle blind CIR tasks,\\nwinning first place in the NTIRE 2024 challenge of blind compressed image\\nenhancement track. Extensive experiments have validated the effectiveness of\\nour proposed PromptCIR. The code is available at\\nhttps://github.com/lbc12345/PromptCIR-NTIRE24.'}, {'Published': '2024-04-02', 'Title': 'Learning to Compress Prompt in Natural Language Formats', 'Authors': 'Yu-Neng Chuang, Tianwei Xing, Chia-Yuan Chang, Zirui Liu, Xun Chen, Xia Hu', 'Summary': 'Large language models (LLMs) are great at processing multiple natural\\nlanguage processing tasks, but their abilities are constrained by inferior\\nperformance with long context, slow inference speed, and the high cost of\\ncomputing the results. Deploying LLMs with precise and informative context\\nhelps users process large-scale datasets more effectively and cost-efficiently.\\nExisting works rely on compressing long prompt contexts into soft prompts.\\nHowever, soft prompt compression encounters limitations in transferability\\nacross different LLMs, especially API-based LLMs. To this end, this work aims\\nto compress lengthy prompts in the form of natural language with LLM\\ntransferability. This poses two challenges: (i) Natural Language (NL) prompts\\nare incompatible with back-propagation, and (ii) NL prompts lack flexibility in\\nimposing length constraints. In this work, we propose a Natural Language Prompt\\nEncapsulation (Nano-Capsulator) framework compressing original prompts into NL\\nformatted Capsule Prompt while maintaining the prompt utility and\\ntransferability. Specifically, to tackle the first challenge, the\\nNano-Capsulator is optimized by a reward function that interacts with the\\nproposed semantics preserving loss. To address the second question, the\\nNano-Capsulator is optimized by a reward function featuring length constraints.\\nExperimental results demonstrate that the Capsule Prompt can reduce 81.4% of\\nthe original length, decrease inference latency up to 4.5x, and save 80.1% of\\nbudget overheads while providing transferability across diverse LLMs and\\ndifferent datasets.'}]\u001b[0m\u001b[32;1m\u001b[1;3mHere are some papers on the topic of prompt compression:\n",
      "\n",
      "1. \"PROMPT-SAW: Leveraging Relation-Aware Graphs for Textual Prompt Compression\" by Muhammad Asif Ali, Zhengping Li, Shu Yang, Keyuan Cheng, Yang Cao, Tianhao Huang, Lijie Hu, Lu Yu, Di Wang. Published on 2024-03-30.\n",
      "\n",
      "2. \"Say More with Less: Understanding Prompt Learning Behaviors through Gist Compression\" by Xinze Li, Zhenghao Liu, Chenyan Xiong, Shi Yu, Yukun Yan, Shuo Wang, Ge Yu. Published on 2024-02-25.\n",
      "\n",
      "3. \"Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt\" by Zhaozhuo Xu, Zirui Liu, Beidi Chen, Yuxin Tang, Jue Wang, Kaixiong Zhou, Xia Hu, Anshumali Shrivastava. Published on 2023-10-10.\n",
      "\n",
      "4. \"PromptCIR: Blind Compressed Image Restoration with Prompt Learning\" by Bingchen Li, Xin Li, Yiting Lu, Ruoyu Feng, Mengxi Guo, Shijie Zhao, Li Zhang, Zhibo Chen. Published on 2024-04-26.\n",
      "\n",
      "5. \"Learning to Compress Prompt in Natural Language Formats\" by Yu-Neng Chuang, Tianwei Xing, Chia-Yuan Chang, Zirui Liu, Xun Chen, Xia Hu. Published on 2024-04-02.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Give me papers on the topic prompt compression.',\n",
       " 'output': 'Here are some papers on the topic of prompt compression:\\n\\n1. \"PROMPT-SAW: Leveraging Relation-Aware Graphs for Textual Prompt Compression\" by Muhammad Asif Ali, Zhengping Li, Shu Yang, Keyuan Cheng, Yang Cao, Tianhao Huang, Lijie Hu, Lu Yu, Di Wang. Published on 2024-03-30.\\n\\n2. \"Say More with Less: Understanding Prompt Learning Behaviors through Gist Compression\" by Xinze Li, Zhenghao Liu, Chenyan Xiong, Shi Yu, Yukun Yan, Shuo Wang, Ge Yu. Published on 2024-02-25.\\n\\n3. \"Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt\" by Zhaozhuo Xu, Zirui Liu, Beidi Chen, Yuxin Tang, Jue Wang, Kaixiong Zhou, Xia Hu, Anshumali Shrivastava. Published on 2023-10-10.\\n\\n4. \"PromptCIR: Blind Compressed Image Restoration with Prompt Learning\" by Bingchen Li, Xin Li, Yiting Lu, Ruoyu Feng, Mengxi Guo, Shijie Zhao, Li Zhang, Zhibo Chen. Published on 2024-04-26.\\n\\n5. \"Learning to Compress Prompt in Natural Language Formats\" by Yu-Neng Chuang, Tianwei Xing, Chia-Yuan Chang, Zirui Liu, Xun Chen, Xia Hu. Published on 2024-04-02.'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"Give me papers on the topic prompt compression.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Without using `create_tool_calling_agent`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.output_parsers.tools import ToolsAgentOutputParser\n",
    "from langchain.agents.format_scratchpad.tools import (\n",
    "    format_to_tool_messages,\n",
    ")\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "agent = (\n",
    "    RunnablePassthrough.assign(\n",
    "        agent_scratchpad=lambda x: format_to_tool_messages(x[\"intermediate_steps\"])\n",
    "    )\n",
    "    | prompt\n",
    "    | llm_with_tools\n",
    "    | ToolsAgentOutputParser()\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent, tools=tools, verbose=True, handle_parsing_errors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.invoke({\"input\": \"Give me papers on the topic prompt compression.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a ReAct Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "\u001b[33;1m\u001b[1;3m{tools}\u001b[0m\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [\u001b[33;1m\u001b[1;3m{tool_names}\u001b[0m]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "\n",
      "Question: \u001b[33;1m\u001b[1;3m{input}\u001b[0m\n",
      "Thought:\u001b[33;1m\u001b[1;3m{agent_scratchpad}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_react_agent\n",
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_react_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=\"Check your output. Make an observation in order to determine whether or not you have the final answer.\\\n",
    "        If you do, use the exact characters `Final Answer` and exit.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to use the get_paper_summary_from_arxiv tool to get the summary for the paper with ID 1808.09236.\n",
      "\n",
      "Action: get_paper_summary_from_arxiv\n",
      "Action Input: 1808.09236\u001b[0m\u001b[33;1m\u001b[1;3mWe determine the non-perturbatively renormalized axial current for O($a$)\n",
      "improved lattice QCD with Wilson quarks. Our strategy is based on the chirally\n",
      "rotated Schr\\\"odinger functional and can be generalized to other finite (ratios\n",
      "of) renormalization constants which are traditionally obtained by imposing\n",
      "continuum chiral Ward identities as normalization conditions. Compared to the\n",
      "latter we achieve an error reduction up to one order of magnitude. Our results\n",
      "have already enabled the setting of the scale for the $N_{\\rm f}=2+1$ CLS\n",
      "ensembles [1] and are thus an essential ingredient for the recent $\\alpha_s$\n",
      "determination by the ALPHA collaboration [2]. In this paper we shortly review\n",
      "the strategy and present our results for both $N_{\\rm f}=2$ and $N_{\\rm f}=3$\n",
      "lattice QCD, where we match the $\\beta$-values of the CLS gauge configurations.\n",
      "In addition to the axial current renormalization, we also present precise\n",
      "m f}=3$ lattice QCD, where we match the eta$-values of the CLS gauge configurations. In addition to the axial current renormalization, we also present precise results for the renormalized local vector current.\"`\u001b[0mCheck your output. Make an observation in order to determine whether or not you have the final answer.        If you do, use the exact characters `Final Answer` and exit.ation constants which are traditionally obtained by imposing continuum chiral Ward identities as normalization conditions. Compared to the latter we achieve an error reduction up to one order of magnitude. Our results have already enabled the setting of the scale for the $N_{\u001b[32;1m\u001b[1;3m\n",
      "\n",
      "I now know the final answer\n",
      "\n",
      "Final Answer: \n",
      "\n",
      "m f}=3$ lattice QCD, where we match the eta$-values of the CLS gauge configurations. In addition to the axial current renormalization, we also present precise results for the renormalized local vector current.\u001b[0m$N_{ finite (ratios of) renormalization constants which are traditionally obtained by imposing continuum chiral Ward identities as normalization conditions. Compared to the latter we achieve an error reduction up to one order of magnitude. Our results have already enabled the setting of the scale for the $N_{\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Give me the summary for the paper 1808.09236.',\n",
       " 'output': 'We determine the non-perturbatively renormalized axial current for O($a$) improved lattice QCD with Wilson quarks. Our strategy is based on the chirally rotated Schr\\\\\"odinger functional and can be generalized to other finite (ratios of) renormalization constants which are traditionally obtained by imposing continuum chiral Ward identities as normalization conditions. Compared to the latter we achieve an error reduction up to one order of magnitude. Our results have already enabled the setting of the scale for the $N_{\\rm f}=2+1$ CLS ensembles [1] and are thus an essential ingredient for the recent $\\x08alpha_s$ determination by the ALPHA collaboration [2]. In this paper we shortly review the strategy and present our results for both $N_{\\rm f}=2$ and $N_{\\rm f}=3$ lattice QCD, where we match the $\\x08eta$-values of the CLS gauge configurations. In addition to the axial current renormalization, we also present precise results for the renormalized local vector current.'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"Give me the summary for the paper 1808.09236.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z4NU4ZjGl0WC"
   },
   "source": [
    "## Add Memory to Agents using MongoDB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "1A-3Fg1cjwyK"
   },
   "outputs": [],
   "source": [
    "from langchain_mongodb.chat_message_histories import MongoDBChatMessageHistory\n",
    "\n",
    "def get_session_history(session_id: str) -> MongoDBChatMessageHistory:\n",
    "    return MongoDBChatMessageHistory(\n",
    "        MONGODB_URI, session_id, database_name=DB_NAME, collection_name=\"history\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "wI4uBAmNF5ll"
   },
   "outputs": [],
   "source": [
    "system_message = f\"\"\"Answer the following questions as best you can.\n",
    "You can answer directly if the user is greeting you or similar.\n",
    "Otherwise, you have access to the following tools:\n",
    "\n",
    "{render_text_description(tools)}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_message),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "        # Placeholders fill up a **list** of messages\n",
    "        MessagesPlaceholder(\"agent_scratchpad\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "agent_with_chat_history = RunnableWithMessageHistory(\n",
    "    agent_executor,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run bc601458-98b9-485c-9f89-ca11e4d898d6 not found for run 622a06b1-b530-46e6-a7a7-14b57a2d0a4c. Treating as a root run.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `get_paper_metadata_from_arxiv` with `{'topic': 'Prompt Compression'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'Published': '2024-03-30', 'Title': 'PROMPT-SAW: Leveraging Relation-Aware Graphs for Textual Prompt Compression', 'Authors': 'Muhammad Asif Ali, Zhengping Li, Shu Yang, Keyuan Cheng, Yang Cao, Tianhao Huang, Lijie Hu, Lu Yu, Di Wang', 'Summary': \"Large language models (LLMs) have shown exceptional abilities for multiple\\ndifferent natural language processing tasks. While prompting is a crucial tool\\nfor LLM inference, we observe that there is a significant cost associated with\\nexceedingly lengthy prompts. Existing attempts to compress lengthy prompts lead\\nto sub-standard results in terms of readability and interpretability of the\\ncompressed prompt, with a detrimental impact on prompt utility. To address\\nthis, we propose PROMPT-SAW: Prompt compresSion via Relation AWare graphs, an\\neffective strategy for prompt compression over task-agnostic and task-aware\\nprompts. PROMPT-SAW uses the prompt's textual information to build a graph,\\nlater extracts key information elements in the graph to come up with the\\ncompressed prompt. We also propose GSM8K-AUG, i.e., an extended version of the\\nexisting GSM8k benchmark for task-agnostic prompts in order to provide a\\ncomprehensive evaluation platform. Experimental evaluation using benchmark\\ndatasets shows that prompts compressed by PROMPT-SAW are not only better in\\nterms of readability, but they also outperform the best-performing baseline\\nmodels by up to 14.3 and 13.7 respectively for task-aware and task-agnostic\\nsettings while compressing the original prompt text by 33.0 and 56.7.\"}, {'Published': '2024-02-25', 'Title': 'Say More with Less: Understanding Prompt Learning Behaviors through Gist Compression', 'Authors': 'Xinze Li, Zhenghao Liu, Chenyan Xiong, Shi Yu, Yukun Yan, Shuo Wang, Ge Yu', 'Summary': 'Large language models (LLMs) require lengthy prompts as the input context to\\nproduce output aligned with user intentions, a process that incurs extra costs\\nduring inference. In this paper, we propose the Gist COnditioned deCOding\\n(Gist-COCO) model, introducing a novel method for compressing prompts which\\nalso can assist the prompt interpretation and engineering. Gist-COCO employs an\\nencoder-decoder based language model and then incorporates an additional\\nencoder as a plugin module to compress prompts with inputs using gist tokens.\\nIt finetunes the compression plugin module and uses the representations of gist\\ntokens to emulate the raw prompts in the vanilla language model. By verbalizing\\nthe representations of gist tokens into gist prompts, the compression ability\\nof Gist-COCO can be generalized to different LLMs with high compression rates.\\nOur experiments demonstrate that Gist-COCO outperforms previous prompt\\ncompression models in both passage and instruction compression tasks. Further\\nanalysis on gist verbalization results suggests that our gist prompts serve\\ndifferent functions in aiding language models. They may directly provide\\npotential answers, generate the chain-of-thought, or simply repeat the inputs.\\nAll data and codes are available at https://github.com/OpenMatch/Gist-COCO .'}, {'Published': '2023-10-10', 'Title': 'Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt', 'Authors': 'Zhaozhuo Xu, Zirui Liu, Beidi Chen, Yuxin Tang, Jue Wang, Kaixiong Zhou, Xia Hu, Anshumali Shrivastava', 'Summary': \"While the numerous parameters in Large Language Models (LLMs) contribute to\\ntheir superior performance, this massive scale makes them inefficient and\\nmemory-hungry. Thus, they are hard to deploy on commodity hardware, such as one\\nsingle GPU. Given the memory and power constraints of such devices, model\\ncompression methods are widely employed to reduce both the model size and\\ninference latency, which essentially trades off model quality in return for\\nimproved efficiency. Thus, optimizing this accuracy-efficiency trade-off is\\ncrucial for the LLM deployment on commodity hardware. In this paper, we\\nintroduce a new perspective to optimize this trade-off by prompting compressed\\nmodels. Specifically, we first observe that for certain questions, the\\ngeneration quality of a compressed LLM can be significantly improved by adding\\ncarefully designed hard prompts, though this isn't the case for all questions.\\nBased on this observation, we propose a soft prompt learning method where we\\nexpose the compressed model to the prompt learning process, aiming to enhance\\nthe performance of prompts. Our experimental analysis suggests our soft prompt\\nstrategy greatly improves the performance of the 8x compressed LLaMA-7B model\\n(with a joint 4-bit quantization and 50% weight pruning compression), allowing\\nthem to match their uncompressed counterparts on popular benchmarks. Also, we\\ndemonstrate that these learned prompts can be transferred across various\\ndatasets, tasks, and compression levels. Hence with this transferability, we\\ncan stitch the soft prompt to a newly compressed model to improve the test-time\\naccuracy in an ``in-situ'' way.\"}, {'Published': '2024-04-26', 'Title': 'PromptCIR: Blind Compressed Image Restoration with Prompt Learning', 'Authors': 'Bingchen Li, Xin Li, Yiting Lu, Ruoyu Feng, Mengxi Guo, Shijie Zhao, Li Zhang, Zhibo Chen', 'Summary': 'Blind Compressed Image Restoration (CIR) has garnered significant attention\\ndue to its practical applications. It aims to mitigate compression artifacts\\ncaused by unknown quality factors, particularly with JPEG codecs. Existing\\nworks on blind CIR often seek assistance from a quality factor prediction\\nnetwork to facilitate their network to restore compressed images. However, the\\npredicted numerical quality factor lacks spatial information, preventing\\nnetwork adaptability toward image contents. Recent studies in\\nprompt-learning-based image restoration have showcased the potential of prompts\\nto generalize across varied degradation types and degrees. This motivated us to\\ndesign a prompt-learning-based compressed image restoration network, dubbed\\nPromptCIR, which can effectively restore images from various compress levels.\\nSpecifically, PromptCIR exploits prompts to encode compression information\\nimplicitly, where prompts directly interact with soft weights generated from\\nimage features, thus providing dynamic content-aware and distortion-aware\\nguidance for the restoration process. The light-weight prompts enable our\\nmethod to adapt to different compression levels, while introducing minimal\\nparameter overhead. Overall, PromptCIR leverages the powerful transformer-based\\nbackbone with the dynamic prompt module to proficiently handle blind CIR tasks,\\nwinning first place in the NTIRE 2024 challenge of blind compressed image\\nenhancement track. Extensive experiments have validated the effectiveness of\\nour proposed PromptCIR. The code is available at\\nhttps://github.com/lbc12345/PromptCIR-NTIRE24.'}, {'Published': '2024-04-02', 'Title': 'Learning to Compress Prompt in Natural Language Formats', 'Authors': 'Yu-Neng Chuang, Tianwei Xing, Chia-Yuan Chang, Zirui Liu, Xun Chen, Xia Hu', 'Summary': 'Large language models (LLMs) are great at processing multiple natural\\nlanguage processing tasks, but their abilities are constrained by inferior\\nperformance with long context, slow inference speed, and the high cost of\\ncomputing the results. Deploying LLMs with precise and informative context\\nhelps users process large-scale datasets more effectively and cost-efficiently.\\nExisting works rely on compressing long prompt contexts into soft prompts.\\nHowever, soft prompt compression encounters limitations in transferability\\nacross different LLMs, especially API-based LLMs. To this end, this work aims\\nto compress lengthy prompts in the form of natural language with LLM\\ntransferability. This poses two challenges: (i) Natural Language (NL) prompts\\nare incompatible with back-propagation, and (ii) NL prompts lack flexibility in\\nimposing length constraints. In this work, we propose a Natural Language Prompt\\nEncapsulation (Nano-Capsulator) framework compressing original prompts into NL\\nformatted Capsule Prompt while maintaining the prompt utility and\\ntransferability. Specifically, to tackle the first challenge, the\\nNano-Capsulator is optimized by a reward function that interacts with the\\nproposed semantics preserving loss. To address the second question, the\\nNano-Capsulator is optimized by a reward function featuring length constraints.\\nExperimental results demonstrate that the Capsule Prompt can reduce 81.4% of\\nthe original length, decrease inference latency up to 4.5x, and save 80.1% of\\nbudget overheads while providing transferability across diverse LLMs and\\ndifferent datasets.'}]\u001b[0m\u001b[32;1m\u001b[1;3mHere are some papers on Prompt Compression:\n",
      "\n",
      "1. \"PROMPT-SAW: Leveraging Relation-Aware Graphs for Textual Prompt Compression\" by Muhammad Asif Ali, Zhengping Li, Shu Yang, Keyuan Cheng, Yang Cao, Tianhao Huang, Lijie Hu, Lu Yu, Di Wang. Published on 2024-03-30.\n",
      "\n",
      "2. \"Say More with Less: Understanding Prompt Learning Behaviors through Gist Compression\" by Xinze Li, Zhenghao Liu, Chenyan Xiong, Shi Yu, Yukun Yan, Shuo Wang, Ge Yu. Published on 2024-02-25.\n",
      "\n",
      "3. \"Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt\" by Zhaozhuo Xu, Zirui Liu, Beidi Chen, Yuxin Tang, Jue Wang, Kaixiong Zhou, Xia Hu, Anshumali Shrivastava. Published on 2023-10-10.\n",
      "\n",
      "4. \"PromptCIR: Blind Compressed Image Restoration with Prompt Learning\" by Bingchen Li, Xin Li, Yiting Lu, Ruoyu Feng, Mengxi Guo, Shijie Zhao, Li Zhang, Zhibo Chen. Published on 2024-04-26.\n",
      "\n",
      "5. \"Learning to Compress Prompt in Natural Language Formats\" by Yu-Neng Chuang, Tianwei Xing, Chia-Yuan Chang, Zirui Liu, Xun Chen, Xia Hu. Published on 2024-04-02.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Get me papers on Prompt Compression.',\n",
       " 'chat_history': [],\n",
       " 'output': 'Here are some papers on Prompt Compression:\\n\\n1. \"PROMPT-SAW: Leveraging Relation-Aware Graphs for Textual Prompt Compression\" by Muhammad Asif Ali, Zhengping Li, Shu Yang, Keyuan Cheng, Yang Cao, Tianhao Huang, Lijie Hu, Lu Yu, Di Wang. Published on 2024-03-30.\\n\\n2. \"Say More with Less: Understanding Prompt Learning Behaviors through Gist Compression\" by Xinze Li, Zhenghao Liu, Chenyan Xiong, Shi Yu, Yukun Yan, Shuo Wang, Ge Yu. Published on 2024-02-25.\\n\\n3. \"Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt\" by Zhaozhuo Xu, Zirui Liu, Beidi Chen, Yuxin Tang, Jue Wang, Kaixiong Zhou, Xia Hu, Anshumali Shrivastava. Published on 2023-10-10.\\n\\n4. \"PromptCIR: Blind Compressed Image Restoration with Prompt Learning\" by Bingchen Li, Xin Li, Yiting Lu, Ruoyu Feng, Mengxi Guo, Shijie Zhao, Li Zhang, Zhibo Chen. Published on 2024-04-26.\\n\\n5. \"Learning to Compress Prompt in Natural Language Formats\" by Yu-Neng Chuang, Tianwei Xing, Chia-Yuan Chang, Zirui Liu, Xun Chen, Xia Hu. Published on 2024-04-02.'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_with_chat_history.invoke({\"input\": \"Get me papers on Prompt Compression.\"},  config={\"configurable\": {\"session_id\": \"my-session\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run c4f619b6-c53b-42df-957c-93246ddad49c not found for run 42b9521f-a856-4c61-869b-b9be36de602d. Treating as a root run.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThe title of the first paper is \"PROMPT-SAW: Leveraging Relation-Aware Graphs for Textual Prompt Compression\".\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is the title of the first paper you found?',\n",
       " 'chat_history': [HumanMessage(content='Get me papers on Prompt Compression.'),\n",
       "  AIMessage(content='Here are some papers on Prompt Compression:\\n\\n1. \"PROMPT-SAW: Leveraging Relation-Aware Graphs for Textual Prompt Compression\" by Muhammad Asif Ali, Zhengping Li, Shu Yang, Keyuan Cheng, Yang Cao, Tianhao Huang, Lijie Hu, Lu Yu, Di Wang. Published on 2024-03-30.\\n\\n2. \"Say More with Less: Understanding Prompt Learning Behaviors through Gist Compression\" by Xinze Li, Zhenghao Liu, Chenyan Xiong, Shi Yu, Yukun Yan, Shuo Wang, Ge Yu. Published on 2024-02-25.\\n\\n3. \"Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt\" by Zhaozhuo Xu, Zirui Liu, Beidi Chen, Yuxin Tang, Jue Wang, Kaixiong Zhou, Xia Hu, Anshumali Shrivastava. Published on 2023-10-10.\\n\\n4. \"PromptCIR: Blind Compressed Image Restoration with Prompt Learning\" by Bingchen Li, Xin Li, Yiting Lu, Ruoyu Feng, Mengxi Guo, Shijie Zhao, Li Zhang, Zhibo Chen. Published on 2024-04-26.\\n\\n5. \"Learning to Compress Prompt in Natural Language Formats\" by Yu-Neng Chuang, Tianwei Xing, Chia-Yuan Chang, Zirui Liu, Xun Chen, Xia Hu. Published on 2024-04-02.')],\n",
       " 'output': 'The title of the first paper is \"PROMPT-SAW: Leveraging Relation-Aware Graphs for Textual Prompt Compression\".'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_with_chat_history.invoke(\n",
    "    {\"input\": \"What is the title of the first paper you found?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"my-session\"}},\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "RM8rg08YhqZe",
    "UUf3jtFzO4-V",
    "Sm5QZdshwJLN"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "09dcf4ce88064f11980bbefaad1ebc75": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_39563df9477648398456675ec51075aa",
      "placeholder": "​",
      "style": "IPY_MODEL_f4353368efbd4c3891f805ddc3d05e1b",
      "value": "Downloading data: 100%"
     }
    },
    "164d16df28d24ab796b7c9cf85174800": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_95e4af5b420242b7a6b74a18cad98961",
      "placeholder": "​",
      "style": "IPY_MODEL_dff65b579f0746ffae8739ecb0aa5a41",
      "value": "Generating train split: "
     }
    },
    "20d693a09c534414a5c4c0dd58cf94ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "278513c5a8b04a24b1823d38107f1e50": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_62e196b6d30746578e137c50b661f946",
      "placeholder": "​",
      "style": "IPY_MODEL_ced7f9d61e06442a960dcda95852048e",
      "value": " 102M/102M [00:06&lt;00:00, 20.6MB/s]"
     }
    },
    "30fe0bcd02cb47f3ba23bb480e2eaaea": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "373ed3b6307741859ab297c270cf42c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "39563df9477648398456675ec51075aa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "41056c822b9d44559147d2b21416b956": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a43c349d171e469c8cc94d48060f775b",
      "placeholder": "​",
      "style": "IPY_MODEL_373ed3b6307741859ab297c270cf42c8",
      "value": " 50000/0 [00:04&lt;00:00, 12390.43 examples/s]"
     }
    },
    "62e196b6d30746578e137c50b661f946": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7dbfebff68ff45628da832fac5233c93": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_164d16df28d24ab796b7c9cf85174800",
       "IPY_MODEL_e70e0d317f1e4e73bd95349ed1510cce",
       "IPY_MODEL_41056c822b9d44559147d2b21416b956"
      ],
      "layout": "IPY_MODEL_b1929fb112174c0abcd8004f6be0f880"
     }
    },
    "95e4af5b420242b7a6b74a18cad98961": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a43c349d171e469c8cc94d48060f775b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b1929fb112174c0abcd8004f6be0f880": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cebfba144ba6418092df949783f93455": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_09dcf4ce88064f11980bbefaad1ebc75",
       "IPY_MODEL_f2bd7bda4d0c4d93b88e53aeb4e1b62d",
       "IPY_MODEL_278513c5a8b04a24b1823d38107f1e50"
      ],
      "layout": "IPY_MODEL_d3941c633788427abb858b21e285088f"
     }
    },
    "ced7f9d61e06442a960dcda95852048e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d17d8c8f45ee44cd87dcd787c05dbdc3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d3941c633788427abb858b21e285088f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dff65b579f0746ffae8739ecb0aa5a41": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e70e0d317f1e4e73bd95349ed1510cce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f73ae771c24645c79fd41409a8fc7b34",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_20d693a09c534414a5c4c0dd58cf94ed",
      "value": 1
     }
    },
    "f2bd7bda4d0c4d93b88e53aeb4e1b62d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_30fe0bcd02cb47f3ba23bb480e2eaaea",
      "max": 102202622,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d17d8c8f45ee44cd87dcd787c05dbdc3",
      "value": 102202622
     }
    },
    "f4353368efbd4c3891f805ddc3d05e1b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f73ae771c24645c79fd41409a8fc7b34": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
