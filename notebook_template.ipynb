{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mongodb-developer/GenAI-Showcase/blob/main/notebooks/agents/agent_fireworks_ai_langchain_mongodb.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3kMALXaMv-MS"
   },
   "source": [
    "## Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cxTXczeTghzU",
    "outputId": "ae3a81b2-cba6-42fc-f593-8646bff77b14"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (0.1.19)\n",
      "Requirement already satisfied: langchain-mongodb in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (0.1.3)\n",
      "Requirement already satisfied: langchain-huggingface in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (0.0.1)\n",
      "Requirement already satisfied: arxiv in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (2.1.0)\n",
      "Requirement already satisfied: pymupdf in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (1.24.4)\n",
      "Requirement already satisfied: datasets in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (2.18.0)\n",
      "Requirement already satisfied: pymongo in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (4.6.3)\n",
      "Requirement already satisfied: tqdm in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (4.66.2)\n",
      "Requirement already satisfied: langsmith in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (0.1.23)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from langchain) (2.0.28)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from langchain) (3.9.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from langchain) (0.6.4)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.38 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from langchain) (0.0.38)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.52 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from langchain) (0.1.52)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from langchain) (0.0.1)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from langchain) (2.6.3)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from langchain-huggingface) (0.23.2)\n",
      "Requirement already satisfied: sentence-transformers>=2.6.0 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from langchain-huggingface) (3.0.0)\n",
      "Requirement already satisfied: text-generation<0.8.0,>=0.7.0 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from langchain-huggingface) (0.7.0)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from langchain-huggingface) (0.19.1)\n",
      "Requirement already satisfied: transformers>=4.39.0 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from langchain-huggingface) (4.41.1)\n",
      "Requirement already satisfied: feedparser==6.0.10 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from arxiv) (6.0.10)\n",
      "Requirement already satisfied: sgmllib3k in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from feedparser==6.0.10->arxiv) (1.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
      "Requirement already satisfied: PyMuPDFb==1.24.3 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from pymupdf) (1.24.3)\n",
      "Requirement already satisfied: filelock in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from datasets) (15.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from datasets) (2.2.1)\n",
      "Requirement already satisfied: xxhash in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from datasets) (2023.10.0)\n",
      "Requirement already satisfied: packaging in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from pymongo) (2.6.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from langsmith) (3.9.15)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (4.10.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from langchain-core<0.2.0,>=0.1.52->langchain) (1.33)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from pydantic<3,>=1->langchain) (2.16.3)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (2.2.1)\n",
      "Requirement already satisfied: scikit-learn in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.4.1.post1)\n",
      "Requirement already satisfied: scipy in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.12.0)\n",
      "Requirement already satisfied: Pillow in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (10.2.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from transformers>=4.39.0->langchain-huggingface) (2023.12.25)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from transformers>=4.39.0->langchain-huggingface) (0.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.52->langchain) (2.4)\n",
      "Requirement already satisfied: six>=1.5 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: sympy in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.1.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (3.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/apoorva.joshi/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from sympy->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain langchain-mongodb langchain-huggingface arxiv pymupdf datasets pymongo tqdm langsmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your Langsmith API key: ········\n"
     ]
    }
   ],
   "source": [
    "LANGCHAIN_API_KEY = getpass.getpass(\"Enter your Langsmith API key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!export LANGCHAIN_TRACING_V2=true\n",
    "!export LANGCHAIN_API_KEY=LANGCHAIN_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RM8rg08YhqZe"
   },
   "source": [
    "## Set Evironment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "oXLWCWEghuOX"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your MongoDB connection string: ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "\n",
    "MONGODB_URI = getpass.getpass(\"Enter your MongoDB connection string:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UUf3jtFzO4-V"
   },
   "source": [
    "## Data Ingestion into MongoDB Vector Database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "referenced_widgets": [
      "cebfba144ba6418092df949783f93455",
      "09dcf4ce88064f11980bbefaad1ebc75",
      "f2bd7bda4d0c4d93b88e53aeb4e1b62d",
      "278513c5a8b04a24b1823d38107f1e50",
      "d3941c633788427abb858b21e285088f",
      "39563df9477648398456675ec51075aa",
      "f4353368efbd4c3891f805ddc3d05e1b",
      "30fe0bcd02cb47f3ba23bb480e2eaaea",
      "d17d8c8f45ee44cd87dcd787c05dbdc3",
      "62e196b6d30746578e137c50b661f946",
      "ced7f9d61e06442a960dcda95852048e",
      "7dbfebff68ff45628da832fac5233c93",
      "164d16df28d24ab796b7c9cf85174800",
      "e70e0d317f1e4e73bd95349ed1510cce",
      "41056c822b9d44559147d2b21416b956",
      "b1929fb112174c0abcd8004f6be0f880",
      "95e4af5b420242b7a6b74a18cad98961",
      "dff65b579f0746ffae8739ecb0aa5a41",
      "f73ae771c24645c79fd41409a8fc7b34",
      "20d693a09c534414a5c4c0dd58cf94ed",
      "a43c349d171e469c8cc94d48060f775b",
      "373ed3b6307741859ab297c270cf42c8"
     ]
    },
    "id": "pq4SA6r7O30i",
    "outputId": "904f4112-79fb-45cc-954b-d2b818cb2748"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a58bb1c92d6499c91c644693d36a876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/31.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fb3ea0c1f60406e88cdc92ac0c67371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/15.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b9c94c471cb4d2884ed842511812785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"mongodb-eai/arxiv-embeddings\")\n",
    "dataset_df = pd.DataFrame(data[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "o2gHwRjMfJlO"
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "# Initialize MongoDB python client\n",
    "client = MongoClient(MONGODB_URI)\n",
    "\n",
    "DB_NAME = \"agent_demo\"\n",
    "COLLECTION_NAME = \"knowledge\"\n",
    "ATLAS_VECTOR_SEARCH_INDEX_NAME = \"vector_index\"\n",
    "collection = client[DB_NAME][COLLECTION_NAME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zJkyy9UbffZT",
    "outputId": "c6f78ea3-fc93-4d57-95eb-98cea5bf15d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data ingestion into MongoDB completed\n"
     ]
    }
   ],
   "source": [
    "# Delete any existing records in the collection\n",
    "collection.delete_many({})\n",
    "\n",
    "# Data Ingestion\n",
    "records = dataset_df.to_dict('records')\n",
    "collection.insert_many(records)\n",
    "\n",
    "print(\"Data ingestion into MongoDB completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6S1Cz9dtGPwL"
   },
   "source": [
    "## Create Vector Search Index Defintion\n",
    "\n",
    "```\n",
    "{\n",
    "  \"fields\": [\n",
    "    {\n",
    "      \"type\": \"vector\",\n",
    "      \"path\": \"embedding\",\n",
    "      \"numDimensions\": 1024,\n",
    "      \"similarity\": \"cosine\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sm5QZdshwJLN"
   },
   "source": [
    "## Configure Chat Completion LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "V4ztCMCtgme_"
   },
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"phi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create MongoDB Vector Store Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_mongodb import MongoDBAtlasVectorSearch\n",
    "\n",
    "embedding_model = OllamaEmbeddings(model= \"mxbai-embed-large\")\n",
    "\n",
    "# Vector Store Creation\n",
    "vector_store = MongoDBAtlasVectorSearch.from_connection_string(\n",
    "    connection_string=MONGODB_URI,\n",
    "    namespace=DB_NAME + \".\" + COLLECTION_NAME,\n",
    "    embedding= embedding_model,\n",
    "    index_name=ATLAS_VECTOR_SEARCH_INDEX_NAME,\n",
    "    text_key=\"abstract\"\n",
    ")\n",
    "\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZfheX5FiIhU"
   },
   "source": [
    "## Agent Tools Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "3eufR9H8gopU"
   },
   "outputs": [],
   "source": [
    "from langchain.agents import tool, Tool\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "from langchain_community.utilities import ArxivAPIWrapper\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Custom Tool Definiton\n",
    "@tool\n",
    "def get_paper_metadata_from_arxiv(topic: str) -> list:\n",
    "    \"\"\"\n",
    "    Fetch and return paper metadata for 10 arxiv papers matching the given topic, for example: Retrieval Augmented Generation.\n",
    "    \n",
    "    Args:\n",
    "    topic (str): The topic to find papers for on arXiv.\n",
    "    \n",
    "    Returns:\n",
    "    list: Metadata about the papers matching the topic.\n",
    "    \"\"\"\n",
    "    docs = ArxivLoader(query=topic, top_k_results = 5, load_max_docs=20).load()\n",
    "    # Extract just the metadata from each document\n",
    "    metadata = [doc.metadata for doc in docs]\n",
    "    return metadata\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_paper_summary_from_arxiv(id: str) -> list:\n",
    "    \"\"\"\n",
    "    Fetch and return the summary for a single research paper from arXiv given the paper ID, for example: 1605.08386.\n",
    "    \n",
    "    Args:\n",
    "    id (str): The paper ID.\n",
    "    \n",
    "    Returns:\n",
    "    str: Summary of the paper.\n",
    "    \"\"\"\n",
    "    doc = ArxivLoader(query=id, load_max_docs=1).get_summaries_as_docs()\n",
    "    if len(doc) == 0:\n",
    "        return \"No summary found for this paper.\"\n",
    "    return doc[0].page_content\n",
    "\n",
    "\n",
    "@tool\n",
    "def answer_questions_about_topics(query: str) -> list:\n",
    "    \"\"\"\n",
    "    Answer questions about a given topic based on information in the knowledge base.\n",
    "    \n",
    "    Args:\n",
    "    query (str): User query about a topic.\n",
    "    \n",
    "    Returns:\n",
    "    str: Information about the topic.\n",
    "    \"\"\"\n",
    "    retrieve = {\"context\": retriever | (lambda docs: \"\\n\\n\".join([d.page_content for d in docs])), \"question\": RunnablePassthrough()}\n",
    "    template = \"\"\"Answer the question based only on the following context. If no context is provided, say I do not know: \\\n",
    "    {context}\n",
    "    \n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "    # Defining the chat prompt\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    # Parse output as a string\n",
    "    parse_output = StrOutputParser()\n",
    "    \n",
    "    # Retrieval chain \n",
    "    retrieval_chain = (\n",
    "        retrieve\n",
    "        | prompt\n",
    "        | llm\n",
    "        | parse_output\n",
    "    )\n",
    "\n",
    "    answer = retrieval_chain.invoke(query)\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Published': '2022-02-13',\n",
       "  'Title': 'A Survey on Retrieval-Augmented Text Generation',\n",
       "  'Authors': 'Huayang Li, Yixuan Su, Deng Cai, Yan Wang, Lemao Liu',\n",
       "  'Summary': 'Recently, retrieval-augmented text generation attracted increasing attention\\nof the computational linguistics community. Compared with conventional\\ngeneration models, retrieval-augmented text generation has remarkable\\nadvantages and particularly has achieved state-of-the-art performance in many\\nNLP tasks. This paper aims to conduct a survey about retrieval-augmented text\\ngeneration. It firstly highlights the generic paradigm of retrieval-augmented\\ngeneration, and then it reviews notable approaches according to different tasks\\nincluding dialogue response generation, machine translation, and other\\ngeneration tasks. Finally, it points out some important directions on top of\\nrecent methods to facilitate future research.'},\n",
       " {'Published': '2024-05-12',\n",
       "  'Title': 'DuetRAG: Collaborative Retrieval-Augmented Generation',\n",
       "  'Authors': 'Dian Jiao, Li Cai, Jingsheng Huang, Wenqiao Zhang, Siliang Tang, Yueting Zhuang',\n",
       "  'Summary': \"Retrieval-Augmented Generation (RAG) methods augment the input of Large\\nLanguage Models (LLMs) with relevant retrieved passages, reducing factual\\nerrors in knowledge-intensive tasks. However, contemporary RAG approaches\\nsuffer from irrelevant knowledge retrieval issues in complex domain questions\\n(e.g., HotPot QA) due to the lack of corresponding domain knowledge, leading to\\nlow-quality generations. To address this issue, we propose a novel\\nCollaborative Retrieval-Augmented Generation framework, DuetRAG. Our\\nbootstrapping philosophy is to simultaneously integrate the domain fintuning\\nand RAG models to improve the knowledge retrieval quality, thereby enhancing\\ngeneration quality. Finally, we demonstrate DuetRAG' s matches with expert\\nhuman researchers on HotPot QA.\"},\n",
       " {'Published': '2023-12-09',\n",
       "  'Title': 'Context Tuning for Retrieval Augmented Generation',\n",
       "  'Authors': 'Raviteja Anantha, Tharun Bethi, Danil Vodianik, Srinivas Chappidi',\n",
       "  'Summary': \"Large language models (LLMs) have the remarkable ability to solve new tasks\\nwith just a few examples, but they need access to the right tools. Retrieval\\nAugmented Generation (RAG) addresses this problem by retrieving a list of\\nrelevant tools for a given task. However, RAG's tool retrieval step requires\\nall the required information to be explicitly present in the query. This is a\\nlimitation, as semantic search, the widely adopted tool retrieval method, can\\nfail when the query is incomplete or lacks context. To address this limitation,\\nwe propose Context Tuning for RAG, which employs a smart context retrieval\\nsystem to fetch relevant information that improves both tool retrieval and plan\\ngeneration. Our lightweight context retrieval model uses numerical,\\ncategorical, and habitual usage signals to retrieve and rank context items. Our\\nempirical results demonstrate that context tuning significantly enhances\\nsemantic search, achieving a 3.5-fold and 1.5-fold improvement in Recall@K for\\ncontext retrieval and tool retrieval tasks respectively, and resulting in an\\n11.6% increase in LLM-based planner accuracy. Additionally, we show that our\\nproposed lightweight model using Reciprocal Rank Fusion (RRF) with LambdaMART\\noutperforms GPT-4 based retrieval. Moreover, we observe context augmentation at\\nplan generation, even after tool retrieval, reduces hallucination.\"},\n",
       " {'Published': '2024-02-16',\n",
       "  'Title': 'Corrective Retrieval Augmented Generation',\n",
       "  'Authors': 'Shi-Qi Yan, Jia-Chen Gu, Yun Zhu, Zhen-Hua Ling',\n",
       "  'Summary': 'Large language models (LLMs) inevitably exhibit hallucinations since the\\naccuracy of generated texts cannot be secured solely by the parametric\\nknowledge they encapsulate. Although retrieval-augmented generation (RAG) is a\\npracticable complement to LLMs, it relies heavily on the relevance of retrieved\\ndocuments, raising concerns about how the model behaves if retrieval goes\\nwrong. To this end, we propose the Corrective Retrieval Augmented Generation\\n(CRAG) to improve the robustness of generation. Specifically, a lightweight\\nretrieval evaluator is designed to assess the overall quality of retrieved\\ndocuments for a query, returning a confidence degree based on which different\\nknowledge retrieval actions can be triggered. Since retrieval from static and\\nlimited corpora can only return sub-optimal documents, large-scale web searches\\nare utilized as an extension for augmenting the retrieval results. Besides, a\\ndecompose-then-recompose algorithm is designed for retrieved documents to\\nselectively focus on key information and filter out irrelevant information in\\nthem. CRAG is plug-and-play and can be seamlessly coupled with various\\nRAG-based approaches. Experiments on four datasets covering short- and\\nlong-form generation tasks show that CRAG can significantly improve the\\nperformance of RAG-based approaches.'},\n",
       " {'Published': '2023-05-27',\n",
       "  'Title': 'Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In',\n",
       "  'Authors': 'Zichun Yu, Chenyan Xiong, Shi Yu, Zhiyuan Liu',\n",
       "  'Summary': \"Retrieval augmentation can aid language models (LMs) in knowledge-intensive\\ntasks by supplying them with external information. Prior works on retrieval\\naugmentation usually jointly fine-tune the retriever and the LM, making them\\nclosely coupled. In this paper, we explore the scheme of generic retrieval\\nplug-in: the retriever is to assist target LMs that may not be known beforehand\\nor are unable to be fine-tuned together. To retrieve useful documents for\\nunseen target LMs, we propose augmentation-adapted retriever (AAR), which\\nlearns LM's preferences obtained from a known source LM. Experiments on the\\nMMLU and PopQA datasets demonstrate that our AAR trained with a small source LM\\nis able to significantly improve the zero-shot generalization of larger target\\nLMs ranging from 250M Flan-T5 to 175B InstructGPT. Further analysis indicates\\nthat the preferences of different LMs overlap, enabling AAR trained with a\\nsingle source LM to serve as a generic plug-in for various target LMs. Our code\\nis open-sourced at https://github.com/OpenMatch/Augmentation-Adapted-Retriever.\"},\n",
       " {'Published': '2024-03-05',\n",
       "  'Title': 'Reliable, Adaptable, and Attributable Language Models with Retrieval',\n",
       "  'Authors': 'Akari Asai, Zexuan Zhong, Danqi Chen, Pang Wei Koh, Luke Zettlemoyer, Hannaneh Hajishirzi, Wen-tau Yih',\n",
       "  'Summary': 'Parametric language models (LMs), which are trained on vast amounts of web\\ndata, exhibit remarkable flexibility and capability. However, they still face\\npractical challenges such as hallucinations, difficulty in adapting to new data\\ndistributions, and a lack of verifiability. In this position paper, we advocate\\nfor retrieval-augmented LMs to replace parametric LMs as the next generation of\\nLMs. By incorporating large-scale datastores during inference,\\nretrieval-augmented LMs can be more reliable, adaptable, and attributable.\\nDespite their potential, retrieval-augmented LMs have yet to be widely adopted\\ndue to several obstacles: specifically, current retrieval-augmented LMs\\nstruggle to leverage helpful text beyond knowledge-intensive tasks such as\\nquestion answering, have limited interaction between retrieval and LM\\ncomponents, and lack the infrastructure for scaling. To address these, we\\npropose a roadmap for developing general-purpose retrieval-augmented LMs. This\\ninvolves a reconsideration of datastores and retrievers, the exploration of\\npipelines with improved retriever-LM interaction, and significant investment in\\ninfrastructure for efficient training and inference.'},\n",
       " {'Published': '2023-02-07',\n",
       "  'Title': 'Augmenting Zero-Shot Dense Retrievers with Plug-in Mixture-of-Memories',\n",
       "  'Authors': 'Suyu Ge, Chenyan Xiong, Corby Rosset, Arnold Overwijk, Jiawei Han, Paul Bennett',\n",
       "  'Summary': 'In this paper we improve the zero-shot generalization ability of language\\nmodels via Mixture-Of-Memory Augmentation (MoMA), a mechanism that retrieves\\naugmentation documents from multiple information corpora (\"external memories\"),\\nwith the option to \"plug in\" new memory at inference time. We develop a joint\\nlearning mechanism that trains the augmentation component with latent labels\\nderived from the end retrieval task, paired with hard negatives from the memory\\nmixture. We instantiate the model in a zero-shot dense retrieval setting by\\naugmenting a strong T5-based retriever with MoMA. Our model, MoMA, obtains\\nstrong zero-shot retrieval accuracy on the eighteen tasks included in the\\nstandard BEIR benchmark. It outperforms systems that seek generalization from\\nincreased model parameters and computation steps. Our analysis further\\nillustrates the necessity of augmenting with mixture-of-memory for robust\\ngeneralization, the benefits of augmentation learning, and how MoMA utilizes\\nthe plug-in memory at inference time without changing its parameters. We plan\\nto open source our code.'},\n",
       " {'Published': '2024-05-21',\n",
       "  'Title': 'The 2nd FutureDial Challenge: Dialog Systems with Retrieval Augmented Generation (FutureDial-RAG)',\n",
       "  'Authors': 'Yucheng Cai, Si Chen, Yi Huang, Junlan Feng, Zhijian Ou',\n",
       "  'Summary': 'The 2nd FutureDial Challenge: Dialog Systems with Retrieval Augmented\\nGeneration (FutureDial-RAG), Co-located with SLT 2024'},\n",
       " {'Published': '2023-10-17',\n",
       "  'Title': 'Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection',\n",
       "  'Authors': 'Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, Hannaneh Hajishirzi',\n",
       "  'Summary': \"Despite their remarkable capabilities, large language models (LLMs) often\\nproduce responses containing factual inaccuracies due to their sole reliance on\\nthe parametric knowledge they encapsulate. Retrieval-Augmented Generation\\n(RAG), an ad hoc approach that augments LMs with retrieval of relevant\\nknowledge, decreases such issues. However, indiscriminately retrieving and\\nincorporating a fixed number of retrieved passages, regardless of whether\\nretrieval is necessary, or passages are relevant, diminishes LM versatility or\\ncan lead to unhelpful response generation. We introduce a new framework called\\nSelf-Reflective Retrieval-Augmented Generation (Self-RAG) that enhances an LM's\\nquality and factuality through retrieval and self-reflection. Our framework\\ntrains a single arbitrary LM that adaptively retrieves passages on-demand, and\\ngenerates and reflects on retrieved passages and its own generations using\\nspecial tokens, called reflection tokens. Generating reflection tokens makes\\nthe LM controllable during the inference phase, enabling it to tailor its\\nbehavior to diverse task requirements. Experiments show that Self-RAG (7B and\\n13B parameters) significantly outperforms state-of-the-art LLMs and\\nretrieval-augmented models on a diverse set of tasks. Specifically, Self-RAG\\noutperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA,\\nreasoning and fact verification tasks, and it shows significant gains in\\nimproving factuality and citation accuracy for long-form generations relative\\nto these models.\"},\n",
       " {'Published': '2023-10-23',\n",
       "  'Title': 'Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy',\n",
       "  'Authors': 'Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, Weizhu Chen',\n",
       "  'Summary': 'Large language models are powerful text processors and reasoners, but are\\nstill subject to limitations including outdated knowledge and hallucinations,\\nwhich necessitates connecting them to the world. Retrieval-augmented large\\nlanguage models have raised extensive attention for grounding model generation\\non external knowledge. However, retrievers struggle to capture relevance,\\nespecially for queries with complex information needs. Recent work has proposed\\nto improve relevance modeling by having large language models actively involved\\nin retrieval, i.e., to improve retrieval with generation. In this paper, we\\nshow that strong performance can be achieved by a method we call Iter-RetGen,\\nwhich synergizes retrieval and generation in an iterative manner. A model\\noutput shows what might be needed to finish a task, and thus provides an\\ninformative context for retrieving more relevant knowledge which in turn helps\\ngenerate a better output in the next iteration. Compared with recent work which\\ninterleaves retrieval with generation when producing an output, Iter-RetGen\\nprocesses all retrieved knowledge as a whole and largely preserves the\\nflexibility in generation without structural constraints. We evaluate\\nIter-RetGen on multi-hop question answering, fact verification, and commonsense\\nreasoning, and show that it can flexibly leverage parametric knowledge and\\nnon-parametric knowledge, and is superior to or competitive with\\nstate-of-the-art retrieval-augmented baselines while causing fewer overheads of\\nretrieval and generation. We can further improve performance via\\ngeneration-augmented retrieval adaptation.'},\n",
       " {'Published': '2023-10-25',\n",
       "  'Title': 'Retrieve Anything To Augment Large Language Models',\n",
       "  'Authors': 'Peitian Zhang, Shitao Xiao, Zheng Liu, Zhicheng Dou, Jian-Yun Nie',\n",
       "  'Summary': \"Large language models (LLMs) face significant challenges stemming from their\\ninherent limitations in knowledge, memory, alignment, and action. These\\nchallenges cannot be addressed by LLMs alone, but should rely on assistance\\nfrom the external world, such as knowledge base, memory store, demonstration\\nexamples, and tools. Retrieval augmentation stands as a vital mechanism for\\nbridging the gap between LLMs and the external assistance. However,\\nconventional methods encounter two pressing issues. On the one hand, the\\ngeneral-purpose retrievers are not properly optimized for the retrieval\\naugmentation of LLMs. On the other hand, the task-specific retrievers lack the\\nrequired versatility, hindering their performance across the diverse retrieval\\naugmentation scenarios.\\n  In this work, we present a novel approach, the LLM-Embedder, which\\ncomprehensively supports the diverse retrieval augmentation needs of LLMs with\\none unified embedding model. Training such a unified model is non-trivial, as\\nvarious retrieval tasks aim to capture distinct semantic relationships, often\\nsubject to mutual interference. To address this challenge, we systematically\\noptimize our training methodology. This includes reward formulation based on\\nLLMs' feedback, the stabilization of knowledge distillation, multi-task\\nfine-tuning with explicit instructions, and homogeneous in-batch negative\\nsampling. These optimization strategies contribute to the outstanding empirical\\nperformance of the LLM-Embedder. Notably, it yields remarkable enhancements in\\nretrieval augmentation for LLMs, surpassing both general-purpose and\\ntask-specific retrievers in various evaluation scenarios. Our checkpoint and\\nsource code are publicly available at\\nhttps://github.com/FlagOpen/FlagEmbedding.\"},\n",
       " {'Published': '2022-07-29',\n",
       "  'Title': 'Paired Cross-Modal Data Augmentation for Fine-Grained Image-to-Text Retrieval',\n",
       "  'Authors': 'Hao Wang, Guosheng Lin, Steven C. H. Hoi, Chunyan Miao',\n",
       "  'Summary': 'This paper investigates an open research problem of generating text-image\\npairs to improve the training of fine-grained image-to-text cross-modal\\nretrieval task, and proposes a novel framework for paired data augmentation by\\nuncovering the hidden semantic information of StyleGAN2 model. Specifically, we\\nfirst train a StyleGAN2 model on the given dataset. We then project the real\\nimages back to the latent space of StyleGAN2 to obtain the latent codes. To\\nmake the generated images manipulatable, we further introduce a latent space\\nalignment module to learn the alignment between StyleGAN2 latent codes and the\\ncorresponding textual caption features. When we do online paired data\\naugmentation, we first generate augmented text through random token\\nreplacement, then pass the augmented text into the latent space alignment\\nmodule to output the latent codes, which are finally fed to StyleGAN2 to\\ngenerate the augmented images. We evaluate the efficacy of our augmented data\\napproach on two public cross-modal retrieval datasets, in which the promising\\nexperimental results demonstrate the augmented text-image pair data can be\\ntrained together with the original data to boost the image-to-text cross-modal\\nretrieval performance.'},\n",
       " {'Published': '2024-05-09',\n",
       "  'Title': 'Redefining Information Retrieval of Structured Database via Large Language Models',\n",
       "  'Authors': 'Mingzhu Wang, Yuzhe Zhang, Qihang Zhao, Juanyi Yang, Hong Zhang',\n",
       "  'Summary': 'Retrieval augmentation is critical when Language Models (LMs) exploit\\nnon-parametric knowledge related to the query through external knowledge bases\\nbefore reasoning. The retrieved information is incorporated into LMs as context\\nalongside the query, enhancing the reliability of responses towards factual\\nquestions. Prior researches in retrieval augmentation typically follow a\\nretriever-generator paradigm. In this context, traditional retrievers encounter\\nchallenges in precisely and seamlessly extracting query-relevant information\\nfrom knowledge bases. To address this issue, this paper introduces a novel\\nretrieval augmentation framework called ChatLR that primarily employs the\\npowerful semantic understanding ability of Large Language Models (LLMs) as\\nretrievers to achieve precise and concise information retrieval. Additionally,\\nwe construct an LLM-based search and question answering system tailored for the\\nfinancial domain by fine-tuning LLM on two tasks including Text2API and API-ID\\nrecognition. Experimental results demonstrate the effectiveness of ChatLR in\\naddressing user queries, achieving an overall information retrieval accuracy\\nexceeding 98.8\\\\%.'},\n",
       " {'Published': '2023-02-22',\n",
       "  'Title': 'X-TRA: Improving Chest X-ray Tasks with Cross-Modal Retrieval Augmentation',\n",
       "  'Authors': 'Tom van Sonsbeek, Marcel Worring',\n",
       "  'Summary': 'An important component of human analysis of medical images and their context\\nis the ability to relate newly seen things to related instances in our memory.\\nIn this paper we mimic this ability by using multi-modal retrieval augmentation\\nand apply it to several tasks in chest X-ray analysis. By retrieving similar\\nimages and/or radiology reports we expand and regularize the case at hand with\\nadditional knowledge, while maintaining factual knowledge consistency. The\\nmethod consists of two components. First, vision and language modalities are\\naligned using a pre-trained CLIP model. To enforce that the retrieval focus\\nwill be on detailed disease-related content instead of global visual appearance\\nit is fine-tuned using disease class information. Subsequently, we construct a\\nnon-parametric retrieval index, which reaches state-of-the-art retrieval\\nlevels. We use this index in our downstream tasks to augment image\\nrepresentations through multi-head attention for disease classification and\\nreport retrieval. We show that retrieval augmentation gives considerable\\nimprovements on these tasks. Our downstream report retrieval even shows to be\\ncompetitive with dedicated report generation methods, paving the path for this\\nmethod in medical imaging.'},\n",
       " {'Published': '2023-10-18',\n",
       "  'Title': 'Understanding Retrieval Augmentation for Long-Form Question Answering',\n",
       "  'Authors': 'Hung-Ting Chen, Fangyuan Xu, Shane Arora, Eunsol Choi',\n",
       "  'Summary': 'We present a study of retrieval-augmented language models (LMs) on long-form\\nquestion answering. We analyze how retrieval augmentation impacts different\\nLMs, by comparing answers generated from models while using the same evidence\\ndocuments, and how differing quality of retrieval document set impacts the\\nanswers generated from the same LM. We study various attributes of generated\\nanswers (e.g., fluency, length, variance) with an emphasis on the attribution\\nof generated long-form answers to in-context evidence documents. We collect\\nhuman annotations of answer attribution and evaluate methods for automatically\\njudging attribution. Our study provides new insights on how retrieval\\naugmentation impacts long, knowledge-rich text generation of LMs. We further\\nidentify attribution patterns for long text generation and analyze the main\\nculprits of attribution errors. Together, our analysis reveals how retrieval\\naugmentation impacts long knowledge-rich text generation and provide directions\\nfor future work.'},\n",
       " {'Published': '2024-04-08',\n",
       "  'Title': 'Retrieval-Augmented Open-Vocabulary Object Detection',\n",
       "  'Authors': 'Jooyeon Kim, Eulrang Cho, Sehyung Kim, Hyunwoo J. Kim',\n",
       "  'Summary': \"Open-vocabulary object detection (OVD) has been studied with Vision-Language\\nModels (VLMs) to detect novel objects beyond the pre-trained categories.\\nPrevious approaches improve the generalization ability to expand the knowledge\\nof the detector, using 'positive' pseudo-labels with additional 'class' names,\\ne.g., sock, iPod, and alligator. To extend the previous methods in two aspects,\\nwe propose Retrieval-Augmented Losses and visual Features (RALF). Our method\\nretrieves related 'negative' classes and augments loss functions. Also, visual\\nfeatures are augmented with 'verbalized concepts' of classes, e.g., worn on the\\nfeet, handheld music player, and sharp teeth. Specifically, RALF consists of\\ntwo modules: Retrieval Augmented Losses (RAL) and Retrieval-Augmented visual\\nFeatures (RAF). RAL constitutes two losses reflecting the semantic similarity\\nwith negative vocabularies. In addition, RAF augments visual features with the\\nverbalized concepts from a large language model (LLM). Our experiments\\ndemonstrate the effectiveness of RALF on COCO and LVIS benchmark datasets. We\\nachieve improvement up to 3.4 box AP$_{50}^{\\\\text{N}}$ on novel categories of\\nthe COCO dataset and 3.6 mask AP$_{\\\\text{r}}$ gains on the LVIS dataset. Code\\nis available at https://github.com/mlvlab/RALF .\"},\n",
       " {'Published': '2023-10-22',\n",
       "  'Title': 'Active Retrieval Augmented Generation',\n",
       "  'Authors': 'Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig',\n",
       "  'Summary': 'Despite the remarkable ability of large language models (LMs) to comprehend\\nand generate language, they have a tendency to hallucinate and create factually\\ninaccurate output. Augmenting LMs by retrieving information from external\\nknowledge resources is one promising solution. Most existing retrieval\\naugmented LMs employ a retrieve-and-generate setup that only retrieves\\ninformation once based on the input. This is limiting, however, in more general\\nscenarios involving generation of long texts, where continually gathering\\ninformation throughout generation is essential. In this work, we provide a\\ngeneralized view of active retrieval augmented generation, methods that\\nactively decide when and what to retrieve across the course of the generation.\\nWe propose Forward-Looking Active REtrieval augmented generation (FLARE), a\\ngeneric method which iteratively uses a prediction of the upcoming sentence to\\nanticipate future content, which is then utilized as a query to retrieve\\nrelevant documents to regenerate the sentence if it contains low-confidence\\ntokens. We test FLARE along with baselines comprehensively over 4 long-form\\nknowledge-intensive generation tasks/datasets. FLARE achieves superior or\\ncompetitive performance on all tasks, demonstrating the effectiveness of our\\nmethod. Code and datasets are available at https://github.com/jzbjyb/FLARE.'},\n",
       " {'Published': '2023-11-02',\n",
       "  'Title': 'Can Retriever-Augmented Language Models Reason? The Blame Game Between the Retriever and the Language Model',\n",
       "  'Authors': 'Parishad BehnamGhader, Santiago Miret, Siva Reddy',\n",
       "  'Summary': \"Augmenting pretrained language models with retrievers has shown promise in\\neffectively solving common NLP problems, such as language modeling and question\\nanswering. In this paper, we evaluate the strengths and weaknesses of popular\\nretriever-augmented language models, namely kNN-LM, REALM, DPR + FiD,\\nContriever + ATLAS, and Contriever + Flan-T5, in reasoning over retrieved\\nstatements across different tasks. Our findings indicate that the simple\\nsimilarity metric employed by retrievers is insufficient for retrieving all the\\nnecessary statements for reasoning. Additionally, the language models do not\\nexhibit strong reasoning even when provided with only the required statements.\\nFurthermore, when combined with imperfect retrievers, the performance of the\\nlanguage models becomes even worse, e.g., Flan-T5's performance drops by 28.6%\\nwhen retrieving 5 statements using Contriever. While larger language models\\nimprove performance, there is still a substantial room for enhancement. Our\\nfurther analysis indicates that multihop retrieve-and-read is promising for\\nlarge language models like GPT-3.5, but does not generalize to other language\\nmodels like Flan-T5-xxl.\"},\n",
       " {'Published': '2023-06-08',\n",
       "  'Title': 'RETA-LLM: A Retrieval-Augmented Large Language Model Toolkit',\n",
       "  'Authors': 'Jiongnan Liu, Jiajie Jin, Zihan Wang, Jiehan Cheng, Zhicheng Dou, Ji-Rong Wen',\n",
       "  'Summary': 'Although Large Language Models (LLMs) have demonstrated extraordinary\\ncapabilities in many domains, they still have a tendency to hallucinate and\\ngenerate fictitious responses to user requests. This problem can be alleviated\\nby augmenting LLMs with information retrieval (IR) systems (also known as\\nretrieval-augmented LLMs). Applying this strategy, LLMs can generate more\\nfactual texts in response to user input according to the relevant content\\nretrieved by IR systems from external corpora as references. In addition, by\\nincorporating external knowledge, retrieval-augmented LLMs can answer in-domain\\nquestions that cannot be answered by solely relying on the world knowledge\\nstored in parameters. To support research in this area and facilitate the\\ndevelopment of retrieval-augmented LLM systems, we develop RETA-LLM, a\\n{RET}reival-{A}ugmented LLM toolkit. In RETA-LLM, we create a complete pipeline\\nto help researchers and users build their customized in-domain LLM-based\\nsystems. Compared with previous retrieval-augmented LLM systems, RETA-LLM\\nprovides more plug-and-play modules to support better interaction between IR\\nsystems and LLMs, including {request rewriting, document retrieval, passage\\nextraction, answer generation, and fact checking} modules. Our toolkit is\\npublicly available at https://github.com/RUC-GSAI/YuLan-IR/tree/main/RETA-LLM.'},\n",
       " {'Published': '2024-05-22',\n",
       "  'Title': 'xRAG: Extreme Context Compression for Retrieval-augmented Generation with One Token',\n",
       "  'Authors': 'Xin Cheng, Xun Wang, Xingxing Zhang, Tao Ge, Si-Qing Chen, Furu Wei, Huishuai Zhang, Dongyan Zhao',\n",
       "  'Summary': 'This paper introduces xRAG, an innovative context compression method tailored\\nfor retrieval-augmented generation. xRAG reinterprets document embeddings in\\ndense retrieval--traditionally used solely for retrieval--as features from the\\nretrieval modality. By employing a modality fusion methodology, xRAG seamlessly\\nintegrates these embeddings into the language model representation space,\\neffectively eliminating the need for their textual counterparts and achieving\\nan extreme compression rate. In xRAG, the only trainable component is the\\nmodality bridge, while both the retriever and the language model remain frozen.\\nThis design choice allows for the reuse of offline-constructed document\\nembeddings and preserves the plug-and-play nature of retrieval augmentation.\\nExperimental results demonstrate that xRAG achieves an average improvement of\\nover 10% across six knowledge-intensive tasks, adaptable to various language\\nmodel backbones, ranging from a dense 7B model to an 8x7B Mixture of Experts\\nconfiguration. xRAG not only significantly outperforms previous context\\ncompression methods but also matches the performance of uncompressed models on\\nseveral datasets, while reducing overall FLOPs by a factor of 3.53. Our work\\npioneers new directions in retrieval-augmented generation from the perspective\\nof multimodality fusion, and we hope it lays the foundation for future\\nefficient and scalable retrieval-augmented systems'}]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_paper_metadata_from_arxiv.invoke(\"Retrieval Augmented Generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We determine the non-perturbatively renormalized axial current for O($a$)\\nimproved lattice QCD with Wilson quarks. Our strategy is based on the chirally\\nrotated Schr\\\\\"odinger functional and can be generalized to other finite (ratios\\nof) renormalization constants which are traditionally obtained by imposing\\ncontinuum chiral Ward identities as normalization conditions. Compared to the\\nlatter we achieve an error reduction up to one order of magnitude. Our results\\nhave already enabled the setting of the scale for the $N_{\\\\rm f}=2+1$ CLS\\nensembles [1] and are thus an essential ingredient for the recent $\\\\alpha_s$\\ndetermination by the ALPHA collaboration [2]. In this paper we shortly review\\nthe strategy and present our results for both $N_{\\\\rm f}=2$ and $N_{\\\\rm f}=3$\\nlattice QCD, where we match the $\\\\beta$-values of the CLS gauge configurations.\\nIn addition to the axial current renormalization, we also present precise\\nresults for the renormalized local vector current.'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_paper_summary_from_arxiv.invoke(\"1808.09236\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No summary found for this paper.'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_paper_summary_from_arxiv.invoke(\"808.09236\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Partial cubes are isometric subgraphs of hypercubes, which play an important role in the theory of partial cubes. These structures are employed in our paper to characterize bipartite graphs and partial cubes of arbitrary dimension. New characterizations are established and new proofs of some known results are given.\\n\\n   \\n  We describe a new algorithm, the \\nQuestion: Tell me about partial cubes.\\n'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_questions_about_topics.invoke(\"Tell me about partial cubes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AS8QmaKVjhbR"
   },
   "outputs": [],
   "source": [
    "tools = [retriever_tool, get_metadata_information_from_arxiv, get_information_from_arxiv]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ueEn73nlliNr"
   },
   "source": [
    "## Agent Prompt Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RY13DrVXFDrm"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "agent_purpose = \"You are a helpful research assistant.\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", agent_purpose),\n",
    "        (\"human\", \"{input}\"),\n",
    "        MessagesPlaceholder(\"agent_scratchpad\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z4NU4ZjGl0WC"
   },
   "source": [
    "## Agent Memory Using MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1A-3Fg1cjwyK"
   },
   "outputs": [],
   "source": [
    "from langchain_mongodb.chat_message_histories import MongoDBChatMessageHistory\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "def get_session_history(session_id: str) -> MongoDBChatMessageHistory:\n",
    "        return MongoDBChatMessageHistory(MONGO_URI, session_id, database_name=DB_NAME, collection_name=\"history\")\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    chat_memory=get_session_history(\"my-session\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O9TqMKyvKhvq"
   },
   "source": [
    "## Agent Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wI4uBAmNF5ll"
   },
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True,\n",
    "    memory=memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RGB4pWTylmFy"
   },
   "source": [
    "## Agent Exectution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DM8GtbjgIJXt",
    "outputId": "328c36f6-b4a0-4a32-e7d6-b606ca044517"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `knowledge_base` with `{'query': 'Prompt Compression'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m  Computation on compressed strings is one of the key approaches to processing\n",
      "massive data sets. We consider local subsequence recognition problems on\n",
      "strings compressed by straight-line programs (SLP), which is closely related to\n",
      "Lempel--Ziv compression. For an SLP-compressed text of length $\\bar m$, and an\n",
      "uncompressed pattern of length $n$, C{\\'e}gielski et al. gave an algorithm for\n",
      "local subsequence recognition running in time $O(\\bar mn^2 \\log n)$. We improve\n",
      "the running time to $O(\\bar mn^{1.5})$. Our algorithm can also be used to\n",
      "compute the longest common subsequence between a compressed text and an\n",
      "uncompressed pattern in time $O(\\bar mn^{1.5})$; the same problem with a\n",
      "compressed pattern is known to be NP-hard.\n",
      "\n",
      "\n",
      "  A new incremental algorithm for data compression is presented. For a sequence\n",
      "of input symbols algorithm incrementally constructs a p-adic integer number as\n",
      "an output. Decoding process starts with less significant part of a p-adic\n",
      "integer and incrementally reconstructs a sequence of input symbols. Algorithm\n",
      "is based on certain features of p-adic numbers and p-adic norm. p-adic coding\n",
      "algorithm may be considered as of generalization a popular compression\n",
      "technique - arithmetic coding algorithms. It is shown that for p = 2 the\n",
      "algorithm works as integer variant of arithmetic coding; for a special class of\n",
      "models it gives exactly the same codes as Huffman's algorithm, for another\n",
      "special model and a specific alphabet it gives Golomb-Rice codes.\n",
      "\n",
      "\n",
      "  This work considers the problem of transmitting multiple compressible sources\n",
      "over a network at minimum cost. The aim is to find the optimal rates at which\n",
      "the sources should be compressed and the network flows using which they should\n",
      "be transmitted so that the cost of the transmission is minimal. We consider\n",
      "networks with capacity constraints and linear cost functions. The problem is\n",
      "complicated by the fact that the description of the feasible rate region of\n",
      "distributed source coding problems typically has a number of constraints that\n",
      "is exponential in the number of sources. This renders general purpose solvers\n",
      "inefficient. We present a framework in which these problems can be solved\n",
      "efficiently by exploiting the structure of the feasible rate regions coupled\n",
      "with dual decomposition and optimization techniques such as the subgradient\n",
      "method and the proximal bundle method.\n",
      "\n",
      "\n",
      "  It has been observed that particular rate-1/2 partially systematic parallel\n",
      "concatenated convolutional codes (PCCCs) can achieve a lower error floor than\n",
      "that of their rate-1/3 parent codes. Nevertheless, good puncturing patterns can\n",
      "only be identified by means of an exhaustive search, whilst convergence towards\n",
      "low bit error probabilities can be problematic when the systematic output of a\n",
      "rate-1/2 partially systematic PCCC is heavily punctured. In this paper, we\n",
      "present and study a family of rate-1/2 partially systematic PCCCs, which we\n",
      "call pseudo-randomly punctured codes. We evaluate their bit error rate\n",
      "performance and we show that they always yield a lower error floor than that of\n",
      "their rate-1/3 parent codes. Furthermore, we compare analytic results to\n",
      "simulations and we demonstrate that their performance converges towards the\n",
      "error floor region, owning to the moderate puncturing of their systematic\n",
      "output. Consequently, we propose pseudo-random puncturing as a means of\n",
      "improving the bandwidth efficiency of a PCCC and simultaneously lowering its\n",
      "error floor.\n",
      "\n",
      "\n",
      "  We raise the question of approximating the compressibility of a string with\n",
      "respect to a fixed compression scheme, in sublinear time. We study this\n",
      "question in detail for two popular lossless compression schemes: run-length\n",
      "encoding (RLE) and Lempel-Ziv (LZ), and present sublinear algorithms for\n",
      "approximating compressibility with respect to both schemes. We also give\n",
      "several lower bounds that show that our algorithms for both schemes cannot be\n",
      "improved significantly.\n",
      "  Our investigation of LZ yields results whose interest goes beyond the initial\n",
      "questions we set out to study. In particular, we prove combinatorial structural\n",
      "lemmas that relate the compressibility of a string with respect to Lempel-Ziv\n",
      "to the number of distinct short substrings contained in it. In addition, we\n",
      "show that approximating the compressibility with respect to LZ is related to\n",
      "approximating the support size of a distribution.\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3mHere are some research papers on the topic Prompt Compression:\n",
      "\n",
      "1. \"Computation on Compressed Strings\" by Cegielski et al.\n",
      "2. \"P-adic Coding Algorithm\" by Kuznetsov\n",
      "3. \"Minimum Cost Transmission of Multiple Compressible Sources Over a Network\" by Sutter et al.\n",
      "4. \"Pseudo-Randomly Punctured Codes\" by Kou et al.\n",
      "5. \"Approximating the Compressibility of a String with Respect to a Fixed Compression Scheme\" by Gopalan et al.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Get me a list of research papers on the topic Prompt Compression',\n",
       " 'chat_history': 'Human: Get me a list of research papers on the topic Prompt Compression\\nAI: In the document \\'A Comprehensive Study of Prompt Compression for LLMs,\\' the authors propose a novel prompt compression method called prompt compression via relation-aware graph (PROMPT-SAW). The PROMPT-SAW algorithm uses a graph-based approach to compress large prompts into shorter ones while preserving contextual coherence and reducing redundancy. It first extracts all entities and their relations from the given prompt to construct a graph, and then uses the graph to find small-scale information units that contain less but still meaningful information.\\nThe authors also evaluate the performance of PROMPT-SAW through experiments on three different tasks: covert character sequence decoding, short answer generation, and summarization. They find\\nHuman: Get me the abstract of the first paper on the list\\nAI: <plain>The abstract of the first paper on the list is \"In a recent paper on wormholes (gr-qc/0503097), the author of that paper didn\\'t know what he was talking about. In this paper I correct the author\\'s naive erroneous misconceptions.\"\\nHuman: Get me a list of research papers on the topic Prompt Compression\\nAI: Here are some research papers on the topic Prompt Compression:\\n\\n1. \"Compressed Full-Text Self-Indexes: A Practitioner\\'s Point of View\" by Navarro, G.\\n2. \"A Code Compression Strategy Based on Control Flow Graph Representation of Embedded Programs\" by Kandemir, M.T., et al.\\n3. \"Fast Fibonacci Decompression for Real-Time Compression of Tree Data Structures\" by Korkmaz, T., et al.\\n4. \"Pushdown Compressors and the Lempel-Ziv Algorithm\" by Navarro, G.\\n5. \"A General Formulation for the Code-Based Test Compression Problem with Fixed-Length Input Blocks\" by Kandemir, M.T., et al.\\nHuman: Get me the abstract of the first paper on the list\\nAI: The first paper on the list is titled \"First Stars IV: Summary Talk\" and was published on 2012-07-27. The authors are Andrea Ferrara. The summary of the paper is \"The paper contains the summary of the First Stars IV 2012 Conference held in Kyoto, Japan\".',\n",
       " 'output': 'Here are some research papers on the topic Prompt Compression:\\n\\n1. \"Computation on Compressed Strings\" by Cegielski et al.\\n2. \"P-adic Coding Algorithm\" by Kuznetsov\\n3. \"Minimum Cost Transmission of Multiple Compressible Sources Over a Network\" by Sutter et al.\\n4. \"Pseudo-Randomly Punctured Codes\" by Kou et al.\\n5. \"Approximating the Compressibility of a String with Respect to a Fixed Compression Scheme\" by Gopalan et al.'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"Get me a list of research papers on the topic Prompt Compression\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oBvTS8S0JUPb",
    "outputId": "13fbb430-eb49-4b91-dd04-33bcc33ecc00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `get_metadata_information_from_arxiv` with `{'word': 'first paper'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m[{'Published': '2012-07-27', 'Title': 'First Stars IV: Summary Talk', 'Authors': 'Andrea Ferrara', 'Summary': 'The paper contains the summary of the First Stars IV 2012 Conference held in\\nKyoto, Japan'}, {'Published': '2020-10-04', 'Title': 'Some inequalities between Laplacian eigenvalues on Riemannian manifolds', 'Authors': 'Guangyue Huang, Xuerong Qi', 'Summary': 'In this paper, we study a first Dirichlet eigenfunction of the weighted\\n$p$-Laplacian on a bounded domain in a complete weighted Riemannian manifold.\\nBy constructing gradient estimates for a first eigenfunction, we obtain some\\nrelationships between weighted $p$-Laplacian first eigenvalues. As an immediate\\napplication, we also obtain some eigenvalue comparison results between the\\nfirst Dirichlet eigenvalue of the weighted Laplacian, the first clamped plate\\neigenvalue and the first buckling eigenvalue.'}, {'Published': '2018-05-20', 'Title': '$λ$-analogues of r-Stirling numbers of the first kind', 'Authors': 'Taekyun Kim, Dae san Kim', 'Summary': 'In this paper, we study $\\\\lambda$-analogues of the r-Stirling numbers of the\\nfirst kind which have close connections with the r-Stirling numbers of the\\nfirst kind and $\\\\lambda$-Stirling numbers of the first kind. Specifically, we\\ngive the recurrence relations for these numbers and show their connections with\\nthe $\\\\lambda$-Stirling numbers of the first kind and higher-order Daehee\\npolynomials.'}, {'Published': '2021-03-14', 'Title': 'Hankel Transform of the First Form (q,r)-Dowling Numbers', 'Authors': 'Roberto B. Corcino', 'Summary': 'In this paper, the Hankel transform of the generalized q-exponential\\npolynomial of the first form (q, r)-Whitney numbers of the second kind is\\nestablished using the method of Cigler. Consequently, the Hankel transform of\\nthe first form (q, r)-Dowling numbers is obtained as special case.'}, {'Published': '1995-11-08', 'Title': 'Comment on \"Dynamics of Weak First Order Phase Transitions\"', 'Authors': 'G. Harris, G. Jungman', 'Summary': 'We comment on an earlier paper of M. Gleiser, regarding mechanisms of\\nfirst-order phase transitions.'}, {'Published': '2008-08-11', 'Title': 'A First-Principles Constitutive Equation for Suspension Rheology: Supplementary Material', 'Authors': 'J. M. Brader, M. E. Cates, M. Fuchs', 'Summary': \"Additional supplementary material for the paper `A First-Principles\\nConstitutive Equation for Suspension Rheology'.\"}, {'Published': '2016-05-29', 'Title': 'First-Order Modal Logic: Frame Definability and Lindström Theorems', 'Authors': 'Reihane Zoghifard, Massoud Pourmahdian', 'Summary': 'This paper involves generalizing the Goldblatt-Thomason and the Lindstr\\\\\"om\\ncharacterization theorems to first-order modal logic.'}, {'Published': '2016-04-24', 'Title': 'Tableaux for First Order Logic of Proofs', 'Authors': 'Meghdad Ghari', 'Summary': 'In this paper we present a tableau proof system for first order logic of\\nproofs FOLP. We show that the tableau system is sound and complete with respect\\nto Mkrtychev models of FOLP.'}, {'Published': '2023-11-12', 'Title': 'Depth and Breadth of Research Area Coverage and Its Impact on Publication Citation: An Analysis of Bibliometric Papers', 'Authors': 'Zhuoran Lin, Yun Wang, Hongjun Li', 'Summary': \"Many other factors affecting citation of publications, except for research\\narea coverage, have been studied. This study aims to investigate impact of\\nresearch area coverage. Bibliometric papers and their related papers (referred\\npapers, citing papers and first author's papers) were screened and matched by\\nPython program. Papers' research areas were classified according to Web of\\nScience. Bibliometric parameters of the most cited 5% and the least cited 5%\\npapers were compared. Firstly, coverage of related papers' research areas\\nimpacts the citation of their original papers. The impact of references and\\nciting papers are positive and negative, separately, while the first author's\\npapers have no influence. Secondly, high-influence papers tend to cite\\nreferences from a wider area and are cited by followers from a wider area.\\nAdditionally, the pattern of knowledge flow differs significantly between high-\\nand low-influence papers. Low-influence papers narrow knowledge flow, whereas\\nhigh-influence papers broaden it. This study has shown that both depth and\\nbreadth of research area coverage can influence citations. It is recommended\\nthat authors should extensively cite high-influence publications, both within\\nand beyond their own area.\"}]\u001b[0m\u001b[32;1m\u001b[1;3mThe first paper on the list is titled \"First Stars IV: Summary Talk\" and was published on 2012-07-27. The authors are Andrea Ferrara. The summary of the paper is \"The paper contains the summary of the First Stars IV 2012 Conference held in Kyoto, Japan\".\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Get me the abstract of the first paper on the list',\n",
       " 'chat_history': 'Human: Get me a list of research papers on the topic Prompt Compression\\nAI: In the document \\'A Comprehensive Study of Prompt Compression for LLMs,\\' the authors propose a novel prompt compression method called prompt compression via relation-aware graph (PROMPT-SAW). The PROMPT-SAW algorithm uses a graph-based approach to compress large prompts into shorter ones while preserving contextual coherence and reducing redundancy. It first extracts all entities and their relations from the given prompt to construct a graph, and then uses the graph to find small-scale information units that contain less but still meaningful information.\\nThe authors also evaluate the performance of PROMPT-SAW through experiments on three different tasks: covert character sequence decoding, short answer generation, and summarization. They find\\nHuman: Get me the abstract of the first paper on the list\\nAI: <plain>The abstract of the first paper on the list is \"In a recent paper on wormholes (gr-qc/0503097), the author of that paper didn\\'t know what he was talking about. In this paper I correct the author\\'s naive erroneous misconceptions.\"\\nHuman: Get me a list of research papers on the topic Prompt Compression\\nAI: Here are some research papers on the topic Prompt Compression:\\n\\n1. \"Compressed Full-Text Self-Indexes: A Practitioner\\'s Point of View\" by Navarro, G.\\n2. \"A Code Compression Strategy Based on Control Flow Graph Representation of Embedded Programs\" by Kandemir, M.T., et al.\\n3. \"Fast Fibonacci Decompression for Real-Time Compression of Tree Data Structures\" by Korkmaz, T., et al.\\n4. \"Pushdown Compressors and the Lempel-Ziv Algorithm\" by Navarro, G.\\n5. \"A General Formulation for the Code-Based Test Compression Problem with Fixed-Length Input Blocks\" by Kandemir, M.T., et al.\\nHuman: Get me the abstract of the first paper on the list\\nAI: The first paper on the list is titled \"First Stars IV: Summary Talk\" and was published on 2012-07-27. The authors are Andrea Ferrara. The summary of the paper is \"The paper contains the summary of the First Stars IV 2012 Conference held in Kyoto, Japan\".\\nHuman: Get me a list of research papers on the topic Prompt Compression\\nAI: Here are some research papers on the topic Prompt Compression:\\n\\n1. \"Computation on Compressed Strings\" by Cegielski et al.\\n2. \"P-adic Coding Algorithm\" by Kuznetsov\\n3. \"Minimum Cost Transmission of Multiple Compressible Sources Over a Network\" by Sutter et al.\\n4. \"Pseudo-Randomly Punctured Codes\" by Kou et al.\\n5. \"Approximating the Compressibility of a String with Respect to a Fixed Compression Scheme\" by Gopalan et al.',\n",
       " 'output': 'The first paper on the list is titled \"First Stars IV: Summary Talk\" and was published on 2012-07-27. The authors are Andrea Ferrara. The summary of the paper is \"The paper contains the summary of the First Stars IV 2012 Conference held in Kyoto, Japan\".'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\":\"Get me the abstract of the first paper you found\"})"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "RM8rg08YhqZe",
    "UUf3jtFzO4-V",
    "Sm5QZdshwJLN"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "09dcf4ce88064f11980bbefaad1ebc75": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_39563df9477648398456675ec51075aa",
      "placeholder": "​",
      "style": "IPY_MODEL_f4353368efbd4c3891f805ddc3d05e1b",
      "value": "Downloading data: 100%"
     }
    },
    "164d16df28d24ab796b7c9cf85174800": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_95e4af5b420242b7a6b74a18cad98961",
      "placeholder": "​",
      "style": "IPY_MODEL_dff65b579f0746ffae8739ecb0aa5a41",
      "value": "Generating train split: "
     }
    },
    "20d693a09c534414a5c4c0dd58cf94ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "278513c5a8b04a24b1823d38107f1e50": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_62e196b6d30746578e137c50b661f946",
      "placeholder": "​",
      "style": "IPY_MODEL_ced7f9d61e06442a960dcda95852048e",
      "value": " 102M/102M [00:06&lt;00:00, 20.6MB/s]"
     }
    },
    "30fe0bcd02cb47f3ba23bb480e2eaaea": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "373ed3b6307741859ab297c270cf42c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "39563df9477648398456675ec51075aa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "41056c822b9d44559147d2b21416b956": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a43c349d171e469c8cc94d48060f775b",
      "placeholder": "​",
      "style": "IPY_MODEL_373ed3b6307741859ab297c270cf42c8",
      "value": " 50000/0 [00:04&lt;00:00, 12390.43 examples/s]"
     }
    },
    "62e196b6d30746578e137c50b661f946": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7dbfebff68ff45628da832fac5233c93": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_164d16df28d24ab796b7c9cf85174800",
       "IPY_MODEL_e70e0d317f1e4e73bd95349ed1510cce",
       "IPY_MODEL_41056c822b9d44559147d2b21416b956"
      ],
      "layout": "IPY_MODEL_b1929fb112174c0abcd8004f6be0f880"
     }
    },
    "95e4af5b420242b7a6b74a18cad98961": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a43c349d171e469c8cc94d48060f775b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b1929fb112174c0abcd8004f6be0f880": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cebfba144ba6418092df949783f93455": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_09dcf4ce88064f11980bbefaad1ebc75",
       "IPY_MODEL_f2bd7bda4d0c4d93b88e53aeb4e1b62d",
       "IPY_MODEL_278513c5a8b04a24b1823d38107f1e50"
      ],
      "layout": "IPY_MODEL_d3941c633788427abb858b21e285088f"
     }
    },
    "ced7f9d61e06442a960dcda95852048e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d17d8c8f45ee44cd87dcd787c05dbdc3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d3941c633788427abb858b21e285088f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dff65b579f0746ffae8739ecb0aa5a41": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e70e0d317f1e4e73bd95349ed1510cce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f73ae771c24645c79fd41409a8fc7b34",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_20d693a09c534414a5c4c0dd58cf94ed",
      "value": 1
     }
    },
    "f2bd7bda4d0c4d93b88e53aeb4e1b62d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_30fe0bcd02cb47f3ba23bb480e2eaaea",
      "max": 102202622,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d17d8c8f45ee44cd87dcd787c05dbdc3",
      "value": 102202622
     }
    },
    "f4353368efbd4c3891f805ddc3d05e1b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f73ae771c24645c79fd41409a8fc7b34": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
