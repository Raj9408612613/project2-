{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mongodb-developer/GenAI-Showcase/blob/main/notebooks/agents/agent_fireworks_ai_langchain_mongodb.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3kMALXaMv-MS"
   },
   "source": [
    "## Install Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cxTXczeTghzU",
    "outputId": "ae3a81b2-cba6-42fc-f593-8646bff77b14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-fireworks\n",
      "  Using cached langchain_fireworks-0.1.3-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting langchain-huggingface\n",
      "  Using cached langchain_huggingface-0.0.1-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting langchain-community\n",
      "  Using cached langchain_community-0.2.1-py3-none-any.whl.metadata (8.9 kB)\n",
      "Collecting langchain-mongodb\n",
      "  Using cached langchain_mongodb-0.1.5-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting arxiv\n",
      "  Using cached arxiv-2.1.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting pymupdf\n",
      "  Downloading PyMuPDF-1.24.5-cp312-none-macosx_11_0_arm64.whl.metadata (3.4 kB)\n",
      "Collecting datasets\n",
      "  Using cached datasets-2.19.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting pymongo\n",
      "  Using cached pymongo-4.7.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (22 kB)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.9.1 (from langchain-fireworks)\n",
      "  Using cached aiohttp-3.9.5-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.5 kB)\n",
      "Collecting fireworks-ai>=0.13.0 (from langchain-fireworks)\n",
      "  Using cached fireworks_ai-0.14.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting langchain-core<0.3,>=0.1.52 (from langchain-fireworks)\n",
      "  Downloading langchain_core-0.2.3-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting openai<2.0.0,>=1.10.0 (from langchain-fireworks)\n",
      "  Downloading openai-1.30.5-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting requests<3,>=2 (from langchain-fireworks)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting huggingface-hub>=0.23.0 (from langchain-huggingface)\n",
      "  Using cached huggingface_hub-0.23.2-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting sentence-transformers>=2.6.0 (from langchain-huggingface)\n",
      "  Using cached sentence_transformers-3.0.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting text-generation<0.8.0,>=0.7.0 (from langchain-huggingface)\n",
      "  Using cached text_generation-0.7.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting tokenizers>=0.19.1 (from langchain-huggingface)\n",
      "  Using cached tokenizers-0.19.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting transformers>=4.39.0 (from langchain-huggingface)\n",
      "  Downloading transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting PyYAML>=5.3 (from langchain-community)\n",
      "  Using cached PyYAML-6.0.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain-community)\n",
      "  Using cached SQLAlchemy-2.0.30-cp312-cp312-macosx_11_0_arm64.whl.metadata (9.6 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
      "  Using cached dataclasses_json-0.6.6-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting langchain<0.3.0,>=0.2.0 (from langchain-community)\n",
      "  Using cached langchain-0.2.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.0 (from langchain-community)\n",
      "  Using cached langsmith-0.1.65-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting numpy<2,>=1 (from langchain-community)\n",
      "  Using cached numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (61 kB)\n",
      "Collecting tenacity<9.0.0,>=8.1.0 (from langchain-community)\n",
      "  Using cached tenacity-8.3.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting feedparser==6.0.10 (from arxiv)\n",
      "  Using cached feedparser-6.0.10-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting requests<3,>=2 (from langchain-fireworks)\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting sgmllib3k (from feedparser==6.0.10->arxiv)\n",
      "  Using cached sgmllib3k-1.0.0-py3-none-any.whl\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3,>=2->langchain-fireworks)\n",
      "  Using cached charset_normalizer-3.3.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3,>=2->langchain-fireworks)\n",
      "  Using cached idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2->langchain-fireworks)\n",
      "  Using cached urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3,>=2->langchain-fireworks)\n",
      "  Using cached certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting PyMuPDFb==1.24.3 (from pymupdf)\n",
      "  Using cached PyMuPDFb-1.24.3-py3-none-macosx_11_0_arm64.whl.metadata (1.4 kB)\n",
      "Collecting filelock (from datasets)\n",
      "  Using cached filelock-3.14.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting pyarrow>=12.0.0 (from datasets)\n",
      "  Using cached pyarrow-16.1.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.0 kB)\n",
      "Collecting pyarrow-hotfix (from datasets)\n",
      "  Using cached pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Using cached pandas-2.2.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (19 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.4.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.3.1,>=2023.1.0 (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets)\n",
      "  Using cached fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting packaging (from datasets)\n",
      "  Using cached packaging-24.0-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo)\n",
      "  Using cached dnspython-2.6.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.9.1->langchain-fireworks)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.9.1->langchain-fireworks)\n",
      "  Using cached attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.9.1->langchain-fireworks)\n",
      "  Using cached frozenlist-1.4.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.9.1->langchain-fireworks)\n",
      "  Using cached multidict-6.0.5-cp312-cp312-macosx_11_0_arm64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.9.1->langchain-fireworks)\n",
      "  Using cached yarl-1.9.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (31 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Using cached marshmallow-3.21.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting httpx (from fireworks-ai>=0.13.0->langchain-fireworks)\n",
      "  Using cached httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting httpx-sse (from fireworks-ai>=0.13.0->langchain-fireworks)\n",
      "  Using cached httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting pydantic (from fireworks-ai>=0.13.0->langchain-fireworks)\n",
      "  Using cached pydantic-2.7.2-py3-none-any.whl.metadata (108 kB)\n",
      "Collecting Pillow (from fireworks-ai>=0.13.0->langchain-fireworks)\n",
      "  Using cached pillow-10.3.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (9.2 kB)\n",
      "Collecting typing-extensions>=3.7.4.3 (from huggingface-hub>=0.23.0->langchain-huggingface)\n",
      "  Using cached typing_extensions-4.12.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain<0.3.0,>=0.2.0->langchain-community)\n",
      "  Using cached langchain_text_splitters-0.2.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3,>=0.1.52->langchain-fireworks)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting packaging (from datasets)\n",
      "  Using cached packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.0->langchain-community)\n",
      "  Using cached orjson-3.10.3-cp312-cp312-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl.metadata (49 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai<2.0.0,>=1.10.0->langchain-fireworks)\n",
      "  Using cached anyio-4.4.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai<2.0.0,>=1.10.0->langchain-fireworks)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting sniffio (from openai<2.0.0,>=1.10.0->langchain-fireworks)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers>=2.6.0->langchain-huggingface)\n",
      "  Using cached torch-2.3.0-cp312-none-macosx_11_0_arm64.whl.metadata (26 kB)\n",
      "Collecting scikit-learn (from sentence-transformers>=2.6.0->langchain-huggingface)\n",
      "  Using cached scikit_learn-1.5.0-cp312-cp312-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Collecting scipy (from sentence-transformers>=2.6.0->langchain-huggingface)\n",
      "  Using cached scipy-1.13.1-cp312-cp312-macosx_12_0_arm64.whl.metadata (60 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers>=4.39.0->langchain-huggingface)\n",
      "  Using cached regex-2024.5.15-cp312-cp312-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers>=4.39.0->langchain-huggingface)\n",
      "  Using cached safetensors-0.4.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Collecting python-dateutil>=2.8.2 (from pandas->datasets)\n",
      "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Using cached pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting httpcore==1.* (from httpx->fireworks-ai>=0.13.0->langchain-fireworks)\n",
      "  Using cached httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->fireworks-ai>=0.13.0->langchain-fireworks)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.52->langchain-fireworks)\n",
      "  Using cached jsonpointer-2.4-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic->fireworks-ai>=0.13.0->langchain-fireworks)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.18.3 (from pydantic->fireworks-ai>=0.13.0->langchain-fireworks)\n",
      "  Using cached pydantic_core-2.18.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.5 kB)\n",
      "Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas->datasets)\n",
      "  Using cached six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting sympy (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface)\n",
      "  Using cached sympy-1.12.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface)\n",
      "  Using cached networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting jinja2 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface)\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface)\n",
      "  Using cached MarkupSafe-2.1.5-cp312-cp312-macosx_10_9_universal2.whl.metadata (3.0 kB)\n",
      "Collecting mpmath<1.4.0,>=1.1.0 (from sympy->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Using cached langchain_fireworks-0.1.3-py3-none-any.whl (15 kB)\n",
      "Using cached langchain_huggingface-0.0.1-py3-none-any.whl (17 kB)\n",
      "Using cached langchain_community-0.2.1-py3-none-any.whl (2.1 MB)\n",
      "Using cached langchain_mongodb-0.1.5-py3-none-any.whl (12 kB)\n",
      "Using cached arxiv-2.1.0-py3-none-any.whl (11 kB)\n",
      "Using cached feedparser-6.0.10-py3-none-any.whl (81 kB)\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Downloading PyMuPDF-1.24.5-cp312-none-macosx_11_0_arm64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached PyMuPDFb-1.24.3-py3-none-macosx_11_0_arm64.whl (14.9 MB)\n",
      "Using cached datasets-2.19.1-py3-none-any.whl (542 kB)\n",
      "Using cached pymongo-4.7.2-cp312-cp312-macosx_11_0_arm64.whl (486 kB)\n",
      "Using cached tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "Using cached aiohttp-3.9.5-cp312-cp312-macosx_11_0_arm64.whl (392 kB)\n",
      "Using cached dataclasses_json-0.6.6-py3-none-any.whl (28 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
      "Using cached fireworks_ai-0.14.0-py3-none-any.whl (81 kB)\n",
      "Using cached fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
      "Using cached huggingface_hub-0.23.2-py3-none-any.whl (401 kB)\n",
      "Using cached langchain-0.2.1-py3-none-any.whl (973 kB)\n",
      "Downloading langchain_core-0.2.3-py3-none-any.whl (310 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.2/310.2 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached langsmith-0.1.65-py3-none-any.whl (124 kB)\n",
      "Using cached numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl (13.7 MB)\n",
      "Downloading openai-1.30.5-py3-none-any.whl (320 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached packaging-23.2-py3-none-any.whl (53 kB)\n",
      "Using cached pyarrow-16.1.0-cp312-cp312-macosx_11_0_arm64.whl (26.0 MB)\n",
      "Using cached PyYAML-6.0.1-cp312-cp312-macosx_11_0_arm64.whl (165 kB)\n",
      "Using cached sentence_transformers-3.0.0-py3-none-any.whl (224 kB)\n",
      "Using cached SQLAlchemy-2.0.30-cp312-cp312-macosx_11_0_arm64.whl (2.1 MB)\n",
      "Using cached tenacity-8.3.0-py3-none-any.whl (25 kB)\n",
      "Using cached text_generation-0.7.0-py3-none-any.whl (12 kB)\n",
      "Using cached tokenizers-0.19.1-cp312-cp312-macosx_11_0_arm64.whl (2.4 MB)\n",
      "Downloading transformers-4.41.2-py3-none-any.whl (9.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached filelock-3.14.0-py3-none-any.whl (12 kB)\n",
      "Using cached multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Using cached pandas-2.2.2-cp312-cp312-macosx_11_0_arm64.whl (11.3 MB)\n",
      "Using cached pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Using cached xxhash-3.4.1-cp312-cp312-macosx_11_0_arm64.whl (30 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached anyio-4.4.0-py3-none-any.whl (86 kB)\n",
      "Using cached attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "Using cached certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp312-cp312-macosx_11_0_arm64.whl (119 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached frozenlist-1.4.1-cp312-cp312-macosx_11_0_arm64.whl (51 kB)\n",
      "Using cached httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "Using cached httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "Using cached idna-3.7-py3-none-any.whl (66 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Using cached langchain_text_splitters-0.2.0-py3-none-any.whl (23 kB)\n",
      "Using cached marshmallow-3.21.2-py3-none-any.whl (49 kB)\n",
      "Using cached multidict-6.0.5-cp312-cp312-macosx_11_0_arm64.whl (29 kB)\n",
      "Using cached orjson-3.10.3-cp312-cp312-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl (253 kB)\n",
      "Using cached pydantic-2.7.2-py3-none-any.whl (409 kB)\n",
      "Using cached pydantic_core-2.18.3-cp312-cp312-macosx_11_0_arm64.whl (1.8 MB)\n",
      "Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Using cached regex-2024.5.15-cp312-cp312-macosx_11_0_arm64.whl (278 kB)\n",
      "Using cached safetensors-0.4.3-cp312-cp312-macosx_11_0_arm64.whl (411 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached torch-2.3.0-cp312-none-macosx_11_0_arm64.whl (61.0 MB)\n",
      "Using cached typing_extensions-4.12.0-py3-none-any.whl (37 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Using cached urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "Using cached yarl-1.9.4-cp312-cp312-macosx_11_0_arm64.whl (79 kB)\n",
      "Using cached httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Using cached pillow-10.3.0-cp312-cp312-macosx_11_0_arm64.whl (3.4 MB)\n",
      "Using cached scikit_learn-1.5.0-cp312-cp312-macosx_12_0_arm64.whl (11.0 MB)\n",
      "Using cached scipy-1.13.1-cp312-cp312-macosx_12_0_arm64.whl (30.4 MB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
      "Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Using cached networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "Using cached sympy-1.12.1-py3-none-any.whl (5.7 MB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Using cached MarkupSafe-2.1.5-cp312-cp312-macosx_10_9_universal2.whl (18 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: sgmllib3k, pytz, mpmath, xxhash, urllib3, tzdata, typing-extensions, tqdm, threadpoolctl, tenacity, sympy, sniffio, six, safetensors, regex, PyYAML, PyMuPDFb, pyarrow-hotfix, Pillow, packaging, orjson, numpy, networkx, mypy-extensions, multidict, MarkupSafe, jsonpointer, joblib, idna, httpx-sse, h11, fsspec, frozenlist, filelock, feedparser, dnspython, distro, dill, charset-normalizer, certifi, attrs, annotated-types, yarl, typing-inspect, SQLAlchemy, scipy, requests, python-dateutil, pymupdf, pymongo, pydantic-core, pyarrow, multiprocess, marshmallow, jsonpatch, jinja2, httpcore, anyio, aiosignal, torch, scikit-learn, pydantic, pandas, huggingface-hub, httpx, dataclasses-json, arxiv, aiohttp, tokenizers, text-generation, openai, langsmith, fireworks-ai, transformers, langchain-core, datasets, sentence-transformers, langchain-text-splitters, langchain-mongodb, langchain-fireworks, langchain-huggingface, langchain, langchain-community\n",
      "Successfully installed MarkupSafe-2.1.5 Pillow-10.3.0 PyMuPDFb-1.24.3 PyYAML-6.0.1 SQLAlchemy-2.0.30 aiohttp-3.9.5 aiosignal-1.3.1 annotated-types-0.7.0 anyio-4.4.0 arxiv-2.1.0 attrs-23.2.0 certifi-2024.2.2 charset-normalizer-3.3.2 dataclasses-json-0.6.6 datasets-2.19.1 dill-0.3.8 distro-1.9.0 dnspython-2.6.1 feedparser-6.0.10 filelock-3.14.0 fireworks-ai-0.14.0 frozenlist-1.4.1 fsspec-2024.3.1 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 httpx-sse-0.4.0 huggingface-hub-0.23.2 idna-3.7 jinja2-3.1.4 joblib-1.4.2 jsonpatch-1.33 jsonpointer-2.4 langchain-0.2.1 langchain-community-0.2.1 langchain-core-0.2.3 langchain-fireworks-0.1.3 langchain-huggingface-0.0.1 langchain-mongodb-0.1.5 langchain-text-splitters-0.2.0 langsmith-0.1.65 marshmallow-3.21.2 mpmath-1.3.0 multidict-6.0.5 multiprocess-0.70.16 mypy-extensions-1.0.0 networkx-3.3 numpy-1.26.4 openai-1.30.5 orjson-3.10.3 packaging-23.2 pandas-2.2.2 pyarrow-16.1.0 pyarrow-hotfix-0.6 pydantic-2.7.2 pydantic-core-2.18.3 pymongo-4.7.2 pymupdf-1.24.5 python-dateutil-2.9.0.post0 pytz-2024.1 regex-2024.5.15 requests-2.31.0 safetensors-0.4.3 scikit-learn-1.5.0 scipy-1.13.1 sentence-transformers-3.0.0 sgmllib3k-1.0.0 six-1.16.0 sniffio-1.3.1 sympy-1.12.1 tenacity-8.3.0 text-generation-0.7.0 threadpoolctl-3.5.0 tokenizers-0.19.1 torch-2.3.0 tqdm-4.66.4 transformers-4.41.2 typing-extensions-4.12.0 typing-inspect-0.9.0 tzdata-2024.1 urllib3-2.2.1 xxhash-3.4.1 yarl-1.9.4\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-fireworks langchain-huggingface langchain-community langchain-mongodb arxiv pymupdf datasets pymongo tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RM8rg08YhqZe"
   },
   "source": [
    "## Set Evironment Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "oXLWCWEghuOX"
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "MONGODB_URI = getpass.getpass(\"Enter your MongoDB connection string:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass(\"Enter Fireworks API key:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UUf3jtFzO4-V"
   },
   "source": [
    "## Ingest Data into MongoDB Vector Database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "referenced_widgets": [
      "cebfba144ba6418092df949783f93455",
      "09dcf4ce88064f11980bbefaad1ebc75",
      "f2bd7bda4d0c4d93b88e53aeb4e1b62d",
      "278513c5a8b04a24b1823d38107f1e50",
      "d3941c633788427abb858b21e285088f",
      "39563df9477648398456675ec51075aa",
      "f4353368efbd4c3891f805ddc3d05e1b",
      "30fe0bcd02cb47f3ba23bb480e2eaaea",
      "d17d8c8f45ee44cd87dcd787c05dbdc3",
      "62e196b6d30746578e137c50b661f946",
      "ced7f9d61e06442a960dcda95852048e",
      "7dbfebff68ff45628da832fac5233c93",
      "164d16df28d24ab796b7c9cf85174800",
      "e70e0d317f1e4e73bd95349ed1510cce",
      "41056c822b9d44559147d2b21416b956",
      "b1929fb112174c0abcd8004f6be0f880",
      "95e4af5b420242b7a6b74a18cad98961",
      "dff65b579f0746ffae8739ecb0aa5a41",
      "f73ae771c24645c79fd41409a8fc7b34",
      "20d693a09c534414a5c4c0dd58cf94ed",
      "a43c349d171e469c8cc94d48060f775b",
      "373ed3b6307741859ab297c270cf42c8"
     ]
    },
    "id": "pq4SA6r7O30i",
    "outputId": "904f4112-79fb-45cc-954b-d2b818cb2748"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since mongodb-eai/arxiv-embeddings couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /Users/apoorva.joshi/.cache/huggingface/datasets/mongodb-eai___arxiv-embeddings/default/0.0.0/489df6ceb90444598a5f73794db75a7dec209134 (last modified on Wed May 29 15:33:09 2024).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"mongodb-eai/arxiv-embeddings\")\n",
    "dataset_df = pd.DataFrame(data[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>submitter</th>\n",
       "      <th>authors</th>\n",
       "      <th>title</th>\n",
       "      <th>comments</th>\n",
       "      <th>journal-ref</th>\n",
       "      <th>doi</th>\n",
       "      <th>report-no</th>\n",
       "      <th>categories</th>\n",
       "      <th>license</th>\n",
       "      <th>abstract</th>\n",
       "      <th>versions</th>\n",
       "      <th>update_date</th>\n",
       "      <th>authors_parsed</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>704.0001</td>\n",
       "      <td>Pavel Nadolsky</td>\n",
       "      <td>C. Bal\\'azs, E. L. Berger, P. M. Nadolsky, C.-...</td>\n",
       "      <td>Calculation of prompt diphoton production cros...</td>\n",
       "      <td>37 pages, 15 figures; published version</td>\n",
       "      <td>Phys.Rev.D76:013009,2007</td>\n",
       "      <td>10.1103/PhysRevD.76.013009</td>\n",
       "      <td>ANL-HEP-PR-07-12</td>\n",
       "      <td>hep-ph</td>\n",
       "      <td>None</td>\n",
       "      <td>A fully differential calculation in perturba...</td>\n",
       "      <td>[{'version': 'v1', 'created': 'Mon, 2 Apr 2007...</td>\n",
       "      <td>1227657600000</td>\n",
       "      <td>[[Balázs, C., ], [Berger, E. L., ], [Nadolsky,...</td>\n",
       "      <td>[0.2324569076, -0.894839108, -0.242858842, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>704.0002</td>\n",
       "      <td>Louis Theran</td>\n",
       "      <td>Ileana Streinu and Louis Theran</td>\n",
       "      <td>Sparsity-certifying Graph Decompositions</td>\n",
       "      <td>To appear in Graphs and Combinatorics</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>math.CO cs.CG</td>\n",
       "      <td>http://arxiv.org/licenses/nonexclusive-distrib...</td>\n",
       "      <td>We describe a new algorithm, the $(k,\\ell)$-...</td>\n",
       "      <td>[{'version': 'v1', 'created': 'Sat, 31 Mar 200...</td>\n",
       "      <td>1229126400000</td>\n",
       "      <td>[[Streinu, Ileana, ], [Theran, Louis, ]]</td>\n",
       "      <td>[0.6949232221, 0.3588359952, 0.1817755997, 0.7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>704.0003</td>\n",
       "      <td>Hongjun Pan</td>\n",
       "      <td>Hongjun Pan</td>\n",
       "      <td>The evolution of the Earth-Moon system based o...</td>\n",
       "      <td>23 pages, 3 figures</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>physics.gen-ph</td>\n",
       "      <td>None</td>\n",
       "      <td>The evolution of Earth-Moon system is descri...</td>\n",
       "      <td>[{'version': 'v1', 'created': 'Sun, 1 Apr 2007...</td>\n",
       "      <td>1200182400000</td>\n",
       "      <td>[[Pan, Hongjun, ]]</td>\n",
       "      <td>[0.1294624656, 1.1964389086, 0.8928941488, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>704.0004</td>\n",
       "      <td>David Callan</td>\n",
       "      <td>David Callan</td>\n",
       "      <td>A determinant of Stirling cycle numbers counts...</td>\n",
       "      <td>11 pages</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>math.CO</td>\n",
       "      <td>None</td>\n",
       "      <td>We show that a determinant of Stirling cycle...</td>\n",
       "      <td>[{'version': 'v1', 'created': 'Sat, 31 Mar 200...</td>\n",
       "      <td>1179878400000</td>\n",
       "      <td>[[Callan, David, ]]</td>\n",
       "      <td>[-0.0994227678, -0.364127785, 0.5390082002, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>704.0005</td>\n",
       "      <td>Alberto Torchinsky</td>\n",
       "      <td>Wael Abu-Shammala and Alberto Torchinsky</td>\n",
       "      <td>From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...</td>\n",
       "      <td>None</td>\n",
       "      <td>Illinois J. Math. 52 (2008) no.2, 681-689</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>math.CA math.FA</td>\n",
       "      <td>None</td>\n",
       "      <td>In this paper we show how to compute the $\\L...</td>\n",
       "      <td>[{'version': 'v1', 'created': 'Mon, 2 Apr 2007...</td>\n",
       "      <td>1381795200000</td>\n",
       "      <td>[[Abu-Shammala, Wael, ], [Torchinsky, Alberto, ]]</td>\n",
       "      <td>[0.0711007342, 0.5356642008, 0.5095595121, 0.4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id           submitter  \\\n",
       "0  704.0001      Pavel Nadolsky   \n",
       "1  704.0002        Louis Theran   \n",
       "2  704.0003         Hongjun Pan   \n",
       "3  704.0004        David Callan   \n",
       "4  704.0005  Alberto Torchinsky   \n",
       "\n",
       "                                             authors  \\\n",
       "0  C. Bal\\'azs, E. L. Berger, P. M. Nadolsky, C.-...   \n",
       "1                    Ileana Streinu and Louis Theran   \n",
       "2                                        Hongjun Pan   \n",
       "3                                       David Callan   \n",
       "4           Wael Abu-Shammala and Alberto Torchinsky   \n",
       "\n",
       "                                               title  \\\n",
       "0  Calculation of prompt diphoton production cros...   \n",
       "1           Sparsity-certifying Graph Decompositions   \n",
       "2  The evolution of the Earth-Moon system based o...   \n",
       "3  A determinant of Stirling cycle numbers counts...   \n",
       "4  From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...   \n",
       "\n",
       "                                  comments  \\\n",
       "0  37 pages, 15 figures; published version   \n",
       "1    To appear in Graphs and Combinatorics   \n",
       "2                      23 pages, 3 figures   \n",
       "3                                 11 pages   \n",
       "4                                     None   \n",
       "\n",
       "                                 journal-ref                         doi  \\\n",
       "0                   Phys.Rev.D76:013009,2007  10.1103/PhysRevD.76.013009   \n",
       "1                                       None                        None   \n",
       "2                                       None                        None   \n",
       "3                                       None                        None   \n",
       "4  Illinois J. Math. 52 (2008) no.2, 681-689                        None   \n",
       "\n",
       "          report-no       categories  \\\n",
       "0  ANL-HEP-PR-07-12           hep-ph   \n",
       "1              None    math.CO cs.CG   \n",
       "2              None   physics.gen-ph   \n",
       "3              None          math.CO   \n",
       "4              None  math.CA math.FA   \n",
       "\n",
       "                                             license  \\\n",
       "0                                               None   \n",
       "1  http://arxiv.org/licenses/nonexclusive-distrib...   \n",
       "2                                               None   \n",
       "3                                               None   \n",
       "4                                               None   \n",
       "\n",
       "                                            abstract  \\\n",
       "0    A fully differential calculation in perturba...   \n",
       "1    We describe a new algorithm, the $(k,\\ell)$-...   \n",
       "2    The evolution of Earth-Moon system is descri...   \n",
       "3    We show that a determinant of Stirling cycle...   \n",
       "4    In this paper we show how to compute the $\\L...   \n",
       "\n",
       "                                            versions    update_date  \\\n",
       "0  [{'version': 'v1', 'created': 'Mon, 2 Apr 2007...  1227657600000   \n",
       "1  [{'version': 'v1', 'created': 'Sat, 31 Mar 200...  1229126400000   \n",
       "2  [{'version': 'v1', 'created': 'Sun, 1 Apr 2007...  1200182400000   \n",
       "3  [{'version': 'v1', 'created': 'Sat, 31 Mar 200...  1179878400000   \n",
       "4  [{'version': 'v1', 'created': 'Mon, 2 Apr 2007...  1381795200000   \n",
       "\n",
       "                                      authors_parsed  \\\n",
       "0  [[Balázs, C., ], [Berger, E. L., ], [Nadolsky,...   \n",
       "1           [[Streinu, Ileana, ], [Theran, Louis, ]]   \n",
       "2                                 [[Pan, Hongjun, ]]   \n",
       "3                                [[Callan, David, ]]   \n",
       "4  [[Abu-Shammala, Wael, ], [Torchinsky, Alberto, ]]   \n",
       "\n",
       "                                           embedding  \n",
       "0  [0.2324569076, -0.894839108, -0.242858842, 0.1...  \n",
       "1  [0.6949232221, 0.3588359952, 0.1817755997, 0.7...  \n",
       "2  [0.1294624656, 1.1964389086, 0.8928941488, -0....  \n",
       "3  [-0.0994227678, -0.364127785, 0.5390082002, -0...  \n",
       "4  [0.0711007342, 0.5356642008, 0.5095595121, 0.4...  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "o2gHwRjMfJlO"
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "# Initialize MongoDB python client\n",
    "client = MongoClient(MONGODB_URI)\n",
    "\n",
    "DB_NAME = \"agent_demo\"\n",
    "COLLECTION_NAME = \"knowledge\"\n",
    "ATLAS_VECTOR_SEARCH_INDEX_NAME = \"vector_index\"\n",
    "collection = client[DB_NAME][COLLECTION_NAME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zJkyy9UbffZT",
    "outputId": "c6f78ea3-fc93-4d57-95eb-98cea5bf15d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data ingestion into MongoDB completed\n"
     ]
    }
   ],
   "source": [
    "# Delete any existing records in the collection\n",
    "collection.delete_many({})\n",
    "\n",
    "# Data Ingestion\n",
    "records = dataset_df.to_dict(\"records\")\n",
    "collection.insert_many(records)\n",
    "\n",
    "print(\"Data ingestion into MongoDB completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6S1Cz9dtGPwL"
   },
   "source": [
    "## Create Vector Search Index Defintion\n",
    "\n",
    "```\n",
    "{\n",
    "  \"fields\": [\n",
    "    {\n",
    "      \"type\": \"vector\",\n",
    "      \"path\": \"embedding\",\n",
    "      \"numDimensions\": 1024,\n",
    "      \"similarity\": \"cosine\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create MongoDB Vector Store Retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_mongodb import MongoDBAtlasVectorSearch\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"mixedbread-ai/mxbai-embed-large-v1\")\n",
    "\n",
    "# Vector Store Creation\n",
    "vector_store = MongoDBAtlasVectorSearch.from_connection_string(\n",
    "    connection_string=MONGODB_URI,\n",
    "    namespace=DB_NAME + \".\" + COLLECTION_NAME,\n",
    "    embedding=embedding_model,\n",
    "    index_name=ATLAS_VECTOR_SEARCH_INDEX_NAME,\n",
    "    text_key=\"abstract\",\n",
    ")\n",
    "\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_fireworks import ChatFireworks\n",
    "\n",
    "llm = ChatFireworks(\n",
    "    model=\"accounts/fireworks/models/firefunction-v1\", temperature=0.0, max_tokens=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZfheX5FiIhU"
   },
   "source": [
    "## Create Agent Tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "id": "3eufR9H8gopU"
   },
   "outputs": [],
   "source": [
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from langchain.tools import BaseTool, tool\n",
    "from typing import Type\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_paper_metadata_from_arxiv(topic: str) -> list:\n",
    "    \"\"\"\n",
    "    Fetch and return paper metadata for 10 arxiv papers matching the given topic, for example: Retrieval Augmented Generation.\n",
    "\n",
    "    Args:\n",
    "    topic (str): The topic to find papers for on arXiv.\n",
    "\n",
    "    Returns:\n",
    "    list: Metadata about the papers matching the topic.\n",
    "    \"\"\"\n",
    "    docs = ArxivLoader(query=topic, load_max_docs=5).load()\n",
    "    # Extract just the metadata from each document\n",
    "    metadata = [doc.metadata for doc in docs]\n",
    "    return metadata\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_paper_summary_from_arxiv(id: str) -> list:\n",
    "    \"\"\"\n",
    "    Fetch and return the summary for a single research paper from arXiv given the paper ID, for example: 1605.08386.\n",
    "\n",
    "    Args:\n",
    "    id (str): The paper ID.\n",
    "\n",
    "    Returns:\n",
    "    str: Summary of the paper.\n",
    "    \"\"\"\n",
    "    doc = ArxivLoader(query=id, load_max_docs=1).get_summaries_as_docs()\n",
    "    if len(doc) == 0:\n",
    "        return \"No summary found for this paper.\"\n",
    "    return doc[0].page_content\n",
    "\n",
    "\n",
    "@tool\n",
    "def answer_questions_about_topics(query: str) -> list:\n",
    "    \"\"\"\n",
    "    Answer questions about a given topic based on information in the knowledge base.\n",
    "\n",
    "    Args:\n",
    "    query (str): User query about a topic.\n",
    "\n",
    "    Returns:\n",
    "    str: Information about the topic.\n",
    "    \"\"\"\n",
    "    retrieve = {\n",
    "        \"context\": retriever\n",
    "        | (lambda docs: \"\\n\\n\".join([d.page_content for d in docs])),\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    template = \"\"\"Answer the question based only on the following context. If no context is provided, say I do not know: \\\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "    # Defining the chat prompt\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    # Parse output as a string\n",
    "    parse_output = StrOutputParser()\n",
    "    # Retrieval chain\n",
    "    retrieval_chain = retrieve | prompt | llm | parse_output\n",
    "\n",
    "    answer = retrieval_chain.invoke(query)\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Published': '2022-02-13',\n",
       "  'Title': 'A Survey on Retrieval-Augmented Text Generation',\n",
       "  'Authors': 'Huayang Li, Yixuan Su, Deng Cai, Yan Wang, Lemao Liu',\n",
       "  'Summary': 'Recently, retrieval-augmented text generation attracted increasing attention\\nof the computational linguistics community. Compared with conventional\\ngeneration models, retrieval-augmented text generation has remarkable\\nadvantages and particularly has achieved state-of-the-art performance in many\\nNLP tasks. This paper aims to conduct a survey about retrieval-augmented text\\ngeneration. It firstly highlights the generic paradigm of retrieval-augmented\\ngeneration, and then it reviews notable approaches according to different tasks\\nincluding dialogue response generation, machine translation, and other\\ngeneration tasks. Finally, it points out some important directions on top of\\nrecent methods to facilitate future research.'},\n",
       " {'Published': '2024-05-12',\n",
       "  'Title': 'DuetRAG: Collaborative Retrieval-Augmented Generation',\n",
       "  'Authors': 'Dian Jiao, Li Cai, Jingsheng Huang, Wenqiao Zhang, Siliang Tang, Yueting Zhuang',\n",
       "  'Summary': \"Retrieval-Augmented Generation (RAG) methods augment the input of Large\\nLanguage Models (LLMs) with relevant retrieved passages, reducing factual\\nerrors in knowledge-intensive tasks. However, contemporary RAG approaches\\nsuffer from irrelevant knowledge retrieval issues in complex domain questions\\n(e.g., HotPot QA) due to the lack of corresponding domain knowledge, leading to\\nlow-quality generations. To address this issue, we propose a novel\\nCollaborative Retrieval-Augmented Generation framework, DuetRAG. Our\\nbootstrapping philosophy is to simultaneously integrate the domain fintuning\\nand RAG models to improve the knowledge retrieval quality, thereby enhancing\\ngeneration quality. Finally, we demonstrate DuetRAG' s matches with expert\\nhuman researchers on HotPot QA.\"},\n",
       " {'Published': '2023-12-09',\n",
       "  'Title': 'Context Tuning for Retrieval Augmented Generation',\n",
       "  'Authors': 'Raviteja Anantha, Tharun Bethi, Danil Vodianik, Srinivas Chappidi',\n",
       "  'Summary': \"Large language models (LLMs) have the remarkable ability to solve new tasks\\nwith just a few examples, but they need access to the right tools. Retrieval\\nAugmented Generation (RAG) addresses this problem by retrieving a list of\\nrelevant tools for a given task. However, RAG's tool retrieval step requires\\nall the required information to be explicitly present in the query. This is a\\nlimitation, as semantic search, the widely adopted tool retrieval method, can\\nfail when the query is incomplete or lacks context. To address this limitation,\\nwe propose Context Tuning for RAG, which employs a smart context retrieval\\nsystem to fetch relevant information that improves both tool retrieval and plan\\ngeneration. Our lightweight context retrieval model uses numerical,\\ncategorical, and habitual usage signals to retrieve and rank context items. Our\\nempirical results demonstrate that context tuning significantly enhances\\nsemantic search, achieving a 3.5-fold and 1.5-fold improvement in Recall@K for\\ncontext retrieval and tool retrieval tasks respectively, and resulting in an\\n11.6% increase in LLM-based planner accuracy. Additionally, we show that our\\nproposed lightweight model using Reciprocal Rank Fusion (RRF) with LambdaMART\\noutperforms GPT-4 based retrieval. Moreover, we observe context augmentation at\\nplan generation, even after tool retrieval, reduces hallucination.\"},\n",
       " {'Published': '2024-02-16',\n",
       "  'Title': 'Corrective Retrieval Augmented Generation',\n",
       "  'Authors': 'Shi-Qi Yan, Jia-Chen Gu, Yun Zhu, Zhen-Hua Ling',\n",
       "  'Summary': 'Large language models (LLMs) inevitably exhibit hallucinations since the\\naccuracy of generated texts cannot be secured solely by the parametric\\nknowledge they encapsulate. Although retrieval-augmented generation (RAG) is a\\npracticable complement to LLMs, it relies heavily on the relevance of retrieved\\ndocuments, raising concerns about how the model behaves if retrieval goes\\nwrong. To this end, we propose the Corrective Retrieval Augmented Generation\\n(CRAG) to improve the robustness of generation. Specifically, a lightweight\\nretrieval evaluator is designed to assess the overall quality of retrieved\\ndocuments for a query, returning a confidence degree based on which different\\nknowledge retrieval actions can be triggered. Since retrieval from static and\\nlimited corpora can only return sub-optimal documents, large-scale web searches\\nare utilized as an extension for augmenting the retrieval results. Besides, a\\ndecompose-then-recompose algorithm is designed for retrieved documents to\\nselectively focus on key information and filter out irrelevant information in\\nthem. CRAG is plug-and-play and can be seamlessly coupled with various\\nRAG-based approaches. Experiments on four datasets covering short- and\\nlong-form generation tasks show that CRAG can significantly improve the\\nperformance of RAG-based approaches.'},\n",
       " {'Published': '2023-05-27',\n",
       "  'Title': 'Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In',\n",
       "  'Authors': 'Zichun Yu, Chenyan Xiong, Shi Yu, Zhiyuan Liu',\n",
       "  'Summary': \"Retrieval augmentation can aid language models (LMs) in knowledge-intensive\\ntasks by supplying them with external information. Prior works on retrieval\\naugmentation usually jointly fine-tune the retriever and the LM, making them\\nclosely coupled. In this paper, we explore the scheme of generic retrieval\\nplug-in: the retriever is to assist target LMs that may not be known beforehand\\nor are unable to be fine-tuned together. To retrieve useful documents for\\nunseen target LMs, we propose augmentation-adapted retriever (AAR), which\\nlearns LM's preferences obtained from a known source LM. Experiments on the\\nMMLU and PopQA datasets demonstrate that our AAR trained with a small source LM\\nis able to significantly improve the zero-shot generalization of larger target\\nLMs ranging from 250M Flan-T5 to 175B InstructGPT. Further analysis indicates\\nthat the preferences of different LMs overlap, enabling AAR trained with a\\nsingle source LM to serve as a generic plug-in for various target LMs. Our code\\nis open-sourced at https://github.com/OpenMatch/Augmentation-Adapted-Retriever.\"}]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_paper_metadata_from_arxiv.invoke(\"Retrieval Augmented Generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We determine the non-perturbatively renormalized axial current for O($a$)\\nimproved lattice QCD with Wilson quarks. Our strategy is based on the chirally\\nrotated Schr\\\\\"odinger functional and can be generalized to other finite (ratios\\nof) renormalization constants which are traditionally obtained by imposing\\ncontinuum chiral Ward identities as normalization conditions. Compared to the\\nlatter we achieve an error reduction up to one order of magnitude. Our results\\nhave already enabled the setting of the scale for the $N_{\\\\rm f}=2+1$ CLS\\nensembles [1] and are thus an essential ingredient for the recent $\\\\alpha_s$\\ndetermination by the ALPHA collaboration [2]. In this paper we shortly review\\nthe strategy and present our results for both $N_{\\\\rm f}=2$ and $N_{\\\\rm f}=3$\\nlattice QCD, where we match the $\\\\beta$-values of the CLS gauge configurations.\\nIn addition to the axial current renormalization, we also present precise\\nresults for the renormalized local vector current.'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_paper_summary_from_arxiv.invoke(\"1808.09236\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No summary found for this paper.'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_paper_summary_from_arxiv.invoke(\"808.09236\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Partial cubes are isometric subgraphs of hypercubes. They are characterized by structures on a graph defined by means of semicubes, and Djokovi\\\\'{c}'s and Winkler's relations. These structures are employed in the paper to characterize bipartite graphs and partial cubes of arbitrary dimension.\""
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_questions_about_topics.invoke(\"What are partial cubes?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    get_paper_metadata_from_arxiv,\n",
    "    get_paper_summary_from_arxiv,\n",
    "    answer_questions_about_topics,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "id": "RY13DrVXFDrm"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "\n",
    "instructions = \"\"\"You are a helpful research assistant.\n",
    "When answering questions about a particular topic, only use the answer_questions_about_topics tool.\n",
    "DO NOT use your own knowledge.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", instructions),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(\"agent_scratchpad\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent, tools=tools, verbose=True, handle_parsing_errors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `get_paper_metadata_from_arxiv` with `{'topic': 'prompt compression'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'Published': '2024-03-30', 'Title': 'PROMPT-SAW: Leveraging Relation-Aware Graphs for Textual Prompt Compression', 'Authors': 'Muhammad Asif Ali, Zhengping Li, Shu Yang, Keyuan Cheng, Yang Cao, Tianhao Huang, Lijie Hu, Lu Yu, Di Wang', 'Summary': \"Large language models (LLMs) have shown exceptional abilities for multiple\\ndifferent natural language processing tasks. While prompting is a crucial tool\\nfor LLM inference, we observe that there is a significant cost associated with\\nexceedingly lengthy prompts. Existing attempts to compress lengthy prompts lead\\nto sub-standard results in terms of readability and interpretability of the\\ncompressed prompt, with a detrimental impact on prompt utility. To address\\nthis, we propose PROMPT-SAW: Prompt compresSion via Relation AWare graphs, an\\neffective strategy for prompt compression over task-agnostic and task-aware\\nprompts. PROMPT-SAW uses the prompt's textual information to build a graph,\\nlater extracts key information elements in the graph to come up with the\\ncompressed prompt. We also propose GSM8K-AUG, i.e., an extended version of the\\nexisting GSM8k benchmark for task-agnostic prompts in order to provide a\\ncomprehensive evaluation platform. Experimental evaluation using benchmark\\ndatasets shows that prompts compressed by PROMPT-SAW are not only better in\\nterms of readability, but they also outperform the best-performing baseline\\nmodels by up to 14.3 and 13.7 respectively for task-aware and task-agnostic\\nsettings while compressing the original prompt text by 33.0 and 56.7.\"}, {'Published': '2024-02-25', 'Title': 'Say More with Less: Understanding Prompt Learning Behaviors through Gist Compression', 'Authors': 'Xinze Li, Zhenghao Liu, Chenyan Xiong, Shi Yu, Yukun Yan, Shuo Wang, Ge Yu', 'Summary': 'Large language models (LLMs) require lengthy prompts as the input context to\\nproduce output aligned with user intentions, a process that incurs extra costs\\nduring inference. In this paper, we propose the Gist COnditioned deCOding\\n(Gist-COCO) model, introducing a novel method for compressing prompts which\\nalso can assist the prompt interpretation and engineering. Gist-COCO employs an\\nencoder-decoder based language model and then incorporates an additional\\nencoder as a plugin module to compress prompts with inputs using gist tokens.\\nIt finetunes the compression plugin module and uses the representations of gist\\ntokens to emulate the raw prompts in the vanilla language model. By verbalizing\\nthe representations of gist tokens into gist prompts, the compression ability\\nof Gist-COCO can be generalized to different LLMs with high compression rates.\\nOur experiments demonstrate that Gist-COCO outperforms previous prompt\\ncompression models in both passage and instruction compression tasks. Further\\nanalysis on gist verbalization results suggests that our gist prompts serve\\ndifferent functions in aiding language models. They may directly provide\\npotential answers, generate the chain-of-thought, or simply repeat the inputs.\\nAll data and codes are available at https://github.com/OpenMatch/Gist-COCO .'}, {'Published': '2023-10-10', 'Title': 'Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt', 'Authors': 'Zhaozhuo Xu, Zirui Liu, Beidi Chen, Yuxin Tang, Jue Wang, Kaixiong Zhou, Xia Hu, Anshumali Shrivastava', 'Summary': \"While the numerous parameters in Large Language Models (LLMs) contribute to\\ntheir superior performance, this massive scale makes them inefficient and\\nmemory-hungry. Thus, they are hard to deploy on commodity hardware, such as one\\nsingle GPU. Given the memory and power constraints of such devices, model\\ncompression methods are widely employed to reduce both the model size and\\ninference latency, which essentially trades off model quality in return for\\nimproved efficiency. Thus, optimizing this accuracy-efficiency trade-off is\\ncrucial for the LLM deployment on commodity hardware. In this paper, we\\nintroduce a new perspective to optimize this trade-off by prompting compressed\\nmodels. Specifically, we first observe that for certain questions, the\\ngeneration quality of a compressed LLM can be significantly improved by adding\\ncarefully designed hard prompts, though this isn't the case for all questions.\\nBased on this observation, we propose a soft prompt learning method where we\\nexpose the compressed model to the prompt learning process, aiming to enhance\\nthe performance of prompts. Our experimental analysis suggests our soft prompt\\nstrategy greatly improves the performance of the 8x compressed LLaMA-7B model\\n(with a joint 4-bit quantization and 50% weight pruning compression), allowing\\nthem to match their uncompressed counterparts on popular benchmarks. Also, we\\ndemonstrate that these learned prompts can be transferred across various\\ndatasets, tasks, and compression levels. Hence with this transferability, we\\ncan stitch the soft prompt to a newly compressed model to improve the test-time\\naccuracy in an ``in-situ'' way.\"}, {'Published': '2024-04-26', 'Title': 'PromptCIR: Blind Compressed Image Restoration with Prompt Learning', 'Authors': 'Bingchen Li, Xin Li, Yiting Lu, Ruoyu Feng, Mengxi Guo, Shijie Zhao, Li Zhang, Zhibo Chen', 'Summary': 'Blind Compressed Image Restoration (CIR) has garnered significant attention\\ndue to its practical applications. It aims to mitigate compression artifacts\\ncaused by unknown quality factors, particularly with JPEG codecs. Existing\\nworks on blind CIR often seek assistance from a quality factor prediction\\nnetwork to facilitate their network to restore compressed images. However, the\\npredicted numerical quality factor lacks spatial information, preventing\\nnetwork adaptability toward image contents. Recent studies in\\nprompt-learning-based image restoration have showcased the potential of prompts\\nto generalize across varied degradation types and degrees. This motivated us to\\ndesign a prompt-learning-based compressed image restoration network, dubbed\\nPromptCIR, which can effectively restore images from various compress levels.\\nSpecifically, PromptCIR exploits prompts to encode compression information\\nimplicitly, where prompts directly interact with soft weights generated from\\nimage features, thus providing dynamic content-aware and distortion-aware\\nguidance for the restoration process. The light-weight prompts enable our\\nmethod to adapt to different compression levels, while introducing minimal\\nparameter overhead. Overall, PromptCIR leverages the powerful transformer-based\\nbackbone with the dynamic prompt module to proficiently handle blind CIR tasks,\\nwinning first place in the NTIRE 2024 challenge of blind compressed image\\nenhancement track. Extensive experiments have validated the effectiveness of\\nour proposed PromptCIR. The code is available at\\nhttps://github.com/lbc12345/PromptCIR-NTIRE24.'}, {'Published': '2024-04-02', 'Title': 'Learning to Compress Prompt in Natural Language Formats', 'Authors': 'Yu-Neng Chuang, Tianwei Xing, Chia-Yuan Chang, Zirui Liu, Xun Chen, Xia Hu', 'Summary': 'Large language models (LLMs) are great at processing multiple natural\\nlanguage processing tasks, but their abilities are constrained by inferior\\nperformance with long context, slow inference speed, and the high cost of\\ncomputing the results. Deploying LLMs with precise and informative context\\nhelps users process large-scale datasets more effectively and cost-efficiently.\\nExisting works rely on compressing long prompt contexts into soft prompts.\\nHowever, soft prompt compression encounters limitations in transferability\\nacross different LLMs, especially API-based LLMs. To this end, this work aims\\nto compress lengthy prompts in the form of natural language with LLM\\ntransferability. This poses two challenges: (i) Natural Language (NL) prompts\\nare incompatible with back-propagation, and (ii) NL prompts lack flexibility in\\nimposing length constraints. In this work, we propose a Natural Language Prompt\\nEncapsulation (Nano-Capsulator) framework compressing original prompts into NL\\nformatted Capsule Prompt while maintaining the prompt utility and\\ntransferability. Specifically, to tackle the first challenge, the\\nNano-Capsulator is optimized by a reward function that interacts with the\\nproposed semantics preserving loss. To address the second question, the\\nNano-Capsulator is optimized by a reward function featuring length constraints.\\nExperimental results demonstrate that the Capsule Prompt can reduce 81.4% of\\nthe original length, decrease inference latency up to 4.5x, and save 80.1% of\\nbudget overheads while providing transferability across diverse LLMs and\\ndifferent datasets.'}]\u001b[0m\u001b[32;1m\u001b[1;3mHere are some papers on the topic of prompt compression:\n",
      "\n",
      "1. \"PROMPT-SAW: Leveraging Relation-Aware Graphs for Textual Prompt Compression\" by Muhammad Asif Ali, Zhengping Li, Shu Yang, Keyuan Cheng, Yang Cao, Tianhao Huang, Lijie Hu, Lu Yu, Di Wang\n",
      "2. \"Say More with Less: Understanding Prompt Learning Behaviors through Gist Compression\" by Xinze Li, Zhenghao Liu, Chenyan Xiong, Shi Yu, Yukun Yan, Shuo Wang, Ge Yu\n",
      "3. \"Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt\" by Zhaozhuo Xu, Zirui Liu, Beidi Chen, Yuxin Tang, Jue Wang, Kaixiong Zhou, Xia Hu, Anshumali Shrivastava\n",
      "4. \"PromptCIR: Blind Compressed Image Restoration with Prompt Learning\" by Bingchen Li, Xin Li, Yiting Lu, Ruoyu Feng, Mengxi Guo, Shijie Zhao, Li Zhang, Zhibo Chen\n",
      "5. \"Learning to Compress Prompt in Natural Language Formats\" by Yu-Neng Chuang, Tianwei Xing, Chia-Yuan Chang, Zirui Liu, Xun Chen, Xia Hu\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Give me papers on the topic prompt compression.',\n",
       " 'output': 'Here are some papers on the topic of prompt compression:\\n\\n1. \"PROMPT-SAW: Leveraging Relation-Aware Graphs for Textual Prompt Compression\" by Muhammad Asif Ali, Zhengping Li, Shu Yang, Keyuan Cheng, Yang Cao, Tianhao Huang, Lijie Hu, Lu Yu, Di Wang\\n2. \"Say More with Less: Understanding Prompt Learning Behaviors through Gist Compression\" by Xinze Li, Zhenghao Liu, Chenyan Xiong, Shi Yu, Yukun Yan, Shuo Wang, Ge Yu\\n3. \"Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt\" by Zhaozhuo Xu, Zirui Liu, Beidi Chen, Yuxin Tang, Jue Wang, Kaixiong Zhou, Xia Hu, Anshumali Shrivastava\\n4. \"PromptCIR: Blind Compressed Image Restoration with Prompt Learning\" by Bingchen Li, Xin Li, Yiting Lu, Ruoyu Feng, Mengxi Guo, Shijie Zhao, Li Zhang, Zhibo Chen\\n5. \"Learning to Compress Prompt in Natural Language Formats\" by Yu-Neng Chuang, Tianwei Xing, Chia-Yuan Chang, Zirui Liu, Xun Chen, Xia Hu'}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"Give me papers on the topic prompt compression.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `get_paper_summary_from_arxiv` with `{'id': '2401.00002'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mWe show evidence of particle acceleration at GEV energies associated directly\n",
      "with protons from the prompt emission of a long-duration M6-class solar flare\n",
      "on July 17, 2023, rather than from protons acceleration by shocks from its\n",
      "associated Coronal Mass Ejection (CME), which erupted with a speed of 1342\n",
      "km/s. Solar Energetic Particles (SEP) accelerated by the blast have reached\n",
      "Earth, up to an almost S3 (strong) category of a radiation storm on the NOAA\n",
      "scale. Also, we show a temporal correlation between the fast rising of GOES-16\n",
      "proton and muon excess at ground level in the count rate of the New-Tupi muon\n",
      "detector at the central SAA region. A Monte Carlo spectral analysis based on\n",
      "muon excess at New-Tupi is consistent with the acceleration of electrons and\n",
      "protons (ions) up to relativistic energies (GeV energy range) in the impulsive\n",
      "phase of the flare. In addition, we present another two marginal particle\n",
      "excesses (with low confidence) at ground-level detectors in correlation with\n",
      "the solar flare prompt emission.\u001b[0m\u001b[32;1m\u001b[1;3mThe paper with ID 2401.00002 is about the discovery of particle acceleration at GEV energies associated directly with protons from the prompt emission of a long-duration M6-class solar flare on July 17, 2023. The paper also discusses the correlation between the fast rising of GOES-16 proton and muon excess at ground level in the count rate of the New-Tupi muon detector at the central SAA region. Additionally, the paper presents two marginal particle excesses (with low confidence) at ground-level detectors in correlation with the solar flare prompt emission.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Give me a summary of paper id 2401.00002',\n",
       " 'output': 'The paper with ID 2401.00002 is about the discovery of particle acceleration at GEV energies associated directly with protons from the prompt emission of a long-duration M6-class solar flare on July 17, 2023. The paper also discusses the correlation between the fast rising of GOES-16 proton and muon excess at ground level in the count rate of the New-Tupi muon detector at the central SAA region. Additionally, the paper presents two marginal particle excesses (with low confidence) at ground-level detectors in correlation with the solar flare prompt emission.'}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"Give me a summary of paper id 2401.00002\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `answer_questions_about_topics` with `{'query': 'Retrieval Augmented Generation'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m I do not know. The provided context does not mention anything about \"Retrieval Augmented Generation.\" <|end|>\u001b[0m\u001b[32;1m\u001b[1;3mI apologize, but I don't have any information about Retrieval Augmented Generation at the moment. Please try again later.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Tell me about Retrieval Augmented Generation.',\n",
       " 'output': \"I apologize, but I don't have any information about Retrieval Augmented Generation at the moment. Please try again later.\"}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"Tell me about Retrieval Augmented Generation.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z4NU4ZjGl0WC"
   },
   "source": [
    "## Agent Memory Using MongoDB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1A-3Fg1cjwyK"
   },
   "outputs": [],
   "source": [
    "from langchain_mongodb.chat_message_histories import MongoDBChatMessageHistory\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> MongoDBChatMessageHistory:\n",
    "    return MongoDBChatMessageHistory(\n",
    "        MONGO_URI, session_id, database_name=DB_NAME, collection_name=\"history\"\n",
    "    )\n",
    "\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\", chat_memory=get_session_history(\"my-session\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O9TqMKyvKhvq"
   },
   "source": [
    "## Agent Creation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wI4uBAmNF5ll"
   },
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True,\n",
    "    memory=memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RGB4pWTylmFy"
   },
   "source": [
    "## Agent Exectution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DM8GtbjgIJXt",
    "outputId": "328c36f6-b4a0-4a32-e7d6-b606ca044517"
   },
   "outputs": [],
   "source": [
    "agent_executor.invoke(\n",
    "    {\"input\": \"Get me a list of research papers on the topic Prompt Compression\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oBvTS8S0JUPb",
    "outputId": "13fbb430-eb49-4b91-dd04-33bcc33ecc00"
   },
   "outputs": [],
   "source": [
    "agent_executor.invoke({\"input\": \"Get me the abstract of the first paper you found\"})"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "RM8rg08YhqZe",
    "UUf3jtFzO4-V",
    "Sm5QZdshwJLN"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "09dcf4ce88064f11980bbefaad1ebc75": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_39563df9477648398456675ec51075aa",
      "placeholder": "​",
      "style": "IPY_MODEL_f4353368efbd4c3891f805ddc3d05e1b",
      "value": "Downloading data: 100%"
     }
    },
    "164d16df28d24ab796b7c9cf85174800": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_95e4af5b420242b7a6b74a18cad98961",
      "placeholder": "​",
      "style": "IPY_MODEL_dff65b579f0746ffae8739ecb0aa5a41",
      "value": "Generating train split: "
     }
    },
    "20d693a09c534414a5c4c0dd58cf94ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "278513c5a8b04a24b1823d38107f1e50": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_62e196b6d30746578e137c50b661f946",
      "placeholder": "​",
      "style": "IPY_MODEL_ced7f9d61e06442a960dcda95852048e",
      "value": " 102M/102M [00:06&lt;00:00, 20.6MB/s]"
     }
    },
    "30fe0bcd02cb47f3ba23bb480e2eaaea": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "373ed3b6307741859ab297c270cf42c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "39563df9477648398456675ec51075aa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "41056c822b9d44559147d2b21416b956": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a43c349d171e469c8cc94d48060f775b",
      "placeholder": "​",
      "style": "IPY_MODEL_373ed3b6307741859ab297c270cf42c8",
      "value": " 50000/0 [00:04&lt;00:00, 12390.43 examples/s]"
     }
    },
    "62e196b6d30746578e137c50b661f946": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7dbfebff68ff45628da832fac5233c93": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_164d16df28d24ab796b7c9cf85174800",
       "IPY_MODEL_e70e0d317f1e4e73bd95349ed1510cce",
       "IPY_MODEL_41056c822b9d44559147d2b21416b956"
      ],
      "layout": "IPY_MODEL_b1929fb112174c0abcd8004f6be0f880"
     }
    },
    "95e4af5b420242b7a6b74a18cad98961": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a43c349d171e469c8cc94d48060f775b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b1929fb112174c0abcd8004f6be0f880": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cebfba144ba6418092df949783f93455": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_09dcf4ce88064f11980bbefaad1ebc75",
       "IPY_MODEL_f2bd7bda4d0c4d93b88e53aeb4e1b62d",
       "IPY_MODEL_278513c5a8b04a24b1823d38107f1e50"
      ],
      "layout": "IPY_MODEL_d3941c633788427abb858b21e285088f"
     }
    },
    "ced7f9d61e06442a960dcda95852048e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d17d8c8f45ee44cd87dcd787c05dbdc3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d3941c633788427abb858b21e285088f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dff65b579f0746ffae8739ecb0aa5a41": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e70e0d317f1e4e73bd95349ed1510cce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f73ae771c24645c79fd41409a8fc7b34",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_20d693a09c534414a5c4c0dd58cf94ed",
      "value": 1
     }
    },
    "f2bd7bda4d0c4d93b88e53aeb4e1b62d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_30fe0bcd02cb47f3ba23bb480e2eaaea",
      "max": 102202622,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d17d8c8f45ee44cd87dcd787c05dbdc3",
      "value": 102202622
     }
    },
    "f4353368efbd4c3891f805ddc3d05e1b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f73ae771c24645c79fd41409a8fc7b34": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
