{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3kMALXaMv-MS"
   },
   "source": [
    "## Install Libraries\n",
    "\n",
    "One-click install of all the required libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cxTXczeTghzU",
    "outputId": "ae3a81b2-cba6-42fc-f593-8646bff77b14"
   },
   "outputs": [],
   "source": [
    "!pip install -qU langchain langchainhub langchain-community langchain-fireworks langchain-huggingface langchain-mongodb arxiv pymupdf datasets pymongo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RM8rg08YhqZe"
   },
   "source": [
    "## Setup Pre-requisites\n",
    "\n",
    "You should have obtained the MongoDB connection string and Fireworks API Key during the first hands-on breakout. Set those here when prompted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "oXLWCWEghuOX"
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "MONGODB_URI = getpass.getpass(\"Enter your MongoDB connection string:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass(\"Enter Fireworks API key:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UUf3jtFzO4-V"
   },
   "source": [
    "## Ingest Data into MongoDB Atlas\n",
    "\n",
    "We will use MongoDB Atlas as the vector store/knowledge base for one of the agent tools. But first, we need to create the knowledge base.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "referenced_widgets": [
      "cebfba144ba6418092df949783f93455",
      "09dcf4ce88064f11980bbefaad1ebc75",
      "f2bd7bda4d0c4d93b88e53aeb4e1b62d",
      "278513c5a8b04a24b1823d38107f1e50",
      "d3941c633788427abb858b21e285088f",
      "39563df9477648398456675ec51075aa",
      "f4353368efbd4c3891f805ddc3d05e1b",
      "30fe0bcd02cb47f3ba23bb480e2eaaea",
      "d17d8c8f45ee44cd87dcd787c05dbdc3",
      "62e196b6d30746578e137c50b661f946",
      "ced7f9d61e06442a960dcda95852048e",
      "7dbfebff68ff45628da832fac5233c93",
      "164d16df28d24ab796b7c9cf85174800",
      "e70e0d317f1e4e73bd95349ed1510cce",
      "41056c822b9d44559147d2b21416b956",
      "b1929fb112174c0abcd8004f6be0f880",
      "95e4af5b420242b7a6b74a18cad98961",
      "dff65b579f0746ffae8739ecb0aa5a41",
      "f73ae771c24645c79fd41409a8fc7b34",
      "20d693a09c534414a5c4c0dd58cf94ed",
      "a43c349d171e469c8cc94d48060f775b",
      "373ed3b6307741859ab297c270cf42c8"
     ]
    },
    "id": "pq4SA6r7O30i",
    "outputId": "904f4112-79fb-45cc-954b-d2b818cb2748"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apoorva.joshi/Documents/ai-agents-workshop/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Download a dataset consisting of a subset of ArxiV papers along with their embedded abstracts.\n",
    "# Available at https://huggingface.co/datasets/mongodb-eai/arxiv-embeddings\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"mongodb-eai/arxiv-embeddings\")\n",
    "dataset_df = pd.DataFrame(data[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>submitter</th>\n",
       "      <th>authors</th>\n",
       "      <th>title</th>\n",
       "      <th>comments</th>\n",
       "      <th>journal-ref</th>\n",
       "      <th>doi</th>\n",
       "      <th>report-no</th>\n",
       "      <th>categories</th>\n",
       "      <th>license</th>\n",
       "      <th>abstract</th>\n",
       "      <th>versions</th>\n",
       "      <th>update_date</th>\n",
       "      <th>authors_parsed</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>704.0001</td>\n",
       "      <td>Pavel Nadolsky</td>\n",
       "      <td>C. Bal\\'azs, E. L. Berger, P. M. Nadolsky, C.-...</td>\n",
       "      <td>Calculation of prompt diphoton production cros...</td>\n",
       "      <td>37 pages, 15 figures; published version</td>\n",
       "      <td>Phys.Rev.D76:013009,2007</td>\n",
       "      <td>10.1103/PhysRevD.76.013009</td>\n",
       "      <td>ANL-HEP-PR-07-12</td>\n",
       "      <td>hep-ph</td>\n",
       "      <td>None</td>\n",
       "      <td>A fully differential calculation in perturba...</td>\n",
       "      <td>[{'version': 'v1', 'created': 'Mon, 2 Apr 2007...</td>\n",
       "      <td>1227657600000</td>\n",
       "      <td>[[Bal√°zs, C., ], [Berger, E. L., ], [Nadolsky,...</td>\n",
       "      <td>[0.2324569076, -0.894839108, -0.242858842, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>704.0002</td>\n",
       "      <td>Louis Theran</td>\n",
       "      <td>Ileana Streinu and Louis Theran</td>\n",
       "      <td>Sparsity-certifying Graph Decompositions</td>\n",
       "      <td>To appear in Graphs and Combinatorics</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>math.CO cs.CG</td>\n",
       "      <td>http://arxiv.org/licenses/nonexclusive-distrib...</td>\n",
       "      <td>We describe a new algorithm, the $(k,\\ell)$-...</td>\n",
       "      <td>[{'version': 'v1', 'created': 'Sat, 31 Mar 200...</td>\n",
       "      <td>1229126400000</td>\n",
       "      <td>[[Streinu, Ileana, ], [Theran, Louis, ]]</td>\n",
       "      <td>[0.6949232221, 0.3588359952, 0.1817755997, 0.7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>704.0003</td>\n",
       "      <td>Hongjun Pan</td>\n",
       "      <td>Hongjun Pan</td>\n",
       "      <td>The evolution of the Earth-Moon system based o...</td>\n",
       "      <td>23 pages, 3 figures</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>physics.gen-ph</td>\n",
       "      <td>None</td>\n",
       "      <td>The evolution of Earth-Moon system is descri...</td>\n",
       "      <td>[{'version': 'v1', 'created': 'Sun, 1 Apr 2007...</td>\n",
       "      <td>1200182400000</td>\n",
       "      <td>[[Pan, Hongjun, ]]</td>\n",
       "      <td>[0.1294624656, 1.1964389086, 0.8928941488, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>704.0004</td>\n",
       "      <td>David Callan</td>\n",
       "      <td>David Callan</td>\n",
       "      <td>A determinant of Stirling cycle numbers counts...</td>\n",
       "      <td>11 pages</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>math.CO</td>\n",
       "      <td>None</td>\n",
       "      <td>We show that a determinant of Stirling cycle...</td>\n",
       "      <td>[{'version': 'v1', 'created': 'Sat, 31 Mar 200...</td>\n",
       "      <td>1179878400000</td>\n",
       "      <td>[[Callan, David, ]]</td>\n",
       "      <td>[-0.0994227678, -0.364127785, 0.5390082002, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>704.0005</td>\n",
       "      <td>Alberto Torchinsky</td>\n",
       "      <td>Wael Abu-Shammala and Alberto Torchinsky</td>\n",
       "      <td>From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...</td>\n",
       "      <td>None</td>\n",
       "      <td>Illinois J. Math. 52 (2008) no.2, 681-689</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>math.CA math.FA</td>\n",
       "      <td>None</td>\n",
       "      <td>In this paper we show how to compute the $\\L...</td>\n",
       "      <td>[{'version': 'v1', 'created': 'Mon, 2 Apr 2007...</td>\n",
       "      <td>1381795200000</td>\n",
       "      <td>[[Abu-Shammala, Wael, ], [Torchinsky, Alberto, ]]</td>\n",
       "      <td>[0.0711007342, 0.5356642008, 0.5095595121, 0.4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id           submitter  \\\n",
       "0  704.0001      Pavel Nadolsky   \n",
       "1  704.0002        Louis Theran   \n",
       "2  704.0003         Hongjun Pan   \n",
       "3  704.0004        David Callan   \n",
       "4  704.0005  Alberto Torchinsky   \n",
       "\n",
       "                                             authors  \\\n",
       "0  C. Bal\\'azs, E. L. Berger, P. M. Nadolsky, C.-...   \n",
       "1                    Ileana Streinu and Louis Theran   \n",
       "2                                        Hongjun Pan   \n",
       "3                                       David Callan   \n",
       "4           Wael Abu-Shammala and Alberto Torchinsky   \n",
       "\n",
       "                                               title  \\\n",
       "0  Calculation of prompt diphoton production cros...   \n",
       "1           Sparsity-certifying Graph Decompositions   \n",
       "2  The evolution of the Earth-Moon system based o...   \n",
       "3  A determinant of Stirling cycle numbers counts...   \n",
       "4  From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...   \n",
       "\n",
       "                                  comments  \\\n",
       "0  37 pages, 15 figures; published version   \n",
       "1    To appear in Graphs and Combinatorics   \n",
       "2                      23 pages, 3 figures   \n",
       "3                                 11 pages   \n",
       "4                                     None   \n",
       "\n",
       "                                 journal-ref                         doi  \\\n",
       "0                   Phys.Rev.D76:013009,2007  10.1103/PhysRevD.76.013009   \n",
       "1                                       None                        None   \n",
       "2                                       None                        None   \n",
       "3                                       None                        None   \n",
       "4  Illinois J. Math. 52 (2008) no.2, 681-689                        None   \n",
       "\n",
       "          report-no       categories  \\\n",
       "0  ANL-HEP-PR-07-12           hep-ph   \n",
       "1              None    math.CO cs.CG   \n",
       "2              None   physics.gen-ph   \n",
       "3              None          math.CO   \n",
       "4              None  math.CA math.FA   \n",
       "\n",
       "                                             license  \\\n",
       "0                                               None   \n",
       "1  http://arxiv.org/licenses/nonexclusive-distrib...   \n",
       "2                                               None   \n",
       "3                                               None   \n",
       "4                                               None   \n",
       "\n",
       "                                            abstract  \\\n",
       "0    A fully differential calculation in perturba...   \n",
       "1    We describe a new algorithm, the $(k,\\ell)$-...   \n",
       "2    The evolution of Earth-Moon system is descri...   \n",
       "3    We show that a determinant of Stirling cycle...   \n",
       "4    In this paper we show how to compute the $\\L...   \n",
       "\n",
       "                                            versions    update_date  \\\n",
       "0  [{'version': 'v1', 'created': 'Mon, 2 Apr 2007...  1227657600000   \n",
       "1  [{'version': 'v1', 'created': 'Sat, 31 Mar 200...  1229126400000   \n",
       "2  [{'version': 'v1', 'created': 'Sun, 1 Apr 2007...  1200182400000   \n",
       "3  [{'version': 'v1', 'created': 'Sat, 31 Mar 200...  1179878400000   \n",
       "4  [{'version': 'v1', 'created': 'Mon, 2 Apr 2007...  1381795200000   \n",
       "\n",
       "                                      authors_parsed  \\\n",
       "0  [[Bal√°zs, C., ], [Berger, E. L., ], [Nadolsky,...   \n",
       "1           [[Streinu, Ileana, ], [Theran, Louis, ]]   \n",
       "2                                 [[Pan, Hongjun, ]]   \n",
       "3                                [[Callan, David, ]]   \n",
       "4  [[Abu-Shammala, Wael, ], [Torchinsky, Alberto, ]]   \n",
       "\n",
       "                                           embedding  \n",
       "0  [0.2324569076, -0.894839108, -0.242858842, 0.1...  \n",
       "1  [0.6949232221, 0.3588359952, 0.1817755997, 0.7...  \n",
       "2  [0.1294624656, 1.1964389086, 0.8928941488, -0....  \n",
       "3  [-0.0994227678, -0.364127785, 0.5390082002, -0...  \n",
       "4  [0.0711007342, 0.5356642008, 0.5095595121, 0.4...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview the dataset\n",
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "o2gHwRjMfJlO"
   },
   "outputs": [],
   "source": [
    "# Initialize MongoDB client and other variables for the vector store\n",
    "\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Initialize MongoDB Python client\n",
    "client = MongoClient(MONGODB_URI)\n",
    "\n",
    "# Name of the database -- Change if needed or leave as is\n",
    "DB_NAME = \"agents_workshop\"\n",
    "# Name of the collection -- Change if needed or leave as is\n",
    "COLLECTION_NAME = \"knowledge\"\n",
    "# Name of the vector search index -- Change if needed or leave as is\n",
    "ATLAS_VECTOR_SEARCH_INDEX_NAME = \"vector_index\"\n",
    "collection = client[DB_NAME][COLLECTION_NAME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zJkyy9UbffZT",
    "outputId": "c6f78ea3-fc93-4d57-95eb-98cea5bf15d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data ingestion into MongoDB completed\n"
     ]
    }
   ],
   "source": [
    "# Delete any existing records in the collection\n",
    "collection.delete_many({})\n",
    "# Ingest data into the collection\n",
    "records = dataset_df.to_dict(\"records\")\n",
    "# Bulk insert multiple records with a single `insert_many` call\n",
    "collection.insert_many(records)\n",
    "\n",
    "print(\"Data ingestion into MongoDB completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6S1Cz9dtGPwL"
   },
   "source": [
    "## Create Vector Search Index Defintion\n",
    "\n",
    "To be done in the Atlas UI. Follow the documentation for instructions.\n",
    "\n",
    "```\n",
    "{\n",
    "  \"fields\": [\n",
    "    {\n",
    "      \"type\": \"vector\",\n",
    "      \"path\": \"embedding\",\n",
    "      \"numDimensions\": 1024,\n",
    "      \"similarity\": \"cosine\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create MongoDB Vector Store Retriever\n",
    "\n",
    "Now that we have ingested data into a MongoDB Collection, let's use it to create a vector store retriever.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding model to use for the vector store -- DO NOT CHANGE\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"mixedbread-ai/mxbai-embed-large-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apoorva.joshi/Documents/ai-agents-workshop/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_mongodb import MongoDBAtlasVectorSearch\n",
    "\n",
    "# Create a MongoDBAtlas vector store object\n",
    "# Reference docs: https://api.python.langchain.com/en/latest/_modules/langchain_mongodb/vectorstores.html#MongoDBAtlasVectorSearch\n",
    "# Use the `from_connection_string` method of the MongoDBAtlAsVectorSearch class.\n",
    "# Arguments: connection_string, namespace, embedding, index_name, text_key\n",
    "vector_store = # INSERT CODE HERE\n",
    "\n",
    "# Construct a retriever from the vector store\n",
    "# Reference docs: https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/vectorstore/\n",
    "# Set the search type for the retriever to `similarity` and `k` to 5.\n",
    "retriever = # INSERT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate Chat Completion LLM\n",
    "\n",
    "Instantiate the chat completion LLM to use as the \"brain\" of our agent and for any of the tools if required.\n",
    "\n",
    "We will use Fireworks AI's open-source AND free `firefunction-v1` model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_fireworks import ChatFireworks\n",
    "\n",
    "# Reference docs: https://python.langchain.com/v0.1/docs/integrations/chat/fireworks/\n",
    "# Create an instance of `ChatFireworks` with the model `accounts/fireworks/models/firefunction-v1`, temperature set to 0.0 and max tokens set to 1024\n",
    "llm = # INSERT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZfheX5FiIhU"
   },
   "source": [
    "## Create Agent Tools\n",
    "\n",
    "Define tools for the agent to use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "3eufR9H8gopU"
   },
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_paper_metadata_from_arxiv(topic: str) -> list:\n",
    "    \"\"\"\n",
    "    Fetch and return paper metadata for 10 arxiv papers matching the given topic, for example: Retrieval Augmented Generation.\n",
    "\n",
    "    Args:\n",
    "    topic (str): The topic to find papers for on arXiv.\n",
    "\n",
    "    Returns:\n",
    "    list: Metadata about the papers matching the topic.\n",
    "    \"\"\"\n",
    "    docs = ArxivLoader(query=topic, load_max_docs=5).load()\n",
    "    # Extract just the metadata from each document\n",
    "    metadata = [doc.metadata for doc in docs]\n",
    "    return metadata\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_paper_summary_from_arxiv(id: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetch and return the summary for a single research paper from arXiv given the paper ID, for example: 1605.08386.\n",
    "\n",
    "    Args:\n",
    "    id (str): The paper ID.\n",
    "\n",
    "    Returns:\n",
    "    str: Summary of the paper.\n",
    "    \"\"\"\n",
    "    # Reference docs:\n",
    "    # https://python.langchain.com/v0.1/docs/integrations/document_loaders/arxiv/\n",
    "    # https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.arxiv.ArxivLoader.html\n",
    "    # Create a tool that uses the ArxivLoader to return summaries given a paper ID (`id`).\n",
    "    # HINT: Use the `get_summaries_as_docs` method of `ArxivLoader`\n",
    "    # Handle the case where the paper ID is invalid i.e. number of docs returned from `ArxivLoader` are 0.\n",
    "    # INSERT CODE HERE\n",
    "\n",
    "\n",
    "@tool\n",
    "def answer_questions_about_topics(query: str) -> list:\n",
    "    \"\"\"\n",
    "    Answer questions about a given topic based on information in the knowledge base.\n",
    "\n",
    "    Args:\n",
    "    query (str): User query about a topic.\n",
    "\n",
    "    Returns:\n",
    "    str: Information about the topic.\n",
    "    \"\"\"\n",
    "    # Reference docs: https://python.langchain.com/v0.1/docs/use_cases/question_answering/quickstart/\n",
    "    # Follow the example in the reference docs above to create a tool that uses the Fireworks LLM we instantiated above to answer questions based on content in our MongoDB Atlas knowledge base.\n",
    "    # Create a RAG chain similar to the one in the example and return the response of the `invoke` call with `query` as an argument\n",
    "    # INSERT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test out the tools\n",
    "\n",
    "Test out the tools individually to make sure we are getting the right responses from them. Remember tools are LangChain Runnables so we can use the `invoke` method on them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_paper_metadata_from_arxiv.invoke(\"Retrieval Augmented Generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_paper_summary_from_arxiv.invoke(\"1808.09236\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_paper_summary_from_arxiv.invoke(\"808.09236\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_questions_about_topics.invoke(\"What are partial cubes?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of tools\n",
    "tools = [\n",
    "    get_paper_metadata_from_arxiv,\n",
    "    get_paper_summary_from_arxiv,\n",
    "    answer_questions_about_topics,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Basic Tool-calling Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "RY13DrVXFDrm"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain.tools.render import render_text_description\n",
    "\n",
    "system_message = f\"\"\"Answer the following questions as best you can.\n",
    "You can answer directly if the user is greeting you or similar.\n",
    "Otherwise, you have access to the following tools:\n",
    "\n",
    "{render_text_description(tools)}\n",
    "\"\"\"\n",
    "\n",
    "# # CoT prompt\n",
    "# system_message = f\"\"\"Given a question, write out in a step-by-step manner your reasoning\n",
    "# for how you will solve the problem to be sure that your conclusion is correct.\n",
    "# Avoid simply stating the correct answer at the outset.You can answer directly if the user\n",
    "# is greeting you or similar. Otherwise, you have access to the following tools:\n",
    "\n",
    "# {render_text_description(tools)}\n",
    "\n",
    "# Begin!\n",
    "# \"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_message),\n",
    "        (\"human\", \"{input}\"),\n",
    "        # Placeholders fill up a **list** of messages\n",
    "        MessagesPlaceholder(\"agent_scratchpad\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent, tools=tools, verbose=True, handle_parsing_errors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `get_paper_metadata_from_arxiv` with `{'topic': 'prompt compression'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'Published': '2024-03-30', 'Title': 'PROMPT-SAW: Leveraging Relation-Aware Graphs for Textual Prompt Compression', 'Authors': 'Muhammad Asif Ali, Zhengping Li, Shu Yang, Keyuan Cheng, Yang Cao, Tianhao Huang, Lijie Hu, Lu Yu, Di Wang', 'Summary': \"Large language models (LLMs) have shown exceptional abilities for multiple\\ndifferent natural language processing tasks. While prompting is a crucial tool\\nfor LLM inference, we observe that there is a significant cost associated with\\nexceedingly lengthy prompts. Existing attempts to compress lengthy prompts lead\\nto sub-standard results in terms of readability and interpretability of the\\ncompressed prompt, with a detrimental impact on prompt utility. To address\\nthis, we propose PROMPT-SAW: Prompt compresSion via Relation AWare graphs, an\\neffective strategy for prompt compression over task-agnostic and task-aware\\nprompts. PROMPT-SAW uses the prompt's textual information to build a graph,\\nlater extracts key information elements in the graph to come up with the\\ncompressed prompt. We also propose GSM8K-AUG, i.e., an extended version of the\\nexisting GSM8k benchmark for task-agnostic prompts in order to provide a\\ncomprehensive evaluation platform. Experimental evaluation using benchmark\\ndatasets shows that prompts compressed by PROMPT-SAW are not only better in\\nterms of readability, but they also outperform the best-performing baseline\\nmodels by up to 14.3 and 13.7 respectively for task-aware and task-agnostic\\nsettings while compressing the original prompt text by 33.0 and 56.7.\"}, {'Published': '2024-02-25', 'Title': 'Say More with Less: Understanding Prompt Learning Behaviors through Gist Compression', 'Authors': 'Xinze Li, Zhenghao Liu, Chenyan Xiong, Shi Yu, Yukun Yan, Shuo Wang, Ge Yu', 'Summary': 'Large language models (LLMs) require lengthy prompts as the input context to\\nproduce output aligned with user intentions, a process that incurs extra costs\\nduring inference. In this paper, we propose the Gist COnditioned deCOding\\n(Gist-COCO) model, introducing a novel method for compressing prompts which\\nalso can assist the prompt interpretation and engineering. Gist-COCO employs an\\nencoder-decoder based language model and then incorporates an additional\\nencoder as a plugin module to compress prompts with inputs using gist tokens.\\nIt finetunes the compression plugin module and uses the representations of gist\\ntokens to emulate the raw prompts in the vanilla language model. By verbalizing\\nthe representations of gist tokens into gist prompts, the compression ability\\nof Gist-COCO can be generalized to different LLMs with high compression rates.\\nOur experiments demonstrate that Gist-COCO outperforms previous prompt\\ncompression models in both passage and instruction compression tasks. Further\\nanalysis on gist verbalization results suggests that our gist prompts serve\\ndifferent functions in aiding language models. They may directly provide\\npotential answers, generate the chain-of-thought, or simply repeat the inputs.\\nAll data and codes are available at https://github.com/OpenMatch/Gist-COCO .'}, {'Published': '2023-10-10', 'Title': 'Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt', 'Authors': 'Zhaozhuo Xu, Zirui Liu, Beidi Chen, Yuxin Tang, Jue Wang, Kaixiong Zhou, Xia Hu, Anshumali Shrivastava', 'Summary': \"While the numerous parameters in Large Language Models (LLMs) contribute to\\ntheir superior performance, this massive scale makes them inefficient and\\nmemory-hungry. Thus, they are hard to deploy on commodity hardware, such as one\\nsingle GPU. Given the memory and power constraints of such devices, model\\ncompression methods are widely employed to reduce both the model size and\\ninference latency, which essentially trades off model quality in return for\\nimproved efficiency. Thus, optimizing this accuracy-efficiency trade-off is\\ncrucial for the LLM deployment on commodity hardware. In this paper, we\\nintroduce a new perspective to optimize this trade-off by prompting compressed\\nmodels. Specifically, we first observe that for certain questions, the\\ngeneration quality of a compressed LLM can be significantly improved by adding\\ncarefully designed hard prompts, though this isn't the case for all questions.\\nBased on this observation, we propose a soft prompt learning method where we\\nexpose the compressed model to the prompt learning process, aiming to enhance\\nthe performance of prompts. Our experimental analysis suggests our soft prompt\\nstrategy greatly improves the performance of the 8x compressed LLaMA-7B model\\n(with a joint 4-bit quantization and 50% weight pruning compression), allowing\\nthem to match their uncompressed counterparts on popular benchmarks. Also, we\\ndemonstrate that these learned prompts can be transferred across various\\ndatasets, tasks, and compression levels. Hence with this transferability, we\\ncan stitch the soft prompt to a newly compressed model to improve the test-time\\naccuracy in an ``in-situ'' way.\"}, {'Published': '2024-04-26', 'Title': 'PromptCIR: Blind Compressed Image Restoration with Prompt Learning', 'Authors': 'Bingchen Li, Xin Li, Yiting Lu, Ruoyu Feng, Mengxi Guo, Shijie Zhao, Li Zhang, Zhibo Chen', 'Summary': 'Blind Compressed Image Restoration (CIR) has garnered significant attention\\ndue to its practical applications. It aims to mitigate compression artifacts\\ncaused by unknown quality factors, particularly with JPEG codecs. Existing\\nworks on blind CIR often seek assistance from a quality factor prediction\\nnetwork to facilitate their network to restore compressed images. However, the\\npredicted numerical quality factor lacks spatial information, preventing\\nnetwork adaptability toward image contents. Recent studies in\\nprompt-learning-based image restoration have showcased the potential of prompts\\nto generalize across varied degradation types and degrees. This motivated us to\\ndesign a prompt-learning-based compressed image restoration network, dubbed\\nPromptCIR, which can effectively restore images from various compress levels.\\nSpecifically, PromptCIR exploits prompts to encode compression information\\nimplicitly, where prompts directly interact with soft weights generated from\\nimage features, thus providing dynamic content-aware and distortion-aware\\nguidance for the restoration process. The light-weight prompts enable our\\nmethod to adapt to different compression levels, while introducing minimal\\nparameter overhead. Overall, PromptCIR leverages the powerful transformer-based\\nbackbone with the dynamic prompt module to proficiently handle blind CIR tasks,\\nwinning first place in the NTIRE 2024 challenge of blind compressed image\\nenhancement track. Extensive experiments have validated the effectiveness of\\nour proposed PromptCIR. The code is available at\\nhttps://github.com/lbc12345/PromptCIR-NTIRE24.'}, {'Published': '2024-04-02', 'Title': 'Learning to Compress Prompt in Natural Language Formats', 'Authors': 'Yu-Neng Chuang, Tianwei Xing, Chia-Yuan Chang, Zirui Liu, Xun Chen, Xia Hu', 'Summary': 'Large language models (LLMs) are great at processing multiple natural\\nlanguage processing tasks, but their abilities are constrained by inferior\\nperformance with long context, slow inference speed, and the high cost of\\ncomputing the results. Deploying LLMs with precise and informative context\\nhelps users process large-scale datasets more effectively and cost-efficiently.\\nExisting works rely on compressing long prompt contexts into soft prompts.\\nHowever, soft prompt compression encounters limitations in transferability\\nacross different LLMs, especially API-based LLMs. To this end, this work aims\\nto compress lengthy prompts in the form of natural language with LLM\\ntransferability. This poses two challenges: (i) Natural Language (NL) prompts\\nare incompatible with back-propagation, and (ii) NL prompts lack flexibility in\\nimposing length constraints. In this work, we propose a Natural Language Prompt\\nEncapsulation (Nano-Capsulator) framework compressing original prompts into NL\\nformatted Capsule Prompt while maintaining the prompt utility and\\ntransferability. Specifically, to tackle the first challenge, the\\nNano-Capsulator is optimized by a reward function that interacts with the\\nproposed semantics preserving loss. To address the second question, the\\nNano-Capsulator is optimized by a reward function featuring length constraints.\\nExperimental results demonstrate that the Capsule Prompt can reduce 81.4% of\\nthe original length, decrease inference latency up to 4.5x, and save 80.1% of\\nbudget overheads while providing transferability across diverse LLMs and\\ndifferent datasets.'}]\u001b[0m\u001b[32;1m\u001b[1;3mHere are some papers on the topic of prompt compression:\n",
      "\n",
      "1. \"PROMPT-SAW: Leveraging Relation-Aware Graphs for Textual Prompt Compression\" by Muhammad Asif Ali, Zhengping Li, Shu Yang, Keyuan Cheng, Yang Cao, Tianhao Huang, Lijie Hu, Lu Yu, Di Wang. Published on 2024-03-30.\n",
      "\n",
      "2. \"Say More with Less: Understanding Prompt Learning Behaviors through Gist Compression\" by Xinze Li, Zhenghao Liu, Chenyan Xiong, Shi Yu, Yukun Yan, Shuo Wang, Ge Yu. Published on 2024-02-25.\n",
      "\n",
      "3. \"Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt\" by Zhaozhuo Xu, Zirui Liu, Beidi Chen, Yuxin Tang, Jue Wang, Kaixiong Zhou, Xia Hu, Anshumali Shrivastava. Published on 2023-10-10.\n",
      "\n",
      "4. \"PromptCIR: Blind Compressed Image Restoration with Prompt Learning\" by Bingchen Li, Xin Li, Yiting Lu, Ruoyu Feng, Mengxi Guo, Shijie Zhao, Li Zhang, Zhibo Chen. Published on 2024-04-26.\n",
      "\n",
      "5. \"Learning to Compress Prompt in Natural Language Formats\" by Yu-Neng Chuang, Tianwei Xing, Chia-Yuan Chang, Zirui Liu, Xun Chen, Xia Hu. Published on 2024-04-02.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Give me papers on the topic prompt compression.',\n",
       " 'output': 'Here are some papers on the topic of prompt compression:\\n\\n1. \"PROMPT-SAW: Leveraging Relation-Aware Graphs for Textual Prompt Compression\" by Muhammad Asif Ali, Zhengping Li, Shu Yang, Keyuan Cheng, Yang Cao, Tianhao Huang, Lijie Hu, Lu Yu, Di Wang. Published on 2024-03-30.\\n\\n2. \"Say More with Less: Understanding Prompt Learning Behaviors through Gist Compression\" by Xinze Li, Zhenghao Liu, Chenyan Xiong, Shi Yu, Yukun Yan, Shuo Wang, Ge Yu. Published on 2024-02-25.\\n\\n3. \"Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt\" by Zhaozhuo Xu, Zirui Liu, Beidi Chen, Yuxin Tang, Jue Wang, Kaixiong Zhou, Xia Hu, Anshumali Shrivastava. Published on 2023-10-10.\\n\\n4. \"PromptCIR: Blind Compressed Image Restoration with Prompt Learning\" by Bingchen Li, Xin Li, Yiting Lu, Ruoyu Feng, Mengxi Guo, Shijie Zhao, Li Zhang, Zhibo Chen. Published on 2024-04-26.\\n\\n5. \"Learning to Compress Prompt in Natural Language Formats\" by Yu-Neng Chuang, Tianwei Xing, Chia-Yuan Chang, Zirui Liu, Xun Chen, Xia Hu. Published on 2024-04-02.'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"Give me papers on the topic prompt compression.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Without using `create_tool_calling_agent`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.output_parsers.tools import ToolsAgentOutputParser\n",
    "from langchain.agents.format_scratchpad.tools import (\n",
    "    format_to_tool_messages,\n",
    ")\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "agent = (\n",
    "    RunnablePassthrough.assign(\n",
    "        agent_scratchpad=lambda x: format_to_tool_messages(x[\"intermediate_steps\"])\n",
    "    )\n",
    "    | prompt\n",
    "    | llm_with_tools\n",
    "    | ToolsAgentOutputParser()\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent, tools=tools, verbose=True, handle_parsing_errors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.invoke({\"input\": \"Give me papers on the topic prompt compression.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a ReAct Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "\u001b[33;1m\u001b[1;3m{tools}\u001b[0m\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [\u001b[33;1m\u001b[1;3m{tool_names}\u001b[0m]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "\n",
      "Question: \u001b[33;1m\u001b[1;3m{input}\u001b[0m\n",
      "Thought:\u001b[33;1m\u001b[1;3m{agent_scratchpad}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_react_agent\n",
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_react_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=\"Check your output. Make an observation in order to determine whether or not you have the final answer.\\\n",
    "        If you do, use the exact characters `Final Answer` and exit.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to use the get_paper_summary_from_arxiv tool to get the summary for the paper with ID 1808.09236.\n",
      "\n",
      "Action: get_paper_summary_from_arxiv\n",
      "Action Input: 1808.09236\u001b[0m\u001b[33;1m\u001b[1;3mWe determine the non-perturbatively renormalized axial current for O($a$)\n",
      "improved lattice QCD with Wilson quarks. Our strategy is based on the chirally\n",
      "rotated Schr\\\"odinger functional and can be generalized to other finite (ratios\n",
      "of) renormalization constants which are traditionally obtained by imposing\n",
      "continuum chiral Ward identities as normalization conditions. Compared to the\n",
      "latter we achieve an error reduction up to one order of magnitude. Our results\n",
      "have already enabled the setting of the scale for the $N_{\\rm f}=2+1$ CLS\n",
      "ensembles [1] and are thus an essential ingredient for the recent $\\alpha_s$\n",
      "determination by the ALPHA collaboration [2]. In this paper we shortly review\n",
      "the strategy and present our results for both $N_{\\rm f}=2$ and $N_{\\rm f}=3$\n",
      "lattice QCD, where we match the $\\beta$-values of the CLS gauge configurations.\n",
      "In addition to the axial current renormalization, we also present precise\n",
      "m f}=3$ lattice QCD, where we match the $\\beta$-values of the CLS gauge configurations. In addition to the axial current renormalization, we also present precise results for the renormalized local vector current.\"`\u001b[0mCheck your output. Make an observation in order to determine whether or not you have the final answer.        If you do, use the exact characters `Final Answer` and exit.malization constants which are traditionally obtained by imposing continuum chiral Ward identities as normalization conditions. Compared to the latter we achieve an error reduction up to one order of magnitude. Our results have already enabled the setting of the scale for the $N_{\u001b[32;1m\u001b[1;3m\n",
      "\n",
      "Final Answer: \n",
      "\n",
      "m f}=3$ lattice QCD, where we match the $\\beta$-values of the CLS gauge configurations. In addition to the axial current renormalization, we also present precise results for the renormalized local vector current.\"\u001b[0m_{inger functional and can be generalized to other finite (ratios of) renormalization constants which are traditionally obtained by imposing continuum chiral Ward identities as normalization conditions. Compared to the latter we achieve an error reduction up to one order of magnitude. Our results have already enabled the setting of the scale for the $N_{\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Give me the summary for the paper 1808.09236.',\n",
       " 'output': 'The summary for the paper with ID 1808.09236 is: \"We determine the non-perturbatively renormalized axial current for O($a$) improved lattice QCD with Wilson quarks. Our strategy is based on the chirally rotated Schr\\\\\"odinger functional and can be generalized to other finite (ratios of) renormalization constants which are traditionally obtained by imposing continuum chiral Ward identities as normalization conditions. Compared to the latter we achieve an error reduction up to one order of magnitude. Our results have already enabled the setting of the scale for the $N_{\\rm f}=2+1$ CLS ensembles [1] and are thus an essential ingredient for the recent $\\\\alpha_s$ determination by the ALPHA collaboration [2]. In this paper we shortly review the strategy and present our results for both $N_{\\rm f}=2$ and $N_{\\rm f}=3$ lattice QCD, where we match the $\\\\beta$-values of the CLS gauge configurations. In addition to the axial current renormalization, we also present precise results for the renormalized local vector current.\"'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"Give me the summary for the paper 1808.09236.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z4NU4ZjGl0WC"
   },
   "source": [
    "## Add Memory to Agents using MongoDB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "1A-3Fg1cjwyK"
   },
   "outputs": [],
   "source": [
    "from langchain_mongodb.chat_message_histories import MongoDBChatMessageHistory\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> MongoDBChatMessageHistory:\n",
    "    return MongoDBChatMessageHistory(\n",
    "        MONGODB_URI, session_id, database_name=DB_NAME, collection_name=\"history\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "wI4uBAmNF5ll"
   },
   "outputs": [],
   "source": [
    "system_message = f\"\"\"Answer the following questions as best you can.\n",
    "You can answer directly if the user is greeting you or similar.\n",
    "Otherwise, you have access to the following tools:\n",
    "\n",
    "{render_text_description(tools)}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_message),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "        # Placeholders fill up a **list** of messages\n",
    "        MessagesPlaceholder(\"agent_scratchpad\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent, tools=tools, verbose=True, handle_parsing_errors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "agent_with_chat_history = RunnableWithMessageHistory(\n",
    "    agent_executor,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `get_paper_metadata_from_arxiv` with `{'topic': 'Prompt Compression'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'Published': '2024-03-30', 'Title': 'PROMPT-SAW: Leveraging Relation-Aware Graphs for Textual Prompt Compression', 'Authors': 'Muhammad Asif Ali, Zhengping Li, Shu Yang, Keyuan Cheng, Yang Cao, Tianhao Huang, Lijie Hu, Lu Yu, Di Wang', 'Summary': \"Large language models (LLMs) have shown exceptional abilities for multiple\\ndifferent natural language processing tasks. While prompting is a crucial tool\\nfor LLM inference, we observe that there is a significant cost associated with\\nexceedingly lengthy prompts. Existing attempts to compress lengthy prompts lead\\nto sub-standard results in terms of readability and interpretability of the\\ncompressed prompt, with a detrimental impact on prompt utility. To address\\nthis, we propose PROMPT-SAW: Prompt compresSion via Relation AWare graphs, an\\neffective strategy for prompt compression over task-agnostic and task-aware\\nprompts. PROMPT-SAW uses the prompt's textual information to build a graph,\\nlater extracts key information elements in the graph to come up with the\\ncompressed prompt. We also propose GSM8K-AUG, i.e., an extended version of the\\nexisting GSM8k benchmark for task-agnostic prompts in order to provide a\\ncomprehensive evaluation platform. Experimental evaluation using benchmark\\ndatasets shows that prompts compressed by PROMPT-SAW are not only better in\\nterms of readability, but they also outperform the best-performing baseline\\nmodels by up to 14.3 and 13.7 respectively for task-aware and task-agnostic\\nsettings while compressing the original prompt text by 33.0 and 56.7.\"}, {'Published': '2024-02-25', 'Title': 'Say More with Less: Understanding Prompt Learning Behaviors through Gist Compression', 'Authors': 'Xinze Li, Zhenghao Liu, Chenyan Xiong, Shi Yu, Yukun Yan, Shuo Wang, Ge Yu', 'Summary': 'Large language models (LLMs) require lengthy prompts as the input context to\\nproduce output aligned with user intentions, a process that incurs extra costs\\nduring inference. In this paper, we propose the Gist COnditioned deCOding\\n(Gist-COCO) model, introducing a novel method for compressing prompts which\\nalso can assist the prompt interpretation and engineering. Gist-COCO employs an\\nencoder-decoder based language model and then incorporates an additional\\nencoder as a plugin module to compress prompts with inputs using gist tokens.\\nIt finetunes the compression plugin module and uses the representations of gist\\ntokens to emulate the raw prompts in the vanilla language model. By verbalizing\\nthe representations of gist tokens into gist prompts, the compression ability\\nof Gist-COCO can be generalized to different LLMs with high compression rates.\\nOur experiments demonstrate that Gist-COCO outperforms previous prompt\\ncompression models in both passage and instruction compression tasks. Further\\nanalysis on gist verbalization results suggests that our gist prompts serve\\ndifferent functions in aiding language models. They may directly provide\\npotential answers, generate the chain-of-thought, or simply repeat the inputs.\\nAll data and codes are available at https://github.com/OpenMatch/Gist-COCO .'}, {'Published': '2023-10-10', 'Title': 'Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt', 'Authors': 'Zhaozhuo Xu, Zirui Liu, Beidi Chen, Yuxin Tang, Jue Wang, Kaixiong Zhou, Xia Hu, Anshumali Shrivastava', 'Summary': \"While the numerous parameters in Large Language Models (LLMs) contribute to\\ntheir superior performance, this massive scale makes them inefficient and\\nmemory-hungry. Thus, they are hard to deploy on commodity hardware, such as one\\nsingle GPU. Given the memory and power constraints of such devices, model\\ncompression methods are widely employed to reduce both the model size and\\ninference latency, which essentially trades off model quality in return for\\nimproved efficiency. Thus, optimizing this accuracy-efficiency trade-off is\\ncrucial for the LLM deployment on commodity hardware. In this paper, we\\nintroduce a new perspective to optimize this trade-off by prompting compressed\\nmodels. Specifically, we first observe that for certain questions, the\\ngeneration quality of a compressed LLM can be significantly improved by adding\\ncarefully designed hard prompts, though this isn't the case for all questions.\\nBased on this observation, we propose a soft prompt learning method where we\\nexpose the compressed model to the prompt learning process, aiming to enhance\\nthe performance of prompts. Our experimental analysis suggests our soft prompt\\nstrategy greatly improves the performance of the 8x compressed LLaMA-7B model\\n(with a joint 4-bit quantization and 50% weight pruning compression), allowing\\nthem to match their uncompressed counterparts on popular benchmarks. Also, we\\ndemonstrate that these learned prompts can be transferred across various\\ndatasets, tasks, and compression levels. Hence with this transferability, we\\ncan stitch the soft prompt to a newly compressed model to improve the test-time\\naccuracy in an ``in-situ'' way.\"}, {'Published': '2024-04-26', 'Title': 'PromptCIR: Blind Compressed Image Restoration with Prompt Learning', 'Authors': 'Bingchen Li, Xin Li, Yiting Lu, Ruoyu Feng, Mengxi Guo, Shijie Zhao, Li Zhang, Zhibo Chen', 'Summary': 'Blind Compressed Image Restoration (CIR) has garnered significant attention\\ndue to its practical applications. It aims to mitigate compression artifacts\\ncaused by unknown quality factors, particularly with JPEG codecs. Existing\\nworks on blind CIR often seek assistance from a quality factor prediction\\nnetwork to facilitate their network to restore compressed images. However, the\\npredicted numerical quality factor lacks spatial information, preventing\\nnetwork adaptability toward image contents. Recent studies in\\nprompt-learning-based image restoration have showcased the potential of prompts\\nto generalize across varied degradation types and degrees. This motivated us to\\ndesign a prompt-learning-based compressed image restoration network, dubbed\\nPromptCIR, which can effectively restore images from various compress levels.\\nSpecifically, PromptCIR exploits prompts to encode compression information\\nimplicitly, where prompts directly interact with soft weights generated from\\nimage features, thus providing dynamic content-aware and distortion-aware\\nguidance for the restoration process. The light-weight prompts enable our\\nmethod to adapt to different compression levels, while introducing minimal\\nparameter overhead. Overall, PromptCIR leverages the powerful transformer-based\\nbackbone with the dynamic prompt module to proficiently handle blind CIR tasks,\\nwinning first place in the NTIRE 2024 challenge of blind compressed image\\nenhancement track. Extensive experiments have validated the effectiveness of\\nour proposed PromptCIR. The code is available at\\nhttps://github.com/lbc12345/PromptCIR-NTIRE24.'}, {'Published': '2024-04-02', 'Title': 'Learning to Compress Prompt in Natural Language Formats', 'Authors': 'Yu-Neng Chuang, Tianwei Xing, Chia-Yuan Chang, Zirui Liu, Xun Chen, Xia Hu', 'Summary': 'Large language models (LLMs) are great at processing multiple natural\\nlanguage processing tasks, but their abilities are constrained by inferior\\nperformance with long context, slow inference speed, and the high cost of\\ncomputing the results. Deploying LLMs with precise and informative context\\nhelps users process large-scale datasets more effectively and cost-efficiently.\\nExisting works rely on compressing long prompt contexts into soft prompts.\\nHowever, soft prompt compression encounters limitations in transferability\\nacross different LLMs, especially API-based LLMs. To this end, this work aims\\nto compress lengthy prompts in the form of natural language with LLM\\ntransferability. This poses two challenges: (i) Natural Language (NL) prompts\\nare incompatible with back-propagation, and (ii) NL prompts lack flexibility in\\nimposing length constraints. In this work, we propose a Natural Language Prompt\\nEncapsulation (Nano-Capsulator) framework compressing original prompts into NL\\nformatted Capsule Prompt while maintaining the prompt utility and\\ntransferability. Specifically, to tackle the first challenge, the\\nNano-Capsulator is optimized by a reward function that interacts with the\\nproposed semantics preserving loss. To address the second question, the\\nNano-Capsulator is optimized by a reward function featuring length constraints.\\nExperimental results demonstrate that the Capsule Prompt can reduce 81.4% of\\nthe original length, decrease inference latency up to 4.5x, and save 80.1% of\\nbudget overheads while providing transferability across diverse LLMs and\\ndifferent datasets.'}]\u001b[0m\u001b[32;1m\u001b[1;3mHere are some papers on Prompt Compression:\n",
      "\n",
      "1. \"PROMPT-SAW: Leveraging Relation-Aware Graphs for Textual Prompt Compression\" by Muhammad Asif Ali, Zhengping Li, Shu Yang, Keyuan Cheng, Yang Cao, Tianhao Huang, Lijie Hu, Lu Yu, Di Wang. Published on 2024-03-30.\n",
      "\n",
      "Summary: This paper proposes PROMPT-SAW, an effective strategy for prompt compression over task-agnostic and task-aware prompts. PROMPT-SAW uses the prompt's textual information to build a graph, later extracts key information elements in the graph to come up with the compressed prompt. The authors also propose GSM8K-AUG, an extended version of the existing GSM8k benchmark for task-agnostic prompts in order to provide a comprehensive evaluation platform.\n",
      "\n",
      "2. \"Say More with Less: Understanding Prompt Learning Behaviors through Gist Compression\" by Xinze Li, Zhenghao Liu, Chenyan Xiong, Shi Yu, Yukun Yan, Shuo Wang, Ge Yu. Published on 2024-02-25.\n",
      "\n",
      "Summary: This paper proposes the Gist COnditioned deCOding (Gist-COCO) model, introducing a novel method for compressing prompts which also can assist the prompt interpretation and engineering. Gist-COCO employs an encoder-decoder based language model and then incorporates an additional encoder as a plugin module to compress prompts with inputs using gist tokens. It finetunes the compression plugin module and uses the representations of gist tokens to emulate the raw prompts in the vanilla language model.\n",
      "\n",
      "3. \"Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt\" by Zhaozhuo Xu, Zirui Liu, Beidi Chen, Yuxin Tang, Jue Wang, Kaixiong Zhou, Xia Hu, Anshumali Shrivastava. Published on 2023-10-10.\n",
      "\n",
      "Summary: This paper introduces a new perspective to optimize the accuracy-efficiency trade-off by prompting compressed models. The authors propose a soft prompt learning method where they expose the compressed model to the prompt learning process, aiming to enhance the performance of prompts. Their experimental analysis suggests that their soft prompt strategy greatly improves the performance of the 8x compressed LLaMA-7B model, allowing them to match their uncompressed counterparts on popular benchmarks.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Get me papers on Prompt Compression.',\n",
       " 'chat_history': [],\n",
       " 'output': 'Here are some papers on Prompt Compression:\\n\\n1. \"PROMPT-SAW: Leveraging Relation-Aware Graphs for Textual Prompt Compression\" by Muhammad Asif Ali, Zhengping Li, Shu Yang, Keyuan Cheng, Yang Cao, Tianhao Huang, Lijie Hu, Lu Yu, Di Wang. Published on 2024-03-30.\\n\\nSummary: This paper proposes PROMPT-SAW, an effective strategy for prompt compression over task-agnostic and task-aware prompts. PROMPT-SAW uses the prompt\\'s textual information to build a graph, later extracts key information elements in the graph to come up with the compressed prompt. The authors also propose GSM8K-AUG, an extended version of the existing GSM8k benchmark for task-agnostic prompts in order to provide a comprehensive evaluation platform.\\n\\n2. \"Say More with Less: Understanding Prompt Learning Behaviors through Gist Compression\" by Xinze Li, Zhenghao Liu, Chenyan Xiong, Shi Yu, Yukun Yan, Shuo Wang, Ge Yu. Published on 2024-02-25.\\n\\nSummary: This paper proposes the Gist COnditioned deCOding (Gist-COCO) model, introducing a novel method for compressing prompts which also can assist the prompt interpretation and engineering. Gist-COCO employs an encoder-decoder based language model and then incorporates an additional encoder as a plugin module to compress prompts with inputs using gist tokens. It finetunes the compression plugin module and uses the representations of gist tokens to emulate the raw prompts in the vanilla language model.\\n\\n3. \"Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt\" by Zhaozhuo Xu, Zirui Liu, Beidi Chen, Yuxin Tang, Jue Wang, Kaixiong Zhou, Xia Hu, Anshumali Shrivastava. Published on 2023-10-10.\\n\\nSummary: This paper introduces a new perspective to optimize the accuracy-efficiency trade-off by prompting compressed models. The authors propose a soft prompt learning method where they expose the compressed model to the prompt learning process, aiming to enhance the performance of prompts. Their experimental analysis suggests that their soft prompt strategy greatly improves the performance of the 8x compressed LLaMA-7B model, allowing them to match their uncompressed counterparts on popular benchmarks.'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_with_chat_history.invoke(\n",
    "    {\"input\": \"Get me papers on Prompt Compression.\"},\n",
    "    config={\"configurable\": {\"session_id\": \"my-session\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThe title of the first paper is \"PROMPT-SAW: Leveraging Relation-Aware Graphs for Textual Prompt Compression\".\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is the title of the first paper you found?',\n",
       " 'chat_history': [HumanMessage(content='Get me papers on Prompt Compression.'),\n",
       "  AIMessage(content='Here are some papers on Prompt Compression:\\n\\n1. \"PROMPT-SAW: Leveraging Relation-Aware Graphs for Textual Prompt Compression\" by Muhammad Asif Ali, Zhengping Li, Shu Yang, Keyuan Cheng, Yang Cao, Tianhao Huang, Lijie Hu, Lu Yu, Di Wang. Published on 2024-03-30.\\n\\nSummary: This paper proposes PROMPT-SAW, an effective strategy for prompt compression over task-agnostic and task-aware prompts. PROMPT-SAW uses the prompt\\'s textual information to build a graph, later extracts key information elements in the graph to come up with the compressed prompt. The authors also propose GSM8K-AUG, an extended version of the existing GSM8k benchmark for task-agnostic prompts in order to provide a comprehensive evaluation platform.\\n\\n2. \"Say More with Less: Understanding Prompt Learning Behaviors through Gist Compression\" by Xinze Li, Zhenghao Liu, Chenyan Xiong, Shi Yu, Yukun Yan, Shuo Wang, Ge Yu. Published on 2024-02-25.\\n\\nSummary: This paper proposes the Gist COnditioned deCOding (Gist-COCO) model, introducing a novel method for compressing prompts which also can assist the prompt interpretation and engineering. Gist-COCO employs an encoder-decoder based language model and then incorporates an additional encoder as a plugin module to compress prompts with inputs using gist tokens. It finetunes the compression plugin module and uses the representations of gist tokens to emulate the raw prompts in the vanilla language model.\\n\\n3. \"Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt\" by Zhaozhuo Xu, Zirui Liu, Beidi Chen, Yuxin Tang, Jue Wang, Kaixiong Zhou, Xia Hu, Anshumali Shrivastava. Published on 2023-10-10.\\n\\nSummary: This paper introduces a new perspective to optimize the accuracy-efficiency trade-off by prompting compressed models. The authors propose a soft prompt learning method where they expose the compressed model to the prompt learning process, aiming to enhance the performance of prompts. Their experimental analysis suggests that their soft prompt strategy greatly improves the performance of the 8x compressed LLaMA-7B model, allowing them to match their uncompressed counterparts on popular benchmarks.')],\n",
       " 'output': 'The title of the first paper is \"PROMPT-SAW: Leveraging Relation-Aware Graphs for Textual Prompt Compression\".'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_with_chat_history.invoke(\n",
    "    {\"input\": \"What is the title of the first paper you found?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"my-session\"}},\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "RM8rg08YhqZe",
    "UUf3jtFzO4-V",
    "Sm5QZdshwJLN"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "09dcf4ce88064f11980bbefaad1ebc75": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_39563df9477648398456675ec51075aa",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_f4353368efbd4c3891f805ddc3d05e1b",
      "value": "Downloading‚Äádata:‚Äá100%"
     }
    },
    "164d16df28d24ab796b7c9cf85174800": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_95e4af5b420242b7a6b74a18cad98961",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_dff65b579f0746ffae8739ecb0aa5a41",
      "value": "Generating‚Äátrain‚Äásplit:‚Äá"
     }
    },
    "20d693a09c534414a5c4c0dd58cf94ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "278513c5a8b04a24b1823d38107f1e50": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_62e196b6d30746578e137c50b661f946",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_ced7f9d61e06442a960dcda95852048e",
      "value": "‚Äá102M/102M‚Äá[00:06&lt;00:00,‚Äá20.6MB/s]"
     }
    },
    "30fe0bcd02cb47f3ba23bb480e2eaaea": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "373ed3b6307741859ab297c270cf42c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "39563df9477648398456675ec51075aa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "41056c822b9d44559147d2b21416b956": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a43c349d171e469c8cc94d48060f775b",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_373ed3b6307741859ab297c270cf42c8",
      "value": "‚Äá50000/0‚Äá[00:04&lt;00:00,‚Äá12390.43‚Äáexamples/s]"
     }
    },
    "62e196b6d30746578e137c50b661f946": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7dbfebff68ff45628da832fac5233c93": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_164d16df28d24ab796b7c9cf85174800",
       "IPY_MODEL_e70e0d317f1e4e73bd95349ed1510cce",
       "IPY_MODEL_41056c822b9d44559147d2b21416b956"
      ],
      "layout": "IPY_MODEL_b1929fb112174c0abcd8004f6be0f880"
     }
    },
    "95e4af5b420242b7a6b74a18cad98961": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a43c349d171e469c8cc94d48060f775b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b1929fb112174c0abcd8004f6be0f880": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cebfba144ba6418092df949783f93455": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_09dcf4ce88064f11980bbefaad1ebc75",
       "IPY_MODEL_f2bd7bda4d0c4d93b88e53aeb4e1b62d",
       "IPY_MODEL_278513c5a8b04a24b1823d38107f1e50"
      ],
      "layout": "IPY_MODEL_d3941c633788427abb858b21e285088f"
     }
    },
    "ced7f9d61e06442a960dcda95852048e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d17d8c8f45ee44cd87dcd787c05dbdc3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d3941c633788427abb858b21e285088f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dff65b579f0746ffae8739ecb0aa5a41": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e70e0d317f1e4e73bd95349ed1510cce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f73ae771c24645c79fd41409a8fc7b34",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_20d693a09c534414a5c4c0dd58cf94ed",
      "value": 1
     }
    },
    "f2bd7bda4d0c4d93b88e53aeb4e1b62d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_30fe0bcd02cb47f3ba23bb480e2eaaea",
      "max": 102202622,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d17d8c8f45ee44cd87dcd787c05dbdc3",
      "value": 102202622
     }
    },
    "f4353368efbd4c3891f805ddc3d05e1b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f73ae771c24645c79fd41409a8fc7b34": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
